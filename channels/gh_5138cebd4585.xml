<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[PaperWeekly]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[PaperWeekly公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM4XfZEiaIxTZFPcSMe0laHNlkWJfvNVMcFY7PrIPmKrQjg/132</url>
      <title>gh_5138cebd4585</title>
    </image>
    <item>
      <title><![CDATA[GPT-5、Gemini 3 Pro谁更懂风控？首个信贷多模态评测基准FCMBench出炉]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkBxFSia6STl2jhnRKmzicaiaCP1sHWGDE3JLWyX87ctl3HPAbMUGrmr4tFJA7OiaFyxk8oRaaDW2HeeA/640?wxtype=jpeg&amp;wxfrom=0"/><p>4043 张物理重拍样本，打破信贷 AI 的数据死锁。在多模态大模型不断刷新各种通用榜单的今天，金融信贷却始终是一个让 SOTA 模型感到力不从心的隐秘角落。这并非因为模型不够聪明，而是整个行业长期陷</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716812&amp;idx=1&amp;sn=3852505d995e2852a9b0f1473f1eefd9&amp;chksm=972176ce5044dd93e87fcbb86a57d47afa067e691fbd335ce674ea5fb63f9e5fa6beb186bc10&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 14 Jan 2026 18:05:34 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 | AutoLink首创自主扩展模式链接，突破大规模Text-to-SQL瓶颈]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkBxFSia6STl2jhnRKmzicaiaCATgxpqTp2iaX116BMN3cj0TsH9X1eGIfw6TbWFMMPcyKHxLQJpu4LnQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>Text-to-SQL（又称 NL2SQL）是一项将用户的自然语言问题自动转换为 SQL 查询的任务，其目标是让不懂 SQL 的用户，也能直接通过自然语言访问数据库。例如，用户只需问一句：“近三年每个</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716812&amp;idx=2&amp;sn=b760f7ffdda80a8151a6f30c72f85fa9&amp;chksm=97c4800d4c0ef97aa3bcaef0fd8673cec0686802d6603f4f6ba2d0380c9a99953915eb70cb6f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 14 Jan 2026 18:05:34 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[殊途同归的第三条道路：DeepSeek用数学推导，撞上了Google的工程直觉]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgk4kdK0jXILJ0pGib4g3LGlsjmOibY8LC5PicbDuwTUUw8aaHuoqibBsVMGxQO0kHkYMRESGdzjkQia52A/640?wxtype=jpeg&amp;wxfrom=0"/><p>Gemma 3n 的技术黑盒，被 DeepSeek 的两篇新论文解开了。Google 在 2025 年 6 月发布 Gemma 3n 的时候，业界的反应分化极其严重。工程界惊叹于它在端侧设备上的极致压</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716764&amp;idx=1&amp;sn=e3b35a3d1d06f2c00613b807ddfee343&amp;chksm=97f27eec7c6af4a1849d365369445cd00d30d205d66db092fa7cc1f54aac88f6292f0a151264&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 13 Jan 2026 18:11:51 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 | 不再盲从弱标签！让强模型自主选择，阿里通义探索超级对齐新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgk4kdK0jXILJ0pGib4g3LGlsu0sasV3WKO8eovgbdKT9kf4ukU9WqTHpibI4vkJHGvNR5SGloCKg8Ew/300?wxtype=jpeg&amp;wxfrom=0"/><p>TL;DR：本研究提出了一种基于选择的弱监督对齐强模型方法，探索了强模型自主选择利用弱标签的解决超级对齐问题新范式。论文标题：Selective Weak-to-Strong Generalizati</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716764&amp;idx=2&amp;sn=a4dda29a9b9dcc5d7d5468893af82a44&amp;chksm=9718c8206854cf703c1db752aeeb89c12d091bba11ab530a075588d5d415d809ba0dd75c0905&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 13 Jan 2026 18:11:51 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[预训练数据太差怎么办？Bengio团队引入显式贝叶斯，无梯度实现In-Context RL]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnQzgIH4GANibQTcfclWXc9UibJjzGCXbicRE1QrgbWsS5eRvSp1jZwAmNZ1znmghq6JXgibyrpyaRnyw/640?wxtype=jpeg&amp;wxfrom=0"/><p>单纯拉长上下文并不能自动涌现强化学习能力，引入显式贝叶斯推断才是破局关键。在 In-Context RL 的研究热潮中，往往存在一种惯性思维，认为只要把 Transformer 做大，把上下文窗口拉长</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716706&amp;idx=1&amp;sn=861ccba253718964b98e2d76503eb879&amp;chksm=97f63bbf2cc1d45cb08147bb38584afec5ecc40c9ec28c5fbfcd0d10138bd8aacebc99b0084d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 12 Jan 2026 20:36:01 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[LLM竟藏多重策略？自动化所 × 腾讯揭示大模型RL多策略博弈新机制]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnQzgIH4GANibQTcfclWXc9URFSWEEQatOUleMhLqCOEiclJsPviaYE1157AvLQLmxqPsNmhCicdt3p8g/300?wxtype=jpeg&amp;wxfrom=0"/><p>当前，大模型+强化学习成为 AI 领域极为热门的研究。现有的强化学习（RL）方法通常将大语言模型（LLM）视为一个单一的整体策略进行优化，主要的算法优化集中在表层的奖励设计等方面，却忽略了模型内部复杂</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716706&amp;idx=2&amp;sn=809d1497d21be1545ec346143d05c924&amp;chksm=9735924552c0099536e0e4b2d7d1fdecf7ebf7bc0e0435e2a967bf739055be69a37d19d26a27&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 12 Jan 2026 20:36:01 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[OpenAI理论失效、μP失灵？邱锡鹏团队重新定义预训练两大核心超参]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglN94QowbsdJlgiayHUibbxLQHbursqEqnDxgftTvtwwXb6U6uaa8nhdaF02pwZZU3a4dpsrb8osqoA/640?wxtype=jpeg&amp;wxfrom=0"/><p>WSD 时代旧经验失效？复旦团队重塑 Scaling Law，让超参设置有章可循。在大模型预训练这项高昂的系统工程中，Batch Size (BS) 和 Learning Rate (LR) 是两个至</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716639&amp;idx=1&amp;sn=8f16cb94de76b1d7265674beb7558668&amp;chksm=9713590211aaacf39ecd81f6f9d90bbdf05e932a26e53e05494e0c40236353af7ed7cfbf9f6b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 11 Jan 2026 10:02:43 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[仅需8张4090！影石Insta360开源DA360，低成本刷新全景深度估计SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglN94QowbsdJlgiayHUibbxLQBEFcO6oTVzN9icQQ7FBJjqicCmIVQaNGyTBzH52NsJ5gbRUxo7F13OIQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>Insta360 Research 团队提出 DA360 模型，成功解决了全景深度估计在真实开放世界中的两大核心难题：零样本泛化能力不足与尺度不一致性。该模型通过创新的平移参数学习与环形填充技术，并延</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716639&amp;idx=2&amp;sn=10a5dc6768b0aa3a13e83d3b52ac5228&amp;chksm=97552634c7aec01d023196b6b846419ff3d1ddac22307e09709d084e029303adbdad77f290ed&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 11 Jan 2026 10:02:43 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICML 2026投稿开启：先别急着提交，详解史上最严的“连坐拒稿”机制]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgliaRd3B92u4vHT1HnHFKYJH6SJAf3kxuyMehe7X1ekYNDQ6qwEroP2rO5TjVCx85FRjKzKHlEWqHg/640?wxtype=jpeg&amp;wxfrom=0"/><p>刚肝完 ACL 别急着投 ICML，先读完这份避坑指南。就在昨天，ICML 2026 的投稿系统已正式对外开放。对于刚结束前两天 ACL 投稿的同学来说，现在或许正准备一鼓作气，将手头剩下的工作，或者</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716561&amp;idx=1&amp;sn=c8af2bb536d84d4428b985956a5c207e&amp;chksm=97d0c2b0295fac92f23e774db33e6b24499ff69e6ee37c3fe9c888375256f6b0d14f09c743e6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 09 Jan 2026 12:16:43 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[美团AAAI 2026中稿精选：破解过度思考与退火Scaling Law | 直播预告]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgliaRd3B92u4vHT1HnHFKYJH2GWcoibO9TpCo1Dm9bAiaeJlkCEgxoFCXtk6J08LHZ5QS8HiaXuQibu9LA/300?wxtype=jpeg&amp;wxfrom=0"/><p>AAAI 是人工智能领域顶级的国际学术会议，本文精选了【美团技术团队】被收录的8篇学术论文（附下载链接），覆盖大模型推理、 退火策略、过程奖励模型、强化学习、视觉文本渲染等多个技术领域，欢迎一起交流学</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716561&amp;idx=2&amp;sn=b52fc1a9c60324932a45ab5d274a9560&amp;chksm=978dc0ba0a10dce5fc7e36a7cd3c483893330fd499f14a2ab27da8d3c90a28ddbad7cd6218ea&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 09 Jan 2026 12:16:43 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[谁说思维链越长越好？Yuan3.0 Flash开源：砍掉70%无效token，重构推理范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgk2JdYDmyicVDrt8xzrlxPNTNbZQ5sEcJm4kzkkP6Opge7FQtnsAzJplxaN3BdNsK5XBicOL9tTgicIg/640?wxtype=jpeg&amp;wxfrom=0"/><p>首创「反思抑制」机制，让大模型学会在答对的那一刻果断停下。过去一年，大模型推理能力的进化几乎沿着一条单向路径前进：更复杂的推理过程、更长的思维链、更“像人类”的自我反思。在数学和科学推理等 bench</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716513&amp;idx=1&amp;sn=d9c9b248da7a28d6ff9399404ea0ab04&amp;chksm=97f632fd1945fda250dab20271d328dc2e45bf93f564762724082ff5b2113169274b8171649b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 08 Jan 2026 13:10:22 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 | 别再盲目采样了！OptScale实现概率最优停止，token消耗减半]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkO7yJELJPKBckFu8oMaNgGZgpV75yiah8IXibsejXZlWDn7SeAyAHlnZE6BfSYB8nia1GA1mCjvSaJg/300?wxtype=jpeg&amp;wxfrom=0"/><p>多采样 = 更强推理？ 在 Inference-time Scaling 成为大模型“最后一公里”标配之后，这几乎成了一条默认公理。从 Self-Consistency、Best-of-N，到 De</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716513&amp;idx=2&amp;sn=d21f65deaebc48aa53d1488d15f1bd7c&amp;chksm=970c4db06a2c23aaa2783da4a1fbdc300f76d45ccc033259800a59ed32319440b3c6438156d5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 08 Jan 2026 13:10:22 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[告别Scaling暴力美学：正如Ilya预言，算力不再是唯一的答案]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgk2JdYDmyicVDrt8xzrlxPNTuD3lhUkmBTyTFCLcgueEl1ITM3YUXwUtZkhzTib90GFt01vKfUOYVWw/640?wxtype=jpeg&amp;wxfrom=0"/><p>Scaling 的黄金十年已过，我们正重新踏入一片充满“惊奇与未知”（Wonder and Discovery）的探索之地。这是 OpenAI 前首席科学家 Ilya Sutskever 在 2025</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716450&amp;idx=1&amp;sn=2da55051e0e159009091ee61c2bea01e&amp;chksm=971628a60d143703ff235a21b07505ebc2067768f436f67433648ede8c693013e66b76e4f609&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 07 Jan 2026 14:04:25 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | Mamba与局部注意力首次碰撞，SegMAN刷新语义分割SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgk2JdYDmyicVDrt8xzrlxPNTkyfHhJKIIWGgJDuXwhVc66GEibicrjnAaK7H1BL4fyOyWstVAA259GaQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>摘要香港大学计算和数据科学学院俞益洲教授（https://i.cs.hku.hk/~yzyu/index.html）及其研究团队提出新型语义分割框架 SegMAN，包含全球首个融合动态状态空间模型（M</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716450&amp;idx=2&amp;sn=82a790e3c81879656749a97b0c3a3fde&amp;chksm=97fb6b77efba4401cc04adfb3a55f5beffc84cabf71382ddbd666ee968fc666e9534f464243a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 07 Jan 2026 14:04:25 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[别让 loss.backward() 成为黑盒：手推Transformer全链路梯度（含LoRA）]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm77InacOVfoHae9OBNuMTZq7acMLh8pwjtBvzCsgvZwMSJguLD0iaMJoFFcGxs1uMol3v1zmIBq5A/640?wxtype=jpeg&amp;wxfrom=0"/><p>硬核拆解 Transformer 梯度黑盒，从 Softmax 守恒律到 LoRA 微分实战。在深度学习框架高度封装的今天， loss.backward() 是一行魔法代码，它掩盖了计算图中数以亿计参</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716385&amp;idx=1&amp;sn=4889b56153225bed55c20f88595c2be0&amp;chksm=972b078c2ad719b1621167bd5e9ac4fdf212fdf14a008852736735e6886959722fd8cc3a0d36&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 06 Jan 2026 14:01:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[150k数据反超Qwen-2509！支持10图输入，MICo-150k刷新多图融合SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm77InacOVfoHae9OBNuMTZnkquUWaiao07lyLNzjb1lIGq8nqwObUZtR28roLA8nLHY4vpB5C2oBg/300?wxtype=jpeg&amp;wxfrom=0"/><p>项目主页：https://mico-150k.github.io/GitHub：https://github.com/A113N-W3I/MICo-150KOnline Demo：https://hu</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716385&amp;idx=2&amp;sn=1e59687f6012c01ce6f3e48621bd5e40&amp;chksm=97845c51f4b0e500145dc6c0a74901984621df660cda69b3fef5eb1d622a7be62232fd031a56&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 06 Jan 2026 14:01:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Vibe Researching来了！斯坦福教授实测：1小时自动复现PNAS论文]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnnMG4SmDAIVibKDWISlt7Edm4d28of5C5nzRRIMoyaubbbhUAprGtQWnXvgv3psqdumXmuS0W5e1A/640?wxtype=jpeg&amp;wxfrom=0"/><p>别只盯着 Vibe Coding 了，Vibe Researching 才是对传统科研的降维打击。当 Vibe Coding 正在改变代码生成的范式时，斯坦福政治经济学教授 Andrew B. Hal</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716309&amp;idx=1&amp;sn=bce0223da5c95147c5c021b7d0120f95&amp;chksm=97f8289f087367d650937a290b442c72fc0d9b8ebfc8bf1f3457a0808a1de41076bf0441b163&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 05 Jan 2026 14:34:38 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[大模型也能「千人千面」？UIUC团队提出个性化LLM路由新框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnnMG4SmDAIVibKDWISlt7EdIcQqJVtMAnuBWLD5iaianVy35gF4PsK6iaA79FpUKt7XmYZYObJ5wfkww/300?wxtype=jpeg&amp;wxfrom=0"/><p>随着大语言模型（LLM）的快速发展，我们正进入一个“模型选择”本身变得越来越复杂的时代。一方面，大模型数量不断增加，不同模型在性能、推理成本以及回答风格上差异显著。另一方面，在真实应用场景中，用户之间</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716309&amp;idx=2&amp;sn=5bed765fb3b34730fb0d9f37b02e31a8&amp;chksm=974321e753ba3c1046af18af621620554b3c21c320377096ac61147f319e4ae20e529c67025a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 05 Jan 2026 14:34:38 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[别再把KL散度加进loss了！Bengio团队实证：回归Reward才是无偏正解]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglCfnY2AWCQiaSXiabFbfl6bQ3TeNvWuXL3ia5p0npcqDETvLI479phoSHRicvcUlBQh3Pwzcq0cmJaFQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>全网都在卷 RLVR，但 Bengio 团队刚泼了盆冷水。DeepSeek-R1 的爆火让 RLVR 成为当下大模型后训练的绝对主流。无论是 PPO 还是近期大热的 GRPO，核心逻辑都是一致的：在最</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716247&amp;idx=1&amp;sn=584edb6041579a68d396bb9a3f0c278f&amp;chksm=97ffa84c96766ed87c4739062912cdbb3f2b1e35fb90d912b2c592dd288e2c8bef1124a23bbb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 04 Jan 2026 14:10:04 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[爆肝96页！NUS联合哈佛发布医疗智能体重磅综述，28万字+300篇文献梳理]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglCfnY2AWCQiaSXiabFbfl6bQ770iaG2iaZZSbxIV7jhIbDmsdjfeuEPKicBCAZHZ7PcjNsib9V8Bk60nWw/300?wxtype=jpeg&amp;wxfrom=0"/><p>©PaperWeekly 原创· 作者 | 钱云航单位 | 新加坡国立大学医学智能体面临着数据隐私和安全、系统的互操作性、临床决策的透明性，准确性和可靠性等关键问题，对患者的健康安全构成严重威胁。针对</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716247&amp;idx=2&amp;sn=401d2a841eb11c34c48fd643eccee479&amp;chksm=9748905958ac7024f1249c8c3640db2362bb9d46d5b9aed70d95a26c850f72e31c9e32f13539&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 04 Jan 2026 14:10:04 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[2026年的大模型范式变了：告别KV Cache爆炸，递归语言模型才是未来？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmokAvT7IRlmhoUvdv66Gu2rckQg3V40PRBRU4t5KKkl0AVRHsG3lCWJiamicpOqFqZVAvicp4Ng5Xfg/640?wxtype=jpeg&amp;wxfrom=0"/><p>2026 年的 AI 范式，或许已悄然剧变。在过去的一年里，我们目睹了上下文窗口（Context Window）的疯狂内卷，从 128k 到 1M 再到 10M。然而，这种基于 Transformer</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716168&amp;idx=1&amp;sn=71d78876c52422c64f6d5f36bd240090&amp;chksm=971edbd252109deb6a9498d5909f44a202379885cddc0f45e349db2e89320b21b4cd1e3eaaee&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 03 Jan 2026 20:09:01 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 | 格式即先验：量化和分析大语言模型在异构数据中的偏见]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmokAvT7IRlmhoUvdv66Gu2m8vNeTh8hwibOzpn4LYxAWibzekbwTX0lgTyu6XxbhRUa1R38rfkBlUA/300?wxtype=jpeg&amp;wxfrom=0"/><p>随着大语言模型（Large Language Models，LLMs）在问答、推理和决策支持等任务中的广泛应用，越来越多的系统开始引入外部知识以缓解幻觉问题并提升推理能力。这些外部知识通常以多种异构格</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716168&amp;idx=2&amp;sn=f6657bafb773ea298f22b11fc7fced22&amp;chksm=974abebbda022d443e0ce43e2e9e783480af89a60735a98aac285fe2add28f950ad7ad5daa15&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 03 Jan 2026 20:09:01 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[字节提出的“Hyper-Connections”，被DeepSeek救活了？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgn4s8NrDm8UWA30XQCqjZAp38aGCcvlugcO5W2t72hjI5l95zFGdYojelSH7oH3hcOicC0lPKiabnKg/640?wxtype=jpeg&amp;wxfrom=0"/><p>当字节的 idea 遇上 DeepSeek 的数学洁癖。DeepSeek 似乎养成了一个习惯，专挑节假日给大家上强度。当大家正忙着庆祝新年时，他们悄然在 arXiv 上发布了一篇硬核论文。论文标题：m</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716108&amp;idx=1&amp;sn=9557b1b31d99b433d0d7f501aed89794&amp;chksm=9768f4b53ab99c9554a230dc7fb44e55a7c85bc2060acea6f3fbee32c3b02636a65010783f64&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 02 Jan 2026 13:51:40 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[优化即几何，几何即推理：用数学终结Transformer的黑盒时代]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmvQDasddbj0Phadjb1JPicOeteibA1RaQoCCVRJrcn8mvHorbibw4ztak5eA6tI3QCeWM4df9xs9Bkg/640?wxtype=jpeg&amp;wxfrom=0"/><p>不是设计，而是进化。当交叉熵遇见 SGD，贝叶斯推理成了唯一的数学必然。长期以来，LLM 的推理能力被视为一种难以解释的“涌现”。我们目睹了 Loss 的下降，却难以透视参数空间内部发生了什么。近日，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716057&amp;idx=1&amp;sn=d2fb93dd5d9e2cc0fcf9cb11e5bc99ec&amp;chksm=97ef625db727998acfcab90e06fb95df18d4fc23d2eea66b5bb1174fc2ddebae48d00485664e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 01 Jan 2026 20:18:10 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[重构通用异常检测新范式：Dinomaly2实现跨模态、跨任务的无缝统一]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmvQDasddbj0Phadjb1JPicOP5VTNoAM9uNqgcsqEUhCSRtE92XNNEbZPcnxlVs1mEYkrzpbKsAjMw/300?wxtype=jpeg&amp;wxfrom=0"/><p>重磅更新还记得在 CVPR 2025 上首次让多类别异常检测（MUAD）达到单类 UAD 模型水平的 Dinomaly 吗？现在，Dinomaly 进一步进化为了 Dinomaly2 —— 一个真正实</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247716057&amp;idx=2&amp;sn=a742874b44a9e11291067bbc5717d6e3&amp;chksm=974df97ad9df82e2673ddd98849f0efcc4d0f78118685d028fe11c392f186c5cd32de9778b01&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 01 Jan 2026 20:18:10 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Attention Is Not What You Need? 用格拉斯曼流形重构序列建模的几何美学]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm3dlKEj0qLxOpShm0PWPNcTvUGmNSB35vY3KancjghMVFWX0TyttMc7cx3Oicmej2c0H9vicCA4XXg/640?wxtype=jpeg&amp;wxfrom=0"/><p>Attention Is All You Need 喊了这么多年，是不是把我们的思维都禁锢住了？自 2017 年以来，Self-Attention 几乎成为了现代序列建模的绝对基石。我们早已习惯了通过</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715986&amp;idx=1&amp;sn=c904ae63b234e1e3085033f91b94c38e&amp;chksm=9775a951787535153252a38b8a36a9e5aa38ba4dd2803218c1e9a3e96076f31ec2ced4808629&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 31 Dec 2025 12:36:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[8B模型超越Gemini 2.5 Flash！南大&amp;腾讯用TimeLens重塑大模型视频时间定位]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgm3dlKEj0qLxOpShm0PWPNcElbPuVKzrztncu3YcRllP1MqdZJicU4wMqiaibRScKAsUyEjy63DpEt6A/300?wxtype=jpeg&amp;wxfrom=0"/><p>南京大学、腾讯 ARC Lab、上海 AI Lab 联合提出 TimeLens，针对基于大模型的视频时间定位任务，从数据和算法两个角度进行了系统性的重新思考。通过构建高质量的评测基准和训练数据集，并提</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715986&amp;idx=2&amp;sn=9844d275a36a0a1602114f197bfab49b&amp;chksm=97ba717b4fae89f3c8beebac801c0684aa77768afffd9752c64074ca3db6c307444a637b0f64&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 31 Dec 2025 12:36:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Mamba还是Transformer？Bengio给出第三选择：Phalanx完美替代局部注意力]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglViaIjmdmDviccYyX1F6m7lQgY32wp0yJaCbQQDIFnSHyMKBUFxrbKiaBqibxDCcWUNsXoQ2XU51BEEg/640?wxtype=jpeg&amp;wxfrom=0"/><p>比 Transformer 快 24%，无损 SOTA。在长序列建模领域，Transformer 架构凭借其捕捉全局依赖的能力占据主导地位，但其 的计算复杂度始终是扩展上下文长度的主要瓶颈。为了突破</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715917&amp;idx=1&amp;sn=9a2ec7bcfe522080a81d34d4d890705a&amp;chksm=973585d4789229cdea27400885d1c8b6ae947a6d18c375127bcbb8a80ea343e040613307c573&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 30 Dec 2025 14:08:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[华为重构Transformer FFN：首创宽深自适应复用，零增参超越MoE]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglViaIjmdmDviccYyX1F6m7lQxLbheOZurdZYxo7Fexq2X4qibyHTpIAKlFibicLHUiaJictllj4PrjHwJlw/300?wxtype=jpeg&amp;wxfrom=0"/><p>在大模型 Scaling Law 依然奏效的今天，为了追求高性能，模型参数量动辄千亿甚至万亿。然而，随之而来的显存墙成为了阻碍模型落地的最大拦路虎。现有的剪枝、量化技术虽然能压缩模型，但往往以牺牲模型</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715917&amp;idx=2&amp;sn=dacadccd1c8fb9be569a32448128a615&amp;chksm=97591b92499cd6f86381b9226187e01aa9531a21ce397caf63a8cb3c9f053080eb62ca761bf1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 30 Dec 2025 14:08:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[比Mathpix更强大的公式识别神器，全免费！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgl22pvdXucYpYeanHhfgbXicHlPnyF0YzDlRYQKCqhaiaicItVo5h1bakpc80eZYt2gAop7TIDTUINibg/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天这篇文章大家一定要仔细看看，说不定不仅能帮你省下不少钱，还能让科研论文写作事半功倍！本周末，在忙于项目的间隙，朋友突然给我分享了一个新发现。他说，PaddleOCR 最近推出了一个新模型——Pad</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715848&amp;idx=1&amp;sn=3308225390d93222a80f7cafdb9f0145&amp;chksm=97ba909549dfb1ffe3f97f35b4d659afb65b5475eacad9f03dd90fecdf2dfe76690964827ce5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 29 Dec 2025 19:51:45 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[中科院 × 北体大提出SportsGPT，打造懂专业、会指导的AI教练]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgl22pvdXucYpYeanHhfgbXic3Q5nxWaB6GOibBB8yxUpl0qp4AK8AWSF3pXZ1u1BCxtQk39VX5ibM4kg/300?wxtype=jpeg&amp;wxfrom=0"/><p>在 AI 席卷各行各业的今天，体育圈的“智能化”走到哪一步了？现有的智能体育系统，大多还停留在“打分+可视化”的阶段。屏幕上画出的骨骼线很酷，但对于运动员和教练来说，往往面临一个尴尬的灵魂拷问：“我知</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715848&amp;idx=2&amp;sn=73d75ff4d832ddb341a38d77d23d8133&amp;chksm=97b901fc80a37c64a1063be1312c7d1c46e7565a28e951deffba81f28eccf4aa68d3b64675eb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 29 Dec 2025 19:51:45 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[RoPE真的完美吗？LSTM之父团队新作：极坐标解耦，零样本无限外推]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnFysJ4ibeUcDBgRDWqjaM7GZtDS7ZneLXuywvLmWnZFqQ9bdvLrEOib2RzANp1g8tdxvoibBCQ5QLLA/640?wxtype=jpeg&amp;wxfrom=0"/><p>告别长文微调！Schmidhuber 团队新作修正 RoPE 理论缺陷，原生支持零样本无限外推 。在当前的大模型架构中，Rotary Position Embedding (RoPE) 是处理序列位置</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715528&amp;idx=1&amp;sn=adba52c259398a566cb7a1ede6374049&amp;chksm=978e5e0e4753e6ea1f046f86687662fc0b9b2a95f7e7750d472e968d3065568d247f41d5b040&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 27 Dec 2025 18:07:01 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[加速流式视频理解！上交团队实现ViT编码与LLM预填充双重加速]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnFysJ4ibeUcDBgRDWqjaM7GHFoNnvMcSDUyHB3MFf1ibIicY1GYGHvibIhtnfvKe9jzZMqJb1fxzs9zQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>随着多模态大模型的爆发，视频理解（Video Understanding）正从离线走向实时流式。然而，高昂的视觉编码成本和不断膨胀的 Token 序列成为了实时部署的拦路虎。近日，上海交通大学 EPI</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715528&amp;idx=2&amp;sn=4b69f1d61fbe920abbdde6abc147427e&amp;chksm=97fadbcfca495f2434dd81953d20ccd6164398e60e08c5737fe8b1f620db513daef419cdcd65&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 27 Dec 2025 18:07:01 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[别再怪SFT了！清华揪出0.1%幻觉神经元：大模型胡编的尽头，其实是过度顺从]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgll3fZ1kiaS8RicbY3GW3zAzKkicKPccsEjoxUQE1IPtAJsRqcgTWocHI9kVKYP7b4b2ftf8tTiaIiaB5w/640?wxtype=jpeg&amp;wxfrom=0"/><p>无需重新训练，只要抑制 0.1% 的特定神经元，就能让模型“闭嘴”？近年来，大语言模型（LLMs）在问答、推理与生成任务中展现出卓越能力，但其幻觉（Hallucination）问题仍然是制约实际应用的</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715466&amp;idx=1&amp;sn=2d0356ae27872941516dcd58eb48c943&amp;chksm=97931cf262c99970a5ea52ef7e1801f7424f44b78109ecdda64aaf6c3c29ca8d3d622c9b319c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 26 Dec 2025 13:05:32 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[不仅是RAG！NUS、人大等联合发布：102页综述揭秘Agent Memory机制]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglN0FZVr3ibnTupwNph2KW9zmFiav3mMtXPQrdDLQPKfsa1fwOhOmecAiaNkbzSeXmLO7iaJxHqVDtP4g/300?wxtype=jpeg&amp;wxfrom=0"/><p>告别 RAG 碎片化，从 Forms 到 Dynamics，一文讲透下一代智能体核心架构。随着大模型能力的持续提升，Agent 正逐渐从具备推理能力的语言接口演化为能够长期运行、持续交互并执行复杂任务</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715466&amp;idx=2&amp;sn=ad43d97a2b9d8def83c94e20d9f12cc4&amp;chksm=9705bf921a7211f7894e257a5e8c78827bcfd6ab960f012028af15f041b0c8344fd8242c51b6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 26 Dec 2025 13:05:32 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[致敬经典！手搓3D版《Attention Is All You Need》，M2.1只用了3分钟]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgll3fZ1kiaS8RicbY3GW3zAzKFCYRobzv4O0BwCChLcXu8MFHicvibelIjZw19UmD1xc1oPh8NvYPhIGw/640?wxtype=jpeg&amp;wxfrom=0"/><p>不写一行代码，测出 M2.1 的全栈极限。2017 年，一篇名为《Attention Is All You Need》的论文横空出世，Google Brain 的 8 位作者可能未曾想到，这篇论文会成</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715426&amp;idx=1&amp;sn=f0709fb87fbdb774e8cf1284b67b24ec&amp;chksm=97036888ede8d2c9ea2d408eeb047dcc68804216ef83cded2c214a9db22474d6df9e19649baa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 25 Dec 2025 18:16:18 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[告别静态刷榜！CATArena开启“技能五子棋”模式：顶流模型互写代码大乱斗]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglQ7aCIk2c0dQ26TnOyUCZqEr6uMFeYSb12vBnCziavMBhz4ZJblic2kO7MJ8PbbQWK2hVSgjCqkTicw/640?wxtype=jpeg&amp;wxfrom=0"/><p>Talk is cheap, show me the code.在 MBPP+、HumanEval+ 这类静态代码评测集上，大模型们早已杀红了眼，分数卷到了 90+，个个都是满分做题家。但我们都清楚，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715338&amp;idx=1&amp;sn=bc2148527e0be0a269ff996a292d9eb4&amp;chksm=97f486856d1eb8cbad91e9a35ba0ee1881dc37d4cbf43032f1b02c2636b1864dad31e4471a96&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 24 Dec 2025 12:05:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[用户行为预测的“专注力革命”：FAIR让生成式推荐不再分心]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglQ7aCIk2c0dQ26TnOyUCZqH9Van3G43iakUEwkSLGwrBsBjrv89FFMXicicSQuDniaXd1CMFiccMp8IpA/300?wxtype=jpeg&amp;wxfrom=0"/><p>©PaperWeekly 原创· 作者 | 肖龙涛单位 | 华中科技大学博士生研究方向 | 推荐系统你有没有想过，推荐系统在预测你的下一步行为时，到底看重了什么？现有的生成式推荐方法将用户的历史交互拆</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715338&amp;idx=2&amp;sn=bb7e7f8eaf45d8774faeac691cd187ae&amp;chksm=97e5cdc98d596e74933ee3dcdfd589e7a3f7c46f38429cd270ca99af9d532c1745048122ed48&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 24 Dec 2025 12:05:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 从“唯Key论”到非对称解耦：利用KV差异重塑长文本推理]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmYD9A59EVdxXyIxibA7Dk21iafKzJweNnH8b6FBbcCEECialyPP99s1k0wslBwiboDskYBNgBLs6CP9w/640?wxtype=jpeg&amp;wxfrom=0"/><p>©PaperWeekly 原创· 作者 | 崔万云，徐明威单位 | 上海财经大学现有的长文本 KV Cache 压缩方法普遍受限于“以 Key 为中心”的工作范式，即隐含地假设 Key 的分布特征完全</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715249&amp;idx=1&amp;sn=8e533d931e810077e2c1e1442c242ac3&amp;chksm=974236d2e1d07fd0bc2e7d0dc264e6efce37315b46ae012359fc95bd7da79b37287d7248901c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 23 Dec 2025 14:30:25 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[视频衍生数据集来了！港科大×美团开源OpenSubject，专攻复杂场景生成与编辑]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmYD9A59EVdxXyIxibA7Dk21KHKJFcGonDm1f7hMERQibOR6mdQ7o0dMcfOee6exb9OlOmgYeyXHIRg/300?wxtype=jpeg&amp;wxfrom=0"/><p>香港科技大学与美团联合推出开源项目 OpenSubject。该项目基于公开视频构建了一个超大规模主体驱动图像生成与编辑数据集，涵盖 250 万样本、435 万张图像，专门面向“指定人物 / 物体的个性</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715249&amp;idx=2&amp;sn=fbabb3a39c1bf195e32e824a13d10188&amp;chksm=97145abd1acb7a57c1f4a8b0b04f7fc23286b427e1956ae452bf4a00e6cfe4de6ff12181ef11&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 23 Dec 2025 14:30:25 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Claude二次创业实录：明面上买PS5搞破产，背地里差点倒卖洋葱去坐牢]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkicen4Bw7QgkBTXSV2j7JIODFj3A3olIrmjfaAzaDQa2mrapo7MfsXhPYVtQnCwUal3VibkGIxleDg/640?wxtype=jpeg&amp;wxfrom=0"/><p>把公司交给 Claude 会怎样？答案是：先破产，再修仙。在 AI Agent 被吹上天的 2025 年，Anthropic 和《华尔街日报》联手整了个真·大活。大家都在畅想以后 AI 能帮我们打工、</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715207&amp;idx=1&amp;sn=839c435c4a1d25643b6f5e2eb470c0d4&amp;chksm=9705fc43bdbbe2a0c6ecc3223baf4c43008094a54a2bc593e2c89e3c4b6a6365635382a632f5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 22 Dec 2025 13:58:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[MiniMax海螺首次开源VTP，Tokenizer才是视频生成Scaling的新主角]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkicen4Bw7QgkBTXSV2j7JIOvUicy01p3H9CbVm1H58ghm9llzpkxqMqdlT7iccw8DrSozC0keWQlx9g/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天，我们很高兴向大家介绍 MiniMax 视频团队刚刚开源的工作—— VTP（Visual Tokenizer Pre-training）；这个工作讨论的是视觉生成模型中的关键组件—— tokeni</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715207&amp;idx=2&amp;sn=f8a958589f2036f819680d1ad226d302&amp;chksm=97cad3f8048da60dd0a80bbf43c2c974be60f69ec20f36c02a34c1b2a8ad802a9824657475ce&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 22 Dec 2025 13:58:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[会走会聊还会求抱抱！迪士尼造出“真”雪宝，把热力学公式写进强化学习]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglwRU4T3KEMRkUUtTz2ibrzApRucTEu4gbicJUX8g9yAv8F1t9By4dV4oB3cEpp15DO46ObX5826MqQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>迪士尼“真·活”雪宝机器人来了！不仅会走会聊，还能卖萌求抱抱。在机器人领域，我们习惯了波士顿动力的 Spot，它们为了运动效率长成了狗的样子。我们也习惯了扫地机器人，为了实用长成了圆盘。但在迪士尼的世</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715129&amp;idx=1&amp;sn=5b2555673e3fba6fcf8ddbb4e3f76a41&amp;chksm=97c4ac292401548943a99fb38d2498cd3ba7cab4fff47325bbf435e6185c500eaab0d24f81a1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 21 Dec 2025 12:31:16 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[身份保真比肩Nano Banana！ContextGen统一上下文，实现布局与身份协同控制]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhglwRU4T3KEMRkUUtTz2ibrzAFDk60w0b0ibwZyWvJ1icwt5HzrBzzjTtHIdLkcP0Qia1xV3xQPia6oiazicQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>©PaperWeekly 原创· 作者 | 许瑞航单位 | 浙江大学本科生研究方向 | 计算机视觉与生成模型近年来，扩散模型（Diffusion Models）在图像生成领域取得了飞速发展，尤其在个性</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715129&amp;idx=2&amp;sn=b205503ec8bf001e610e0b2bbae40344&amp;chksm=9720fecbb5e436ff99c7434c13f458efefd2b63e3861b15fe72bcad0dc434a4deba16a387cc7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 21 Dec 2025 12:31:16 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[OpenAI官方论文“泄密”GPT-5：RL到底有没有教坏CoT？万字深度实测]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnmLS3sIkeJ2CPxeWRofHtibdXtYXVUFXK9icnLM28nDpftBnzah9P4jGb0nlYZ5zHlEZiajVBL7HAcQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>你的模型是在真思考，还是为了讨好 Reward Model 在演戏？随着 OpenAI o1/o3 系列的发布以及 DeepSeek R1 的开源，大模型正式迈入了 System 2 慢思考（Reas</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715048&amp;idx=1&amp;sn=4d77441dd965f7a80b4f2492d315f4be&amp;chksm=9774f18e37f1f2f8d573848f161cb0a1ac6ddc87f95aae028ee2ddc389a4691c7d88d96323aa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 20 Dec 2025 14:10:33 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[LLM符号推理全景综述：迈向可验证、可解释、更可信的大模型推理范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgnmLS3sIkeJ2CPxeWRofHtibV5DXj04JLWYxKp0ZRjCnDh49vBjobLBEK0dCX8oicKYcGQFicEKBP8uQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>自然语言推理与符号推理长期各具优势与局限。自然语言具有灵活表达能力，但缺乏严格的逻辑保障；符号推理强调语法结构与逻辑一致性，但难以覆盖复杂开放任务。LLM Symbolic Reasoning 则在两</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247715048&amp;idx=2&amp;sn=f5e1d32df61428e5d80a3042bfeeb1c3&amp;chksm=9757e25ba3fde0a93e81958ce6e101ebea4756e8a7326876324a117b0c36c43f9afe16b6ac6e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 20 Dec 2025 14:10:33 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[拒绝蜡像感！美团开源LongCat-Video-Avatar：5分钟超长续航，虚拟人终于会呼吸了]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkoFz6lYah2go7oT4SJiaF54SBM1eGtib2v5PjLlLUfibiadEqgX3PRkL9icVBNYYel541Tq6fFialo2ZSg/640?wxtype=jpeg&amp;wxfrom=0"/><p>捅破 5 分钟长续航天花板，SOTA 级权重直接全开源。2025 年，视频生成赛道已经进入了卷细节、卷长时序的深水区。当行业已经能够产出几秒钟极具视觉冲击力的镜头时，实际落地中却总会撞上几堵隐形的墙：</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247714990&amp;idx=1&amp;sn=c2ae883e81a746852a074be4b732fd92&amp;chksm=97ffbe4ac9e91c2f34cd931ea622421cbd3cd2f14e89f4ed97f898a45cc182679d24021ba9a0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 19 Dec 2025 14:20:47 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 | 当知识图谱变成乱码，LLM还能推理吗？ARoG破解RAG隐私困境]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgkoFz6lYah2go7oT4SJiaF54FUZjhagYPR1eYQzh5jwTCOGGficce4XeB4GmbuUUFPiaVvg7OYwgQ9lg/300?wxtype=jpeg&amp;wxfrom=0"/><p>研究背景大型语言模型虽能力强大，却常受困于事实幻觉和知识滞后 [1]。检索增强生成技术通过引入外部知识源（如知识图谱 KG）来弥补这些缺陷，已成为提升模型可靠性的关键 [2]。然而，当 RAG 系统需</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247714990&amp;idx=2&amp;sn=9bfe289e5079f0b2812cc33d27c73ed0&amp;chksm=97df004b5106a62cf4e68a1a44378cd8b99927046be4e476b4f7668cc247460425838ff8fa92&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 19 Dec 2025 14:20:47 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[RK-∞降维打击Mamba？线性注意力真的有“免费午餐”！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmeQ1n1sSTDx250BohhOxiauKAychVnYb8QibicWibLlrHVKyCKpRqZIibib9lCDZEhaHgj0Rk6bFZzadlQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>这顿“免费午餐”，或许正是通向长文本高保真建模的下一块基石。在大模型迈向超长上下文的当下，混合注意力（Hybrid Attention）已成为 MiniMax、Qwen、Kimi 及 NVIDIA 等</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247714906&amp;idx=1&amp;sn=88f461e412fee53526da9f10e611d8ba&amp;chksm=9722cada117cc18f1478c0e395296286c0033c638be1d31b250967b4b63339758d5c2ee0dc59&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 18 Dec 2025 17:37:27 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[为什么你的多任务模型总在“打架”？解决融合冲突的终极方案来了]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/VBcD02jFhgmeQ1n1sSTDx250BohhOxiauQBoWWNkicvEAr6Y4VY6sVmYSUFr3l8ibMBPQas6fn7xvjeYztKNHAicCg/300?wxtype=jpeg&amp;wxfrom=0"/><p>“预训练-微调”已经成为 AI 应用标配。然而这却带来一个难题：为不同任务微调的模型数量激增，维护成本与日俱增。我们如何将这些“专才”模型，高效地整合成一个强大的“多面手”？模型融合为此提供了一条路径</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247714906&amp;idx=2&amp;sn=72f71ef785d78805e744038be0cbcbe8&amp;chksm=974b3e8b69ded1c8ad5cff567f6452027b4c8e8d43351f21f9e2fcf87a927a0864398531b7f8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 18 Dec 2025 17:37:27 +0800</pubDate>
    </item>
  </channel>
</rss>