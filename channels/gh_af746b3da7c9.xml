<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_af746b3da7c9.jpg</url>
      

      <title>gh_af746b3da7c9</title>
      

    </image>
    








    <item>
      <title><![CDATA[三张图速通 DeepSeek-R1 论文和技术原理]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajGDDc7DghcxMB72qiagJMPewpYA6Bibn9aUOiamibBqz7scibQ2Cwbib9WibYKxSL7L5QTuKhLKXhVomz7A/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：木尧（已授权）链接：https://zhuanlan.zhihu.com/p/20538667476编辑：「深度学习自然语言处理」公众号总览最近在研究和复现 DeepSeek-R1（671B 参</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535912&amp;idx=1&amp;sn=fbe0396d92c2f6f522a25083e8a28397&amp;chksm=eaf20ef93e4368203c9e39acc9794adf7e074113225c3b9c766ac6a2c6e59b1aa83db06631dc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 11 Feb 2025 12:21:40 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[2轮推理赶超已对齐模型？TPO：无需更新参数的对齐新思路]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajGDDc7DghcxMB72qiagJMPeeEYZN14U6eyVWylfJdicyo1by1grSXVZQ8K1sCwGotAh3bnU4R3xLhw/300?wxtype=jpeg&amp;wxfrom=0"/><p>传统的对齐方法（如RLHF和DPO）通过更新模型参数来最小化损失函数，但需要迭代重训练，限制了大语言模型（LLM）对新数据和新需求的快速适应。本文提出推理时偏好优化（Test-time Prefere</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535912&amp;idx=2&amp;sn=9914f8faaaeb585a7bd8e983cefe6e86&amp;chksm=eac43245c51ad797fd4e05f661c90e5557375a3302ded9fa636337baefb5bff17bbd970beadc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 11 Feb 2025 12:21:40 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[聊聊Reasoning Model的精巧实现（ReFT, Kimi K1.5, DeepSeek R1）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahKL7Pib1C9FI0yjtoy2xfj46fHY8yH0KxTK6DzpsseDGfMYciarKCRTDmQ4DKOkY7l27vSoa2acvEA/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：姜富春（已授权）链接：https://zhuanlan.zhihu.com/p/20356958978编辑：「深度学习自然语言处理」公众号引言最近Reasoning Model（推理模型）异常火</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535834&amp;idx=1&amp;sn=cd73a09d67369d94087094b43846c623&amp;chksm=ea4b79e930d2a875919cd8e1677d5e3cd8ecc57c948fec863ee900416d0c0a68dab40b16dfc2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 13:35:31 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Speculative Ensemble - 让大模型集合推理飞起来！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahKL7Pib1C9FI0yjtoy2xfj4cLG4ia6S4FAIqP7KMfk8P54uAicdWyFS1dUo9ldVqIsVBcI4txt3fCUA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天我们要聊的是一篇关于大模型集合推理的论文。大家都知道，大模型集合（Ensemble）是个好东西，它能把多个模型的智慧结合起来，生成更高质量的文本。但是！集合推理有个大问题——慢！就像你等外</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535834&amp;idx=2&amp;sn=c331d6ff2ab3928141a86fa29887ea84&amp;chksm=ea392d8ec3ff00319fcc6d35aa5e497d36f25e671621a7bf56d5abfbc2738a50a36ccb159fe9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 13:35:31 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[国产模型再秀硅谷！阿里Qwen2.5-Max数学、编程能力全球第一]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bah6KiaxqiauTgvdWic4UChZUQV5iaI0gD21iaEooGoYZPmYoLYKbwGRBYsSiatNNK6TffaCoticziaPxvlN8A/640?wxtype=jpeg&amp;wxfrom=0"/><p>春节期间，咱们国产AI大模型就给全球网友送上了一份“见面礼”——阿里巴巴的 Qwen2.5-Max 正式登顶 中国最强，更是在 Chatbot Arena全球排行榜 上杀进 前十，超越DeepSeek</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535784&amp;idx=1&amp;sn=3871937d172696a7959df1c45179e772&amp;chksm=ea02560062bac0bb0e3efe514f9114f8480a2a9ce78690182d57bdd29465c571271ff90fa0d1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 07 Feb 2025 04:18:06 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Deepseek R1 Zero成功复现, 三阶段RL，Response长度涨幅超50%，涌现语言混杂，double-check]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahuZvEI97znd0G0l6VouDSVxIo0C0MqyGAOqdvcOiblKBAKFibvNVd1VpepK04tZthkWNqBNoRcPqfA/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：涮月亮的谪仙人（已授权）链接：https://zhuanlan.zhihu.com/p/21290410831编辑：「深度学习自然语言处理」公众号项目代码可见：Unakar/Logic-RL(h</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535692&amp;idx=1&amp;sn=b91e950bd4132e79756c6edf3f3b6963&amp;chksm=ea995e425920598681b5d3ae3287c356a7d9eb2333015ebd204ed97a2b8b8c4261e25ef15431&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 05 Feb 2025 08:22:14 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LightTransfer：将你的LLM轻松转为Hybrid model，增强o1-like长文本生成能力]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahuZvEI97znd0G0l6VouDSVbtz438qbnbIxP1ZcicicSG3Ygb4iaoRrVK6UOWW4Iia8kN7jjndICKoaOA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近hybrid model的有效性已被广泛验证（比如minimax-01和gemma2等）。该篇论文研究了一个非常有趣的主题：如何将预训练好的dense transformer（如qwq）转化为hy</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535692&amp;idx=2&amp;sn=eda5ead962e3e136ead35a2abe6593a7&amp;chksm=ea15139569faea05e0704cebf75eb8183fa8cc42297f7b7873f40bbb77ff05ca834359b02f8d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 05 Feb 2025 08:22:14 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯AI Lab联合苏大上交提出：少切思路多挖矿，让o1类LLM做题不再「三心二意」]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajhH7bBxWoicqmXpSvpJ1pgvgibt1ayicoldevaKYCO4GjhTI2snOzsewRz1Zwic2lmVI4IPby9ibwZXyw/640?wxtype=jpeg&amp;wxfrom=0"/><p>想象一下，你班上有个超级聪明的学霸，但他做题时总像得了「思维多动症」——一会儿用代数算，突然又切到几何法，再蹦出个微积分，最后…答案错了！这篇论文抓到的正是大语言模型（比如OpenAI的o1）的这个小</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535645&amp;idx=1&amp;sn=d8575ba255fe80a320f670b114a4ce40&amp;chksm=eafd563db4c33bb9fe3900364549708ea6309c9b567ee49602d514a456d7b4f06fcf87133462&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 04 Feb 2025 08:33:34 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[新突破！xJailbreak：用强化学习「越狱」大模型，可解释性黑盒攻击来了]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajhH7bBxWoicqmXpSvpJ1pgvpSQOuBrOKZbaJnUSiaYEicWnBLUibjkMSWnwtQQdsS4XFGuceic6DfSfjQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自原作者团队投稿编辑：深度学习自然语言处理大型语言模型（如 GPT-4）虽经过安全对齐，但仍易被“越狱”。现有黑盒攻击依赖启发式算法（如遗传算法）优化提示词模板，缺乏可解释性且效率无法保证；白盒攻击</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535645&amp;idx=2&amp;sn=75fd9427555455ca1956d87efe5a166b&amp;chksm=eaba8fc533be052178e9f1b5874290bf8eadb7895291a387d651624f06f3cc20f614adaca735&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 04 Feb 2025 08:33:34 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
