<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_af746b3da7c9.jpg</url>
      

      <title>gh_af746b3da7c9</title>
      

    </image>
    













    <item>
      <title><![CDATA[2024 年度总结 LLM System Research：过去半年的科研心路历程]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bah09xp2rHB7hoicS00IGHQYNwMnI76mrnxcIF7ny5cmTaE3Qeoje3ULI5r0LF0d6ic3JhJw1zRHiaTlg/640?wxtype=jpeg&amp;wxfrom=0"/><p>飞往SFO的沿途风景，Shot on IPhone恰逢年末年度总结盛行，回国无心科研，我便强迫自己分享一下自己的过去半年的科研心路历程。目的有二：1. 继往开来，学有所思。2.受东川路第一伊蕾娜：年度</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535132&amp;idx=1&amp;sn=6b2e1136873251d8ad4563659594e6da&amp;chksm=eac10533e11934d08bbfe4fe49218964ce617cc1c13bed9a103f4e27ed3076313f41ca40d22d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 09 Jan 2025 14:04:46 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[24届毕业生聊PhD就业，国内or国外、学术界or工业界or创业]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagiaB4FmoGYPJSHLDr60qorJia4uzJwSzlax1ibCYGcJzPW9ibiadDSjVhC2icRsvjnH6iaylWraI1EQetzg/640?wxtype=jpeg&amp;wxfrom=0"/><p>主题PhD如何择业之国内or国外、学术界or工业界or创业 时间北京时间 2025.1.11 10:30-12:00内容PhD怎么找工作每种工作的好处和坏处工业界和学术界的抉择国内和海外的抉择张林峰 </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535080&amp;idx=1&amp;sn=eb45ef1e36d4acbbdbe39b0c3e078d35&amp;chksm=eaccd735e49e24074f9b0fe0794e796f7bf35eb4a18321013cf39106a3b3de71fb0dc21daba8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 07 Jan 2025 12:05:09 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[清华团队靠强化学习让 7B 模型打败GPT-4o数学推理]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagiaB4FmoGYPJSHLDr60qorJ1N1ic14OXo4TrOathTBcxkicDy3wlBA3pOUDlRgZm5ibEHFXxou9e3I8A/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：OpenBMB开源社区LLM所有细分方向群+ACL25/ICML25/NAACL25投稿群->LLM所有细分领域群、投稿群从这里进入！OpenAI o1 和 o3 模型的发布证明了强化学习能够让</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535080&amp;idx=2&amp;sn=c99eb0900bc84f9b0e310e57f40e6969&amp;chksm=ea8c1a516703411e0c3a99f47a6c28c25b21ed6ccfdff292c46983bec2ed2812dd937a677292&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 07 Jan 2025 12:05:09 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[博士生罢工、工资上涨、学术头秃：PI生存指南]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahNWJY6QnAp0deHR0Z9H0L4q26v5qQQjZ2uRPcfQ9wE4SGsCMQuTsCSUXRu9dXkdBkyVLcqn6b71Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>好久不写知乎了，新年除个草。其实这个碎碎念上个月就写好了，没发。现在想想反正写都写了，就图一乐呗，大家随便看看。知乎：周博磊链接：https://zhuanlan.zhihu.com/p/161192</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535068&amp;idx=1&amp;sn=6884d4efc2ea8ea6d2a7d87c461f033a&amp;chksm=ea3fd95fb6efeaaa22819ca5ad693e307935e8166978cba9fb4219c36f5c0d757d5ec975ee96&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 04 Jan 2025 13:36:15 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[[vLLM vs TensorRT-LLM]：动态序列长度场景对比]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahNWJY6QnAp0deHR0Z9H0L4rBCWZ27AGibFeTMQDKoZ7GhJL3Daic9Ku0WDPlGmwF9dqK7RHZfeYI1g/300?wxtype=jpeg&amp;wxfrom=0"/><p>来源：oldpan原文：https://medium.com/squeezebits-team-blog/vllm-vs-tensorrt-llm-5-dynamic-sequence-lengths</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535068&amp;idx=2&amp;sn=1163accc985bb456836003abc68859c1&amp;chksm=eaeeadc72f76968026552181f0c2e9ac08d364358b8317361e0d8bc1c4ac5e25574e83e07c68&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 04 Jan 2025 13:36:15 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图解Megatron TP中的计算通信overlap]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahNWJY6QnAp0deHR0Z9H0L4VaNwNXeibSNlODnE8TmEvLlB9yH9PaJs9IvFOSYTtD2fx5Q4OVpJz6A/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：大猿搬砖简记这篇文章想来探索Megatron中实现计算通信overlap的方法。具体来说，Megatron的dp、tp和pp部分，都有可以做overlap的地方，本文探索的是tp部分（更准确地说</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535068&amp;idx=3&amp;sn=9d9ad81e192d9971197767f8f527c03d&amp;chksm=ea080a8c6bea4824a4ee5dba97e6c0ac15fc2975ff0e092413eba6724f219afa28bcc375eb46&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 04 Jan 2025 13:36:15 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[从infra的视角聊聊DeepSeek-V3]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajMYUgLDn8eZiciayp5Qb2A2kzoZGPuQMhejEQUQPRNhEIlXtINapouqBkG4A6Ann5uWHtH2bib5ZT4Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>看完技术报告，从infra的视角分享一些个人看法，供大家讨论。首先，训练超大号的MoE模型，仅使用两千张H800加两个月的时间，就能达到如此好的效果，这点实在是太强了。只能说实践出先知，从DeepSe</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535043&amp;idx=1&amp;sn=db95d511c57704ea2f7fbe88f8bdd87e&amp;chksm=ea7d94992d91b06324936368538042b27c314f3bad83572c0bdf286645dc12d749ea7834a000&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 03 Jan 2025 11:40:49 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[手写self-attention的四重境界 self-attention]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajMYUgLDn8eZiciayp5Qb2A2kQiaXrm1coY3Dj1YeYGjCgFIPzw3od0Syic8YsQOsvKgXDLZ0AibBQiaiaKg/300?wxtype=jpeg&amp;wxfrom=0"/><p>背景在 AI 相关的面试中，经常会有面试官让写 self-attention，但是因为 transformer 这篇文章其实包含很多的细节，因此可能面试官对于 self-attention 实现到什么</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535043&amp;idx=2&amp;sn=e8bf018fc516e01e1e218b8c6cb6dff8&amp;chksm=eab3166358018d606b2257c489c97aacd01e99b870ee0afbd6e3d46d86603d4bb87a62d61c5e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 03 Jan 2025 11:40:49 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[o1类大模型的过度思考: 2+3=？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagcf3kfqIMv2aF3dJOvKWIb6SBznv5kN3Y4Yt9ZvD4hLMS4bgMNxQdow7guaK58nvCl9EI26cbJYw/640?wxtype=jpeg&amp;wxfrom=0"/><p>腾讯AI Lab和上交发现在面对一个基本的算术问题“2+3=？”时，o1类LLMs为何会表现出过度思考的现象。这个问题虽然简单，但它揭示了当在处理复杂任务时，这些模型是否真正高效和智能。下面一起深入剖</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535026&amp;idx=1&amp;sn=56d9cb19f0dae70aab538a86cf1328ad&amp;chksm=eab87cb5e24337a670ae5b9b57d8d9c627dd4c5acacd3e5ca302ed3f8f596d51d7fb97d4e865&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 02 Jan 2025 07:57:20 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2024年RAG：回顾与展望]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagcf3kfqIMv2aF3dJOvKWIbslPGo2Dv7C1f2MV27UMYoWnUdGLZdbMOvKUks3C7LBXrNvtoILTVQA/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：ChallengeHubLLM所有细分方向群+ACL25/ICML25/NAACL25投稿群->LLM所有细分领域群、投稿群从这里进入！2024年，RAG（Retrieval-Augmented</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535026&amp;idx=2&amp;sn=d7669c5a323022f305eac3d62550f016&amp;chksm=eaf242f2e2bf5bf0073a13ff312c3524c0c79d908f0c8a359b7120912928869148b894a544bb&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 02 Jan 2025 07:57:20 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Building effective agents笔记]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagcf3kfqIMv2aF3dJOvKWIbtpVk0JPKriaIcicp649fGhR6AJIPmphLjz2neCYPOyfNnCrwibIwF9Rgw/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：SimpleAI and ...最近阅读了 Anthropic 发表于12月20号的一篇文章《Building effective agents》（https://www.anthropic.c</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535026&amp;idx=3&amp;sn=4139fae2b502d58d5099404dc6e7aaf2&amp;chksm=eac1985a037f65ff0cc1d0989da20a2d294209213ec2f497848740c1d86e5f1478cd9b59b881&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 02 Jan 2025 07:57:20 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[微软公布OpenAI闭源模型参数！4o-mini 8B！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagJXDvsF3HlcGS7dbMEnUw0JKPzEt6aK2DUIMkwdubspoInY4oe8Xy1acHiaicsia1W5rxSfwibwgncicg/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家新年好！祝大家新的一年薪资歘欻的涨，论文嗖嗖的发！没错，就在前几天，Microsoft发布的arxiv里竟然写了OpenAI闭源的大模型的具体参数！（消息来自：xhs博主 Scarlett_WH）</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534988&amp;idx=1&amp;sn=9415a77980637b5a8c0fd3039e3c1256&amp;chksm=ea43fd48480e94c9c15fad10ab031d188d8dc2f8f4f9f23c173fe3e41b06b571766a2dfa9c76&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 01 Jan 2025 11:39:14 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NICE42期 | 语言模型不听话怎么办？关于格式忠实性的探索]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagJXDvsF3HlcGS7dbMEnUw01E8YaGKbgMFZQCicibibrUiamcSbT226pBrxiaY12iaBEQMfjwTUUbwhAWhg/300?wxtype=jpeg&amp;wxfrom=0"/><p>1. 主题语言模型不听话怎么办？关于格式忠实性的探索2. 时间2025.1.3 20:00-21:003. 引言遵循给定的格式要求生成结构良好的文本是大语言模型的一项基本功能。然而，语言模型往往不能充</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247534988&amp;idx=2&amp;sn=a96a1c76a51775a1d615651d78ec1626&amp;chksm=ea16afdf745e52aac3e6d4b44224f309f8eb015bd636fa07bd19bcaa212edbed3cc1d28181c9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 01 Jan 2025 11:39:14 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
