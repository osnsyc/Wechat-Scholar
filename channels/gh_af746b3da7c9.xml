<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[深度学习自然语言处理]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_af746b3da7c9.jpg</url>
      

      <title>gh_af746b3da7c9</title>
      

    </image>
    
















    <item>
      <title><![CDATA[港理工提出TokenSkip：让大模型在CoT中“跳”过冗余token，压缩40%，性能几乎不降！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahRL0QSQYP43wSAlIV5LLKofgPIxAicnaeicqzd3IBhnD7cOc0lmvNyST6ricZmDLtbOwlGz9J3M1Ojg/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天我们要聊的是一篇关于LLM的论文，主题是如何让大模型的“思维链”（Chain-of-Thought, CoT）变得更高效。大家都知道，大模型在处理复杂问题时，会生成一系列的推理步骤，这就是</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247536078&amp;idx=1&amp;sn=de04ccdc94881cbc662844bfe09a789f&amp;chksm=ea031df50694546d6f4e8f7251663996fa1c65570c803cc5fea2c7e7100be9e55fd63ff9caa2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 20 Feb 2025 00:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[DeepSeek 背后的数学原理：深入探究群体相对策略优化 (GRPO)]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahRL0QSQYP43wSAlIV5LLKoAQvMqx3mWnCKerF4piaINKxqxaqYYia0koLEqe0kNy49u9lvEOg18r0w/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自：ChallengeHub原文译自：https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247536078&amp;idx=2&amp;sn=6baaa36aaa6614ceefc283a8c459f0db&amp;chksm=eace4da8b04a518eff8f6c47b6ca68a7e57128e2868b90f54bbed77cb4dda5a80c5696308f86&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 20 Feb 2025 00:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[推理步骤长度对大型语言模型的影响]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahRL0QSQYP43wSAlIV5LLKoy5GZyharrkuibMBn9AVZRsiaUI6530D9aWcqZwF3brcAFOLMrZGstIug/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天我们要聊的这篇经典论文，简直是为那些喜欢“想太多”的AI模型量身定制的！你有没有想过，为什么有些AI在解决复杂问题时表现得像个“推理大师”，而有些却像个“愣头青”？这篇论文发现了一个有趣的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247536056&amp;idx=1&amp;sn=820912d8620a3e21b907b0d407a4849f&amp;chksm=ea370d1dc2d271ee722d0f81eff3404933c555dc20ae7be23eba57e8c8c08fb2a63d295a67fa&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 19 Feb 2025 12:45:58 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[JAI | 图像+语音+文本多模态语法归纳]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahRL0QSQYP43wSAlIV5LLKoCf9tuCGCgTHITkDey5sgF4CbadbEdvpp1R3PGSe9ic1WORCFia4twQGw/300?wxtype=jpeg&amp;wxfrom=0"/><p>概括: 在大模型时代，语言模型在语言理解和生成上已实现得了卓越的性能，然而关于语言模型的语法理解能力的研究仍然不足。语法归纳（Grammar Induction）是一个NLP领域的经典任务，旨在研究无</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247536056&amp;idx=2&amp;sn=ae0ca734dba1485843cd9f7de673d106&amp;chksm=ea8111c283cf2f840c1b9a74b60dd93b95f069adc6051cc29e6c7214f2255b8cfb2e25d8eecb&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 19 Feb 2025 12:45:58 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[上海算法创新研究院大模型实习生招聘]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahRL0QSQYP43wSAlIV5LLKoe3HLKeprkvbpuia3IfnRjVo2rRbrxzNzFWm7MRFM4oFqTzESHynEyWQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>我们是来自上海算法创新研究院大模型中心的算法团队，团队成员来自普林斯顿、北京大学、上海交大等著名学府，主要从从事大模型预训练、增强相关的算法工作。团队拥有足够多的算力和研究分为，围绕模型架构、训练、数</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247536056&amp;idx=3&amp;sn=b27c6d8c6de3b95800b7c7eb22ac2e28&amp;chksm=eaf9412fc06a95ce3f33d228e0fda97fd81eacea16abaf42b7e79740e6205f3af1a811bd99de&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 19 Feb 2025 12:45:58 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[通过模型生成的解释理解LLM后门攻击]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiamLELDHtJLDCW8KdpvnqBou0pphTRC6yZgKcOPHYHHgPft8JSEh1Ykz5JAWPqT1SvxXPU6fhLgUA/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天我们要聊的是一篇让AI“自曝黑历史”的论文！大家都知道，LLM 就像我们的“智能小秘书”，但它们也可能被植入“后门”——比如在训练数据里偷偷塞一些暗号（触发器），让模型一看到暗号就“黑化”</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535947&amp;idx=1&amp;sn=c6a6fc401c2a128b039c1aa006d71f24&amp;chksm=ea08a1930c06cd0308847b7f814ba37d91265b096e56672b9aa5a048c66865c651376cee600c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 13 Feb 2025 12:24:51 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ICLR2025分享会报名啦！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiamLELDHtJLDCW8KdpvnqBoCLBY6oH9hXMdS8cDeAp1BU840njXziaxFoxe4OGqg46wNSgIsY35qBg/300?wxtype=jpeg&amp;wxfrom=0"/><p>ICLR2025放榜半个月啦，NICE将要在本月下旬开始（2.15ARR之后）分批举办论文分享会，在此邀请大家来分享自己的工作，好的工作应该让更多的人看到！（文末了解NICE详情）报名方式扫码(或者)</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535947&amp;idx=2&amp;sn=606f6f0a2bc341d535d0592ffde8fe77&amp;chksm=ea1c30c7f724849325e4a7ecc580551f676908306823b09f71b708ed509d11e1624eff57ccdf&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 13 Feb 2025 12:24:51 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[从理论到代码剖析DeepSeek-R1：从PPO到Reinforce++，再对比GRPO]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajaRnOSDibA8Dl9t2rSFUJ0loEMf00LY9MZ9N82CNGD3qe9rNSzwBIznUXWsUOXwf8y8zv7H4jMT3A/640?wxtype=jpeg&amp;wxfrom=0"/><p>前置说明：我只是 RLHF 一个刚入门的小菜鸡，完全可能错误百出，欢迎大家批评指教！Reinforce++ 和 GRPO 都是 PPO 的变体。PPO 有 4 个模型，actor，critic，ref</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535925&amp;idx=1&amp;sn=beccd3b61ad9c0655aa534db5a6f1e1d&amp;chksm=ea15aa2e119d10fcb7b8830c7011c53418df68d842e49610f76dc37086bad13668b0383c5ebe&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 12 Feb 2025 14:39:45 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[三张图速通 DeepSeek-R1 论文和技术原理]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajGDDc7DghcxMB72qiagJMPewpYA6Bibn9aUOiamibBqz7scibQ2Cwbib9WibYKxSL7L5QTuKhLKXhVomz7A/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：木尧（已授权）链接：https://zhuanlan.zhihu.com/p/20538667476编辑：「深度学习自然语言处理」公众号总览最近在研究和复现 DeepSeek-R1（671B 参</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535912&amp;idx=1&amp;sn=fbe0396d92c2f6f522a25083e8a28397&amp;chksm=eaf20ef93e4368203c9e39acc9794adf7e074113225c3b9c766ac6a2c6e59b1aa83db06631dc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 11 Feb 2025 12:21:40 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2轮推理赶超已对齐模型？TPO：无需更新参数的对齐新思路]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajGDDc7DghcxMB72qiagJMPeeEYZN14U6eyVWylfJdicyo1by1grSXVZQ8K1sCwGotAh3bnU4R3xLhw/300?wxtype=jpeg&amp;wxfrom=0"/><p>传统的对齐方法（如RLHF和DPO）通过更新模型参数来最小化损失函数，但需要迭代重训练，限制了大语言模型（LLM）对新数据和新需求的快速适应。本文提出推理时偏好优化（Test-time Prefere</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535912&amp;idx=2&amp;sn=9914f8faaaeb585a7bd8e983cefe6e86&amp;chksm=eac43245c51ad797fd4e05f661c90e5557375a3302ded9fa636337baefb5bff17bbd970beadc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 11 Feb 2025 12:21:40 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[聊聊Reasoning Model的精巧实现（ReFT, Kimi K1.5, DeepSeek R1）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahKL7Pib1C9FI0yjtoy2xfj46fHY8yH0KxTK6DzpsseDGfMYciarKCRTDmQ4DKOkY7l27vSoa2acvEA/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：姜富春（已授权）链接：https://zhuanlan.zhihu.com/p/20356958978编辑：「深度学习自然语言处理」公众号引言最近Reasoning Model（推理模型）异常火</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535834&amp;idx=1&amp;sn=cd73a09d67369d94087094b43846c623&amp;chksm=ea4b79e930d2a875919cd8e1677d5e3cd8ecc57c948fec863ee900416d0c0a68dab40b16dfc2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 13:35:31 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Speculative Ensemble - 让大模型集合推理飞起来！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahKL7Pib1C9FI0yjtoy2xfj4cLG4ia6S4FAIqP7KMfk8P54uAicdWyFS1dUo9ldVqIsVBcI4txt3fCUA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天我们要聊的是一篇关于大模型集合推理的论文。大家都知道，大模型集合（Ensemble）是个好东西，它能把多个模型的智慧结合起来，生成更高质量的文本。但是！集合推理有个大问题——慢！就像你等外</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535834&amp;idx=2&amp;sn=c331d6ff2ab3928141a86fa29887ea84&amp;chksm=ea392d8ec3ff00319fcc6d35aa5e497d36f25e671621a7bf56d5abfbc2738a50a36ccb159fe9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 13:35:31 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[国产模型再秀硅谷！阿里Qwen2.5-Max数学、编程能力全球第一]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bah6KiaxqiauTgvdWic4UChZUQV5iaI0gD21iaEooGoYZPmYoLYKbwGRBYsSiatNNK6TffaCoticziaPxvlN8A/640?wxtype=jpeg&amp;wxfrom=0"/><p>春节期间，咱们国产AI大模型就给全球网友送上了一份“见面礼”——阿里巴巴的 Qwen2.5-Max 正式登顶 中国最强，更是在 Chatbot Arena全球排行榜 上杀进 前十，超越DeepSeek</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535784&amp;idx=1&amp;sn=3871937d172696a7959df1c45179e772&amp;chksm=ea02560062bac0bb0e3efe514f9114f8480a2a9ce78690182d57bdd29465c571271ff90fa0d1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 07 Feb 2025 04:18:06 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Deepseek R1 Zero成功复现, 三阶段RL，Response长度涨幅超50%，涌现语言混杂，double-check]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahuZvEI97znd0G0l6VouDSVxIo0C0MqyGAOqdvcOiblKBAKFibvNVd1VpepK04tZthkWNqBNoRcPqfA/640?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：涮月亮的谪仙人（已授权）链接：https://zhuanlan.zhihu.com/p/21290410831编辑：「深度学习自然语言处理」公众号项目代码可见：Unakar/Logic-RL(h</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535692&amp;idx=1&amp;sn=b91e950bd4132e79756c6edf3f3b6963&amp;chksm=ea995e425920598681b5d3ae3287c356a7d9eb2333015ebd204ed97a2b8b8c4261e25ef15431&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 05 Feb 2025 08:22:14 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LightTransfer：将你的LLM轻松转为Hybrid model，增强o1-like长文本生成能力]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahuZvEI97znd0G0l6VouDSVbtz438qbnbIxP1ZcicicSG3Ygb4iaoRrVK6UOWW4Iia8kN7jjndICKoaOA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近hybrid model的有效性已被广泛验证（比如minimax-01和gemma2等）。该篇论文研究了一个非常有趣的主题：如何将预训练好的dense transformer（如qwq）转化为hy</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535692&amp;idx=2&amp;sn=eda5ead962e3e136ead35a2abe6593a7&amp;chksm=ea15139569faea05e0704cebf75eb8183fa8cc42297f7b7873f40bbb77ff05ca834359b02f8d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 05 Feb 2025 08:22:14 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯AI Lab联合苏大上交提出：少切思路多挖矿，让o1类LLM做题不再「三心二意」]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajhH7bBxWoicqmXpSvpJ1pgvgibt1ayicoldevaKYCO4GjhTI2snOzsewRz1Zwic2lmVI4IPby9ibwZXyw/640?wxtype=jpeg&amp;wxfrom=0"/><p>想象一下，你班上有个超级聪明的学霸，但他做题时总像得了「思维多动症」——一会儿用代数算，突然又切到几何法，再蹦出个微积分，最后…答案错了！这篇论文抓到的正是大语言模型（比如OpenAI的o1）的这个小</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535645&amp;idx=1&amp;sn=d8575ba255fe80a320f670b114a4ce40&amp;chksm=eafd563db4c33bb9fe3900364549708ea6309c9b567ee49602d514a456d7b4f06fcf87133462&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 04 Feb 2025 08:33:34 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[新突破！xJailbreak：用强化学习「越狱」大模型，可解释性黑盒攻击来了]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajhH7bBxWoicqmXpSvpJ1pgvpSQOuBrOKZbaJnUSiaYEicWnBLUibjkMSWnwtQQdsS4XFGuceic6DfSfjQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>来自原作者团队投稿编辑：深度学习自然语言处理大型语言模型（如 GPT-4）虽经过安全对齐，但仍易被“越狱”。现有黑盒攻击依赖启发式算法（如遗传算法）优化提示词模板，缺乏可解释性且效率无法保证；白盒攻击</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247535645&amp;idx=2&amp;sn=75fd9427555455ca1956d87efe5a166b&amp;chksm=eaba8fc533be052178e9f1b5874290bf8eadb7895291a387d651624f06f3cc20f614adaca735&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 04 Feb 2025 08:33:34 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
