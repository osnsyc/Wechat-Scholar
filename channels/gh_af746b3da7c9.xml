<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[深度学习自然语言处理]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_af746b3da7c9.jpg</url>
      <title>gh_af746b3da7c9</title>
    </image>
    <item>
      <title><![CDATA[强化学习背后的隐藏代价：幻觉税]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagobB6cKeicuo960PRFpDiaBksvqzMwia4YBBRibJjQNhLIZMqmdONTticXyYIYWgZMVvH3ic8b1QQ4lacQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文：The Hallucination Tax of Reinforcement Finetuning链接：https://arxiv.org/pdf/2505.13988大模型的“自信陷阱”：强化</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539278&amp;idx=1&amp;sn=582dc4b0006ca3f98b0844f8a8bc5151&amp;chksm=eac303862dfc6feca4b085c50566855a7a2ca6f8431418462bf011389a7a8e26441cd5179e10&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 09:31:48 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[PhD补录 | 哈工深计算机学院陈科海老师补录2025级9月入学博士生，10天内有效，先到先得]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagobB6cKeicuo960PRFpDiaBkC8LaYh6gC2FnBicWRYF7DiaNJKQgcz965gvSRC9DseAhPvZdVKOGt8nQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>哈尔滨工业大学（深圳）计算机学院陈科海老师补录2025级9月入学博士研究生导师简介陈科海，博士，哈尔滨工业大学（深圳）计算机学院教授，博士生导师，国家级青年人才。2020年获中国中文信息学会“优秀博士</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539278&amp;idx=2&amp;sn=d814fd5eca4a5ab75de21514f993168a&amp;chksm=ea173ba6e92958b18e96b8281080906720c6d0462103bd53fe1f1e2eba9006af4e8493b67a6f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 09:31:48 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[学会“模糊思考”：推理速度提升22%，还少犯错]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagobB6cKeicuo960PRFpDiaBkaWAKs5OtCgrlC4yzUbhDH5NrgM1EojsnO2DYohgELsotdrXoBhtX8Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>过去的 LLM 在推理时，每一步只能选一个确定的词（比如“苹果”或“香蕉”），就像考试时必须在ABCD中硬选一个答案。这种离散符号的思考方式有两个致命问题：容易走错路：一旦某一步选错词，后续推理全盘崩</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539278&amp;idx=3&amp;sn=bdf197b63047f3082b2dc383661e8b4a&amp;chksm=ea91ddbcfa73536d5a0768645d4e58ed36fd340994985a444d5eb7392905b721724264fb1d58&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 09:31:48 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICML2025分享会报名收集啦！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagobB6cKeicuo960PRFpDiaBkWcFH9Lmnf736pgz5WYB90vMoAE9jZsLPUlWVh2b4kcYH5IH0y1sibIQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>NICE将要在五月末和六月初分批举办论文分享会，在此邀请大家来分享自己的工作，好的工作应该让更多的人看到！（文末了解NICE详情）报名方式报名后，我们会通过微信尽快联系你哒~扫码(或者) 链接http</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539278&amp;idx=4&amp;sn=3e64aeb0ba698753545eb84d7f1b47e5&amp;chksm=ea5e56d042b6dfb6a38d3b58069df136e2c370d51012379d1c1441240d9fcb6ba34a85e3cc65&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 09:31:48 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[从零到有：打造迷你DeepSeek-R1最全教程]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia6SJB7GrjjtB26BGUA8aqEA6hBN8Q8VAdJGZoKkqGsFWeBwZ7jN4rH8C5vuuCibEYv7FIhLK7NV7Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>打开GpuGeek算力市场，地址如下：https://gpugeek.com/login?type=register&amp;source=wechat_DLNLP_01其中最香的是RTX-A5000-24G</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539238&amp;idx=1&amp;sn=52785c5ce8bf8169b3087d109a852f67&amp;chksm=eae23feb8fe70062995a23c046b0808bdee302fece8bacfff936abe4603a91e9ca69d8d56018&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 May 2025 08:50:58 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ACL 2025 | 清华&amp;港中文提出 MorphMark：全新理论视角破解大模型水印效力与文本质量的两难困境]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia6SJB7GrjjtB26BGUA8aqE7icnWaDVISQkNbEMReYb6qeHHTTPOLibUsibeicwWLAPuqOoVAGEFiceuJQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：MorphMark: Flexible Adaptive Watermarking for Large Language Models链接：https://arxiv.org/abs/2505.</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539238&amp;idx=2&amp;sn=f42937219114bd79d418a432c2393b0a&amp;chksm=eac5e9ef12b01ce4b0e95cfb1752ebfa16f92429abf057a4334de3a7b1d74ddae32918e94748&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 May 2025 08:50:58 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[在Think中边搜索边调整的搜索增强Reasoning方法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia6SJB7GrjjtB26BGUA8aqE07InJpGGCdXXwsiczBrVnperhBmzxEXFSfElS80UFPSBsC5fs538wvQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>为什么需要“外接大脑”？LLM虽然“知识渊博”，但本质是“死记硬背”——训练数据外的信息它无法掌握。比如问它“2024年奥运会新增项目”，它可能瞎猜。于是科学家们给AI装上“外接大脑”：检索增强生成（</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539238&amp;idx=3&amp;sn=0dd675aef346c43f7670b3b4acc80e7a&amp;chksm=ea5058287f3db2b3cedb821f392e652c7f5870da170a2c92ccce8ad10c1a05fe46b740b38879&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 May 2025 08:50:58 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICML 2025 | 无需训练，即时对齐大模型偏好]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiag98LoUGfvTHicRVK9jRYv4uk96EfBbJiaia1MvxSBZibv20npNEA7JJHo3b5v3ldtqtqMIlgW66gZ3g/640?wxtype=jpeg&amp;wxfrom=0"/><p>TPO：推理时即时偏好对齐的新方案 为了让大模型（LLM）的行为更符合人类预期，一系列训练时对齐方法（例如RLHF和DPO）通过微调模型参数来实现偏好优化。然而，这种“训练时对齐”模式不仅耗时耗力，而</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539202&amp;idx=1&amp;sn=0eed57d030fb967564ac635ecd6293cb&amp;chksm=ea23a5c1758b3fa2d9e33a2b45484abcf3be298e66bbd554288e05851075326d2568751fd525&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 20 May 2025 14:18:31 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR 2025 Oral | LLM也有从众心理！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiag98LoUGfvTHicRVK9jRYv4SI1Qb1NTqNrOHYsN7xTjZhF3HvQdYrkicicZB3uOqmuYiaNib3QWiarCakQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>当你和朋友玩“谁是卧底”时，明明知道正确答案，但看到所有人都选同一个错误选项，你会不会怀疑自己？最新研究发现，大型语言模型（LLM）组成的AI团队，居然也会犯这种“从众”错误！论文：Do as We </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539202&amp;idx=2&amp;sn=41b0bea4575111a5626729fdc97fbe2d&amp;chksm=ea035f0438df5cea3499a833ff531c129b93d408bf1087cc63765263f900003085232214bea2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 20 May 2025 14:18:31 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ACL2025 | 抓出0.1%的捣乱分子压缩方法OTT：近乎无损 超越KIVI，内存减6.4倍 吞吐量提2.3倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahjSlXD4qibHCzpgnVNuhNmgaoPdbpk4tb2sicfEibLz978rdGIkqibZsLxY8X0ThVS0XV35alvIjgV6w/640?wxtype=jpeg&amp;wxfrom=0"/><p>LLM 生成文本时，需要记住所有已生成内容的关键信息（类似“临时笔记”），这就是KV Cache。它的存在让计算复杂度从平方级降到线性级，但代价是内存占用飙升。举个栗子🌰：LLaMA-3-8B模型处理</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539160&amp;idx=1&amp;sn=cf1acd98d304b129d66b51f3b05020b9&amp;chksm=ea5ea8445246a2824098bee257d34f5f8bab93e6c61ffbae26030a875a895edf9fac7d0f1fe2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 19 May 2025 08:09:48 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Reasoning新突破：SoftCoT++如何让LLM‘多想几条路’？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahjSlXD4qibHCzpgnVNuhNmgmBZelEQlzNjVhhAiciaEs40yBkdtjNBzibDmZ7k2GBcOvsc0lodo1bUeA/300?wxtype=jpeg&amp;wxfrom=0"/><p>LRM 的推理能力依赖“思维链”（Chain-of-Thought, CoT），即生成中间推理步骤。但传统方法在离散的token空间生成这些步骤，存在两大问题：信息丢失：每一步只能选一个词，复杂逻辑可</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539160&amp;idx=2&amp;sn=d27ce5cd589b1bf020fcabdb94a585b2&amp;chksm=ea0b129e2d969c0d2b1ab435fff220a19ad4c04c83c9a597836b11bbe660d69fe534f8180112&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 19 May 2025 08:09:48 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[FudanNLP邱锡鹏老师组-25普博/26直博/26普博/26保研 招生]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahlhzvX5DAHVe5pUvFEZaj3I8A2AsRLhfrfuXxTtdZKtuwy8sXwIrQpXaJc6lUficCEXevgHHhZQag/640?wxtype=jpeg&amp;wxfrom=0"/><p>复旦大学自然语言处理实验室（FudanNLP）邱锡鹏老师组开放第二批2025年普通博士招生，复旦官方报名系统已开放（即日起至 5 月 25 号）。该批次招生拟入学时间为 2025年9月。 同时，实验室</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539135&amp;idx=1&amp;sn=22e6faf4fbba354ecc80c6923d4b9160&amp;chksm=eaf04a910ae8a51a4036ed1d68fdc706e21012afef99abee8101c2e801101cb2360c2abfda22&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 18 May 2025 10:48:52 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[多模态大模型集体翻车，GPT-4o仅50%安全通过率——SIUO 揭示跨模态安全盲区]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahlhzvX5DAHVe5pUvFEZaj33dL6ZtOnkSUOqia5YaZrcenI25vd8qlNKqElE2K5LvXY5wpXt0b5c3g/300?wxtype=jpeg&amp;wxfrom=0"/><p>随着通用人工智能（AGI）日益融入人类生活的各个方面，确保多模态大模型的安全对齐已成为亟需解决的重要问题。现有研究大多聚焦于单模态风险（如有害文本或图像），却往往忽视跨模态组合所潜藏的安全隐患——即便</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539135&amp;idx=2&amp;sn=cfa3b789b365096529ec5a72b4bdad1d&amp;chksm=ea64b439a7e41197492f76298d1006a4f6e264f31aa58ca6e649c64edbf774332690fdc7000c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 18 May 2025 10:48:52 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[苏州大学OpenNLG小组近期录用15篇ACL、2篇ICML等论文！招生贴]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bah306xYta2M8cHhJfFqzp97FCicRJoOh3PoIVl5S8DMe8Xoh93azKQ6LVR22lPesqtBDQ3Pc7UMyHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>近期国际顶会或期刊陆续公布录取结果，我们刚成立4年多的年轻Team苏州大学OpenNLG小组，共收获19篇论文录用，分别为15篇ACL（8main+7findings）、2篇ICML、1篇TACL（A</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539101&amp;idx=1&amp;sn=04f41a3e3b9fc2c2f667ef41fb3c82de&amp;chksm=ea4ebed7bd70a7b85af9b78a93100dc9cf9cefcba46652225677a7e5402d45b1fafd9072e2e9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 17 May 2025 14:59:26 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Qwen突破：用「并行计算」代替「堆参数」，新方法内存降22倍、延迟降6倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bah306xYta2M8cHhJfFqzp97pk9mC9lPMqQ0OTHMFT8nzkjE3truCEvDLy8RTnwpWDPA7v8kcLcFXQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文：Parallel Scaling Law for Language Models链接：https://arxiv.org/pdf/2505.10475LLM 的进化一直依赖「堆参数」，但模型越大</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247539101&amp;idx=2&amp;sn=a6f6d4ecfcbf475195a3c7251303e5d2&amp;chksm=ea7cf4aa8be820a55eff7729fd3d10aac27af7ed0c6e2ca94cc962c2aa00afb3a3a3aac7d2a7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 17 May 2025 14:59:26 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[完全从0开始，仅用8元+9h！即可训练出Tiny LLM全流程教程，包含Reasoning、MoE等]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajEib6TPfeUGlfZ3mPB17GmibibMdGolxcDrctjFlQIzc6tichYCPH09Dtu8IcBUNcxlM2nucPiariblGeg/640?wxtype=jpeg&amp;wxfrom=0"/><p>打开GpuGeek算力市场，地址如下：https://gpugeek.com/login?type=register&amp;source=wechat_DLNLP_01其中最香的是RTX-A5000-24G</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538995&amp;idx=1&amp;sn=1d849e875791df98f860302e90b4cc46&amp;chksm=eaaf7db648eec7f2db329a8988ab1a736eec3dc119619bf7b019b0f3cf5eb516598301aad5ba&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 16 May 2025 04:37:02 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[告别Reasoning模型的“灵光一现”，推理能力可控了]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajEib6TPfeUGlfZ3mPB17Gmibia7icywNxnvicVKqk6gNePobduS3Q4MplReCHLb0BVQXHR7eoia8fk4SYw/300?wxtype=jpeg&amp;wxfrom=0"/><p>为什么说模型的“灵光一现”不可靠？当前像GPT-4o、DeepSeek-R1等大模型虽然能生成复杂推理链，但它们的“高级操作”（比如自我纠正、反向验证）往往是随机触发的，就像突然的“灵光一现”。这种不</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538995&amp;idx=2&amp;sn=a3e25d61998452eafe08d0d5eff3cc13&amp;chksm=eab925aba321d6f85fe5592c25364b624bc9c5afba88ec8bc25ee2754a5b610ff2e1d9f14e40&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 16 May 2025 04:37:02 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ACL2025 | 代码助手火了，但安全吗？所有模型评估结果都很扎心]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajEib6TPfeUGlfZ3mPB17GmibyyxvWS5T6icqA53nRdavIOHUaJEDEykDPtNROBicSIqroxicH2ibbVOtZg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，GitHub Copilot、ChatGPT等AI代码生成工具火遍全网，程序员只需输入需求，它就能秒出代码，效率提升肉眼可见。但问题来了：这些生成的代码真的安全吗？论文：Can You Real</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538995&amp;idx=3&amp;sn=70d1ccfa41f6b2adac05de8281e48b76&amp;chksm=ea2572efcfb6b4e3c6cc1d3eb26a4c0c56d7329259b090e57f1eebb58462e3e047214ee78b90&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 16 May 2025 04:37:02 +0000</pubDate>
    </item>
  </channel>
</rss>