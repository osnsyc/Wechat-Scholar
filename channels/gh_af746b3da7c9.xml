<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[深度学习自然语言处理]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_af746b3da7c9.jpg</url>
      <title>gh_af746b3da7c9</title>
    </image>
    <item>
      <title><![CDATA[LLM长文本内卷，谁是真英雄？告别跑分玄学，我们需要一把“公道秤”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajhlCZrXsaqfPAaGytWdWU93ibiacTgrEtHUPqicdrfwbTbPa1icics4V0zjKgAUFya8HTN2NKaG4FBDjA/640?wxtype=jpeg&amp;wxfrom=0"/><p>TL;DR: 我们做了一个统一的长上下文评估框架，兼容目前市面上主流的benchmark、model、长上下文加速方法，帮助社区高效地、“真实”地测出长上下文模型的能力。欢迎大家使用，提供宝贵意见（欢</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540918&amp;idx=1&amp;sn=3660b77338c99a39469321d4822cd366&amp;chksm=ea87b5d86b6a1a3d76f7c65941aa46e5cfe494ef23b54052c38c97c3b60402ce7f78d4c677bb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 09 Jul 2025 05:20:32 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[直播预约 | 从大模型的安全对齐到欺骗性对齐系列工作分享]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajhlCZrXsaqfPAaGytWdWU9LFYyHHrHz9S9N8HicBrLIdYQp1lQWAnfW0DuNKWLbBmLaX5ToghPFBw/300?wxtype=jpeg&amp;wxfrom=0"/><p>主题从大模型的安全对齐到欺骗性对齐时间北京时间：2025.07.11 (周五) 10:00直播平台微信视频号：b站直播间：https://live.bilibili.com/27784098（点击文末</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540918&amp;idx=2&amp;sn=7829b1cfc8911746fb1535be5780d7ec&amp;chksm=ea6f7101006cfb0f199d074f0ba6cf7c263afff36c7d48806c98fc41b6a8a792bde9150019af&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 09 Jul 2025 05:20:32 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[奖励模型迎来预训练新时代！上海AI Lab和复旦联合重塑RL奖励机制]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaJSJaia9VUiaQQLoBnQLw5NsbqV0UTGXu5jibEW1xIaLdgqMcBQhdUrBxFyJkh7kWbnuhn5Cw4eXibaQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型（LLM）的精调常依赖强化学习人类反馈（RLHF），其效果核心在于奖励模型（RM）能否提供精准的反馈信号。传统RM面临两大瓶颈：数据依赖：需海量人工标注的偏好对（如"A回复优于B"），成本</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540895&amp;idx=1&amp;sn=702c3ab7347f598da37d663cc6b3d0f1&amp;chksm=eaebf5e3e96cea8e2cea2b05189a2194fca3631a9e423062e4975ad1959c1a56f7e746e77010&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 08 Jul 2025 07:57:01 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[重塑AI记忆边界：MemOS开源！时序推理较OpenAI提升159%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaJSJaia9VUiaQQLoBnQLw5NsbD7Tr78aYvnotrn0JF2MQyAGALGic5L4yJ5GXHZvSd9HOlKicIJVfggg/300?wxtype=jpeg&amp;wxfrom=0"/><p>大模型记忆管理和优化框架是当前各大厂商争相优化的热点方向，MemOS 相比现有 OpenAI 的全局记忆在大模型记忆评测集上呈现出显著的提升，平均准确性提升超过 38.97%，Tokens 的开销进一</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540895&amp;idx=2&amp;sn=cc14a44916fad313ef977ea4ff2b47c1&amp;chksm=ea19e540c604c4bb66e547fdcd1a5591179acc8f7be2512c5e7fa1c706e7eb84e8a481db3ce2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 08 Jul 2025 07:57:01 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Doc2X：构建可扩展高精度多格式文档解析管道的API技术实践]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia7FibGuKaZuSO6fbicgfPTD18wOOfNEOuiatBComk3S7dF1wnJPicIfYYO2QhErQNexEOia8uh8uY9dxQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在数字化快速发展的当下，文档的智能处理与高效应用尤为重要。开发者在项目执行中普遍遭遇一个核心难题：如何将不同格式的文档高效转化为结构化数据，以支持后续的向量化处理和检索。然而，传统文档解析方案存在明显</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540874&amp;idx=1&amp;sn=4bbde63a06ff2a290a6e45769d1d2a62&amp;chksm=ea31997f611c7b010ee97190f6fa0732300781654a838d412a377033ef64ed0ad621b2af226d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 07 Jul 2025 07:12:14 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[数学推理热潮下的冷思考！如何训练真正"全能"的推理模型？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baia7FibGuKaZuSO6fbicgfPTD1jopLHzpiawYdjO3OpHMvtXWYFXfCz1HdXhGuUuDQXc1b6hHuLXkACMQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型（LLM）在数学推理任务上的突破令人瞩目——MATH、AIME等基准的人类记录频频被打破，模型仿佛拥有了"数学天才"的光环。但这种进步是真实的能力跃迁，还是针对数学题的过度拟合？论文：Do</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540874&amp;idx=2&amp;sn=ec6ef859471c1efd64f12a378b2d86a6&amp;chksm=eaee5dc7eb7c9cb58338a9b26a9e2d61c54b7864d0c550778bd83af5a3d3956f51e53b4d5d10&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 07 Jul 2025 07:12:14 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[提示工程死亡？不，它刚刚重生为计算科学：一篇讲透Prompt设计的科学基础]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajgKdjds5jnxnbD2thBRjvxMlEQKZCIN4HDpAnkJ3g37yXia4zJkibvu0f8iayH5znh1F5HJ1hFurWLw/640?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型（LLM）虽然在知识任务中表现出色，但在数学证明、棋类推演等多步推理任务中屡屡受挫。根本原因在于Transformer架构的计算深度限制：其注意力机制只能执行固定步数的序列计算（TC⁰复杂</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540813&amp;idx=1&amp;sn=aa3de773e5cad73a901c5f740a9377d0&amp;chksm=eabfd27ade680af86f554de8a947575e7f740ef6a2a59c538c1d63643fad4001894bee5327d5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 06 Jul 2025 15:56:40 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[关键突破：理论验证了 RL for LLM 路线的可行性]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajgKdjds5jnxnbD2thBRjvxGl60peneFicPzegenaMLW5fMdOoxJzwLicxg1g38ibZ29KW5DBwruF2Hw/640?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型（LLM）的对齐（Alignment）是让模型行为符合人类价值观的关键技术。传统主流方法RLHF依赖人工标注偏好数据训练奖励模型（RM），成本高昂且难以扩展。虽然RLAIF等方案尝试用AI</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540797&amp;idx=1&amp;sn=aeefbf1d55aaacfe42d873cd89aa7d7d&amp;chksm=eadd0c9937f3c8dc3dff83ced61365c65211b7560f5de607e196516e16deb5390063e436902c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 06 Jul 2025 08:31:50 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[牛津证实CoT不可解释！大家不要再用错了]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaWmgCyFvlO6o9nbibLsgUz4PpEp7wquZN8UTkpC1iaseXKj1nR0oM5Yibs0yVDdkicBQ87fH261JM3Sg/640?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型（LLM）的“思维链”（Chain-of-Thought, CoT）技术因其能生成类人推理步骤，被视为打开模型“黑箱”的钥匙。例如面对数学问题“求直角三角周长（直角边5cm、12cm）”，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540772&amp;idx=1&amp;sn=bf678ce6af328664aea1c0d17a3d4f9d&amp;chksm=ea2b666ac2a1449d07244cb52ac4724d97682dc8bd08812912a82ca83dd925b19a6c7c1d5544&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 05 Jul 2025 11:45:22 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[GRPO-λ破解LLM强化学习崩溃难题！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaWmgCyFvlO6o9nbibLsgUz4dPmbK7AfHSmeAN29S5TvmFFVoPicScm62l7vnbIicIst2I2GCibCAmBPw/300?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型（如DeepSeek-R1）通过思维链（CoT）生成复杂推理路径，但传统强化学习方法（如GRPO）存在严重缺陷：过度追求答案正确性导致模型"想太多"（过思考）——生成冗长的推理步骤，降低效</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540772&amp;idx=2&amp;sn=4ade721f54076a5615e1219c54d4bfd9&amp;chksm=eab946b2f488ef63a229830774b5a9c698544ac7cea613c7cc8902af89960efbcb779deffbe2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 05 Jul 2025 11:45:22 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[普林斯顿等高校提出AgentDistill：无需任何训练即可继承大Agent复杂能力，性能提升48%，成本降低90%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiapVNVAKLOSuA7zjG3ICLa6mFDv5Trlm95BnGvS3ibfKWLvSAIg32WIvVrRT8KbHerDADzAfXoFF8A/640?wxtype=jpeg&amp;wxfrom=0"/><p>在AI领域，大型语言模型（LLM）智能体已能执行复杂任务（如规划、工具调用），但其庞大的计算需求限制了实际部署。传统"蒸馏"技术通过让小模型模仿大模型的输出来压缩模型，但这种方法对智能体（涉及多步骤推</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540730&amp;idx=1&amp;sn=8d997ce8614b4147496a804d9e039547&amp;chksm=eac125a3d3c364348bc9253019f508ce8d3fc1fd3bd53557538f17a732ea5a60e5679f8c20ee&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 04 Jul 2025 14:19:02 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[中山大学&amp;华为联合提出 Issue Resolution 数据集构建神器SWE-Factory：每条只要$0.024]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiapVNVAKLOSuA7zjG3ICLa6iaNPdKWqAw0CRXy4J0A04S5icfw5IPbboaW80n4NO5GuFicCjbLS6C50Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>还在为构建高质量的软件 Issue 解决数据集而头疼吗？传统方法不仅耗时耗力，成本更是高得离谱。中山大学和华为联合提出全新的低成本开源解决方案—— SWE-Factory ，能够通过多智能体框架自动收</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540730&amp;idx=2&amp;sn=0697810aeea97c3020ac833760756d5c&amp;chksm=ea4539a83652891a7949c9077fbb656a420e831859ce4a025a5bd7e999670a18cb14a61731ef&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 04 Jul 2025 14:19:02 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[邱锡鹏老师团队发现SFT与DPO破壁统一：内隐奖励作为桥梁]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajzGOUduaptGichiaJTyjoFeQytSrMnbxj4gWM7SOrDFSAmnibHGKDcExoHgYd0H5XWX8yHX59OxCr8A/640?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型（LLM）的后训练是将其应用于实际任务的关键阶段，主要包括监督微调（SFT） 和基于人类反馈的偏好学习（如DPO）。传统观点认为SFT仅是DPO的"热身步骤"，两者缺乏理论关联。本文突破性</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540695&amp;idx=1&amp;sn=4f459ca72c63347df4b3b2d9d28977ad&amp;chksm=ea93e4d8d0f0ec6e486e016fd4af7975059bc61646f696604156c1d416c1c789a232592f5113&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 03 Jul 2025 15:57:56 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[2025.7.4上午 | 视觉智能驱动的多模态对齐与推理的近期系列工作分享 - 复旦博士生王思尹]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajzGOUduaptGichiaJTyjoFeQz0mo2VIMGB01DJlNYJRRk0QwQaN7aTmNesNALKn4DHjAId73qLnY8w/640?wxtype=jpeg&amp;wxfrom=0"/><p>主题视觉智能驱动的多模态对齐与推理时间2025.07.04 周五 10:30 北京时间b站直播间：https://live.bilibili.com/27784098（点击文末「阅读原文」即可跳转） </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540678&amp;idx=1&amp;sn=35ba0b9a0a82ce5dbf6b2bfcfe84cae0&amp;chksm=ea1e17591c363cf4d34feecdb5940cf8d4f30f768919a99a5eb9a813d76a915625991d901708&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 03 Jul 2025 13:28:01 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[别只卷文本了！港科大、微软这份爆火的“视觉思维”路线图，才是多模态的未来！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiauBEjsyYhsSN2luZkkd5eXInf0oCanPGbWyxmtJNyWdfZwyiadCicCt7h75Yw9kibXJFeS2b00NFhLA/640?wxtype=jpeg&amp;wxfrom=0"/><p>想象一下，你在解决一道复杂的几何题时，会自然地在草稿纸上画辅助线来验证思路——这不仅是“看”图，而是“用”图思考。然而，当前的多模态AI（如GPT-4V）大多停留在“静态看图”阶段：先将图像编码为固定</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540672&amp;idx=1&amp;sn=0e2af2aaf97dafa15dce0c7d7ee41d66&amp;chksm=ea61025ba49fc5559b0ab20884a306fd1e72a00d6a8d74331af324750abc09ad70bf886b845c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 01 Jul 2025 10:21:02 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[直播预约 | 视觉智能驱动的多模态对齐与推理系列工作分享]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiauBEjsyYhsSN2luZkkd5eX70Av3XOKD1Y22REurtrBH9STJRDHVnbC8xCibib4BicKEPmwmQ1zgQvTg/300?wxtype=jpeg&amp;wxfrom=0"/><p>主题视觉智能驱动的多模态对齐与推理时间2025.07.04 周五 10:30 北京时间b站直播间：https://live.bilibili.com/27784098（点击文末「阅读原文」即可跳转） </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540672&amp;idx=2&amp;sn=9c8df8e39ef6d4ab9c249631136aef32&amp;chksm=eaafa234ffcfb5269c8866e6b765371696a8c7fbcd88b607180052e64ee9a7495759f281da93&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 01 Jul 2025 10:21:02 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[看了这本全是图解的书，算是真的掌握LLM基础和前沿了...]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiauBEjsyYhsSN2luZkkd5eXyicY7wtwauvJGpR66rUa4g8ZWgkYOCXbzICWYU5L4Gt4vlJllGouHJw/300?wxtype=jpeg&amp;wxfrom=0"/><p>01The Illustrated 系列如果你关注大模型技术动态，你可能知道这两个名字：Jay &amp; Maarten，如果你不知道，那你大概率知道这篇文章——“The Illustrated Trans</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540672&amp;idx=3&amp;sn=b4f213d4cfdd18c2dac360a7549ef4eb&amp;chksm=eaa1b0c2bf1b7c1ed9e97a131961785ef19f30d4e892765bd45631869f7d276cbf152a88770c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 01 Jul 2025 10:21:02 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[思维锚点：破解LLM Reasoning黑箱的关键句]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagaLU6mHvozeiaKX8Sz6gWsWicFXewWGzCiaMn9fzB7fyw8rM9LYCfCzUmpcpwCjJ65NeicLZkeibFsJPQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型（LLM）的思维链（Chain-of-Thought, CoT）推理虽提升了复杂任务性能，但其自回归特性导致计算过程难以分解。传统可解释性方法关注单次前向传播的神经元激活，对多步推理的"黑</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247540618&amp;idx=1&amp;sn=a30969d09a00e14c924c8ae0d0b68aff&amp;chksm=eac3d97521d2ddf6948dfe45303e6f9291d0f83a8ce54fd01ec5455fae5d93d1c60f2cd012e8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 29 Jun 2025 09:57:25 +0000</pubDate>
    </item>
  </channel>
</rss>