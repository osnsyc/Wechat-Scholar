<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[深度学习自然语言处理]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[深度学习自然语言处理公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_af746b3da7c9.jpg</url>
      <title>gh_af746b3da7c9</title>
    </image>
    <item>
      <title><![CDATA[DeepSeek-R1发布后的100天复现之旅方法总结]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajhB24sPyaVd1yFKFxEz4ZuXSjXpD8VUE047EAfwHhicRFQwOJ459148vosYibD3cK9lebGf4LnIBLg/640?wxtype=jpeg&amp;wxfrom=0"/><p>100天前，DeepSeek团队发布了「推理大模型」DeepSeek-R1。这个模型不仅能回答问题，还能像人类一样一步步「写草稿」「验算」「纠错」，比如解数学题时先列公式再计算，写代码时边写边检查。这</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538432&amp;idx=1&amp;sn=552ba0d70e976ef8aa5cffcb27436d15&amp;chksm=eabfc499c29b8842fb0e75c7ed5108b720d9a579e859a95f311f79b4cb36f1c2c673f83bc047&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 02 May 2025 10:22:28 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Reasoning的最终答案可能不是模型想要的答案！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajhB24sPyaVd1yFKFxEz4Zu2v2CnwQcchRK2JibicMQzopwiccQ9uYpK66FYB04Aniautibib4pzgcUmYag/300?wxtype=jpeg&amp;wxfrom=0"/><p>打破常规：为什么LLM的最终答案可能不靠谱？大型语言模型（如ChatGPT）解决复杂问题时，通常会生成一段“推理过程”，最后给出答案。传统评估方法只看最终答案的对错，但论文提出一个反直觉的观点：最终答</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538432&amp;idx=2&amp;sn=1c638d9975e82cff98fc3997dd1b8d42&amp;chksm=eaa857d33672232c982a73c1ba8d9fc6217deeca4481e2717235328da84e77a4ba297f439904&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 02 May 2025 10:22:28 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[本周六直播预约 | Test-Time Reasoning综述分享！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajhB24sPyaVd1yFKFxEz4ZuDZGoibTFBhcIhjxgwxe3icWVnPAkqXkb4vj8r80PoTNT4fkORsa8cRqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>主题Test time scaling 综述! 从what, how, where 和how well帮你系统解构！时间2025.5.3 10:30 北京时间内容介绍论文：What, How, Whe</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538432&amp;idx=3&amp;sn=e86ba5067bb2de103c266992160b004e&amp;chksm=ea30cb59ca18cd67412f7f6c8af059266d9ae88d21f5c96b0888e3b97e919fe10443c52fa223&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 02 May 2025 10:22:28 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[体积大、算得慢，如何推的快？LLM高效推理服务最新最全综述！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajGdRbqibZxzSkycIkJ0Yz8ReNIuLr6moMnKbywN3UB9XycGdzzDA6xSNp9BXtBquic5pGj5uANcgnQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家都知道的一件事就是LLM真好用，就是太大了。咋部署才能更加高效呢？真的是个让脑阔疼的问题。比如最近的几十万下载量Mistral-Small-24B-Instruct-2501和 Llama-3.3</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538405&amp;idx=1&amp;sn=b4488ef8f5c55aa8b1f69fcc93da3465&amp;chksm=eab77b1d25963701c3f483dafc4ac32ffc2adbd50f9d74b3649b024ecea54dc922f6e27b6d25&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 01 May 2025 10:52:10 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[直播预约 | Test time scaling 综述! 从what, how, where 和how well帮你系统解构！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajGdRbqibZxzSkycIkJ0Yz8Rib0TD2Rbg3HWJJ4tySAkMbVW8l2yPwpicuyKILOUItRibexxf4fLC1shQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>主题Test time scaling 综述! 从what, how, where 和how well帮你系统解构！时间2025.5.3 10:30 北京时间内容介绍论文：What, How, Whe</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538405&amp;idx=2&amp;sn=f4c2a32943d60e6af56a079d3f9e09b6&amp;chksm=ead76eda627fa229e2cd9b6c99324d23e29801effe60fe694884f127ae11d6e8acd221bd5593&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 01 May 2025 10:52:10 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[PD分离也有问题？semi-PD降低两倍延时，增加一半吞吐！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bajGdRbqibZxzSkycIkJ0Yz8RibkFsQ7PagYVz6qU9Mia9iaPjNng5uDZzRibIMs7ibhYOso7EM53ic5aYC1g/300?wxtype=jpeg&amp;wxfrom=0"/><p>大模型服务的效率困局大型语言模型（如ChatGPT、Llama）的服务过程分为两个阶段：预填充（prefill）和 解码（decode）。前者负责理解用户输入并生成第一个词，后者像“打字机”一样逐个输</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538405&amp;idx=3&amp;sn=d7ace18f3ae0cc65ba4d173df0d9baa6&amp;chksm=ead67f71936efe3e4e54865ef5530b66cc5f2cafb3658244456c2d4d75073d5d30e41cea2763&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 01 May 2025 10:52:10 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[小心！你的手机正在偷偷『学做人』——LLM如何让它自主订咖啡、抢优惠？| 手机端GUI Agent综述]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiapATcrAicCCmYlE5dia9BykQjld4KZGj5IwLNDQ4EaF0ScogNfticSqcdV3WoIfSOBFRsicBGOeU4r2Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>手机自动化的革命：从「机械操作」到「智能大脑」想象一下，你对着手机说「帮我订一杯拿铁」，手机自动打开外卖APP、选好咖啡、填写地址并支付——整个过程无需任何手动操作。这就是LLM（大语言模型）驱动的手</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538367&amp;idx=1&amp;sn=f74c1c4563d8d3f5d590dcd976ff7201&amp;chksm=eae36a443eb14d1cff22534d53400352ab108ee2c3cf56a670fcdd8e0e791b862df693f46635&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Apr 2025 14:41:41 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[可信大模型 Reliable LLM（四）：利用不确定性估计强化大模型的事实性表达]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiapATcrAicCCmYlE5dia9BykQITHHHN2jibCu5Gq0p05LoicU0loOpv7XJT77uZgPibibibiaaHfK5NABBLWg/300?wxtype=jpeg&amp;wxfrom=0"/><p>知乎：Beyond Hsueh链接：https://zhuanlan.zhihu.com/p/16213981801本系列 blog 是有关大模型幻觉、知识、不确定性等方向的学习笔记分享，我会持续更新</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538367&amp;idx=2&amp;sn=e0230fd6e192503ebc94b26d5265f00e&amp;chksm=ea78f75e66baee21e6075c6b7d291f8b9ab8bfc01792ea5558245d5ceee1cabb7468aada1a81&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Apr 2025 14:41:41 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Reasoning涨点新方法：LLM「左右互搏」训练模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiapATcrAicCCmYlE5dia9BykQGb7ZrSl4HG1fbXbLpJsxI6icGSPSlDZ9tlKFxTJIbpZqlliaJlgM2WRQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>自我进化的新范式：用「左右互搏」训练模型！这一对抗训练的核心框架想象两个AI在玩「猫鼠游戏」：一个AI专门制造隐蔽的数学推理错误（比如偷偷把方程里的加号改成减号），另一个AI则化身「福尔摩斯」，火眼金</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538367&amp;idx=3&amp;sn=a8b4b73123153b6b2ef5cd97be37d60e&amp;chksm=ea05b3444bdd9b07859822d592a9c4e18eb592191f7a3b5d94f734f4c8149fdd78b1dfd8df42&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Apr 2025 14:41:41 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[推测性思维链SCoT：小模型“模仿”大模型，最高提速2.9倍，准确率几乎不降]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagh0MCSmj5RRdX2CVnFoeB1mJaJZQx8KUXY94LLBWxeru43RkrECTgiaJrT1xxpO6nCHFM8mb5m8QA/640?wxtype=jpeg&amp;wxfrom=0"/><p>现在的大模型（比如论文提到的Deepseek-R1）虽然能解决复杂数学题，但有两个致命缺点：体型庞大：动辄几百亿参数，像“超级计算机”一样耗资源；思考过程长：解一道题要生成几千甚至上万字的思维链（Ch</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538301&amp;idx=1&amp;sn=a10d91ccf6703a9d03d1893cfb4ba654&amp;chksm=ea20d817046bf5767356cde19a3d3cd88971fe1dfd0505e867d4df008e23011aa5468d62e96f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 29 Apr 2025 12:08:23 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[When2Call：哈佛认为LLM也需要“边界感”，要意识到是否何时需要工具调用]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagh0MCSmj5RRdX2CVnFoeB1bha3zyVWMgSwLlnVNUxKCibiaM1mGIyI7iboMg1VTIAibrcs9xXvjaC2iaw/300?wxtype=jpeg&amp;wxfrom=0"/><p>AI也需要“边界感”？工具调用背后的新问题如今的大语言模型（如ChatGPT）越来越擅长调用外部工具，比如查天气、查数据库。但问题来了：如果AI没有对应的工具，或者用户问题信息不全，它会怎么办？想象一</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538301&amp;idx=2&amp;sn=d1dd1163ac6fb8989b1b9239365519ca&amp;chksm=ea18e0fa036ca15c1dc4dc1e178216fb6e77ffe8cb1469ae64914876aa850f0345a19d58188a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 29 Apr 2025 12:08:23 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大+阿里提出快慢思考新方法：在多模态Reasoning上准确率涨10%，长度砍半]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagMGIXCyzVhiawY4Jjpa9x5KibqmCEvzibFUBe4L7gJHzNAEb5RlTly3rwfkaAptfjpXxlhrJNzlQOxQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>为什么大模型会“想太多”？当你被问到“1+1等于几”时，如果非要先写一篇《论加法本源》再回答“2”，这就是典型的“过度思考”。当前的大型视觉语言模型（LVLM）也面临同样问题：无论问题难易，它们都会生</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538275&amp;idx=1&amp;sn=959fd8c85780a51ceee9a78ab2aa5c45&amp;chksm=ea009cfc5e93d12510e23309a7ab96d613e97673ca87f02ad3fcd55c71a4aad9d77e9145075d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Apr 2025 13:12:07 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[直播预约 | LUFFY：让推理模型实现“即学即用”的强化学习训练方法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagMGIXCyzVhiawY4Jjpa9x5KkdaxHpbicdpCQju49webpyO4jy5IUwQiaLzIibhw5HL7KVzBU19n0lIBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>主题LUFFY：让推理模型实现“即学即用”的强化学习训练方法时间2025.4.29 20:00 北京时间分享内容paper: Learning to Reason under Off-Policy G</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538275&amp;idx=2&amp;sn=7ae8c129c62000bbfdf842e7a49c30d7&amp;chksm=eaf921219af75f19ce0d1fa0800a97b9419ddf62c60dd9fccc2f3310c0cb95883e8e82e9cca3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Apr 2025 13:12:07 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[不用训练！TRACE让LLM生成既安全又有料]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagMGIXCyzVhiawY4Jjpa9x5K4XZWsibXbCh97asiciavlp3AzA3XDZtb9yib3xUudPxCIG1s1eE82hAMCQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>为什么需要控制生成内容？如今的大语言模型（如GPT、Gemma）虽然能写诗聊天，但有个致命问题——它们像“走一步看一步”的短视者。生成内容时，模型只根据当前已生成的文本预测下一个词，无法全局考虑“整段</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538275&amp;idx=3&amp;sn=228503600aee8179bc4d301c2ba04b5a&amp;chksm=ea2f15d241cc223a9123834a39aa57906f3694a51457135c18cf54105dcc22f0469c93ec0320&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Apr 2025 13:12:07 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[特征工程、模型结构、AIGC——大模型在推荐系统中的3大落地方向]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bagMGIXCyzVhiawY4Jjpa9x5KCQRKak6icJ1hblXP5J0M36c0uic4anSWEKbNGEdTxECroy39mia8Qu6NA/300?wxtype=jpeg&amp;wxfrom=0"/><p>特征工程、模型结构、AIGC——大模型在推荐系统中的3大落地方向今天我们谈谈一个搜广推行业这两年怎么都绕不开的一个话题，大模型在推荐系统中的应用。两年前，我们可以说大模型是推荐系统的未来，但如今，大模</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538275&amp;idx=4&amp;sn=5c1f55b6f8954c99c0c0e4b55ccbfb22&amp;chksm=eaeba874c9b0661aa68bf63cd1e1ff065b1b6aff2a7a33e7297fdf198303884b17ebdc57067f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Apr 2025 13:12:07 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[谷歌发现LLM是Greedy Agent，提出用RL调教出理性决策]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaB1aa7ZM4O8fSkf0YJlnQtRH2iaNWdejqh8TdiaIv99G8EXpRibD6ibqY3z164oiaoxSice2V7UG3BPCSw/640?wxtype=jpeg&amp;wxfrom=0"/><p>大模型的“决策短板”从何而来？大语言模型（如ChatGPT、Gemma2）在文本生成、代码编写等领域大放异彩，但当它们被用作“智能体”做决策时，却常犯低级错误：比如玩井字棋胜率只有15%（不如随机玩家</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538240&amp;idx=1&amp;sn=80ae3560e3cfde3c0d32449b93585131&amp;chksm=eacc6b5a23cf17563ff1a3355fdbc768057d1635bf569869def5b1d3613b1b895aa68a6d8f56&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 27 Apr 2025 13:32:04 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Tiny Reasoning模型：LoRA+RL=9美元训练费，性能碾压同行]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6baiaB1aa7ZM4O8fSkf0YJlnQtcNTUBj9ZqD43lBibOJgPhkKkeXMd8H5I50Z5Aia2LBDr6xScU9dWyqtg/300?wxtype=jpeg&amp;wxfrom=0"/><p>低成本也能训练“聪明”小模型？Tina的野心当前大语言模型动辄千亿参数，训练成本高达数百万美元，但Tina团队反其道而行——用1.5B参数的“迷你”模型，搭配创新方法，实现低成本高效推理。核心问题：如</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538240&amp;idx=2&amp;sn=62045adadb2756b6e62e93d8f3408aba&amp;chksm=ea52ec561c681b8ff13d7533dfe371d50859375dd9d5a40e8f9955b887a9017c04b20ad08ef3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 27 Apr 2025 13:32:04 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[NUS发布Reasoning中的安全问题综述，idea满满~]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahqxXwfEf6HqS9H9usmkMaegVc2lztdYENhsbtIzia6U2S5ibEGoma2yjuYoaKu5Vr8wzmhc6CcyD2w/640?wxtype=jpeg&amp;wxfrom=0"/><p>当AI学会“思考”，安全问题如何破局？最近，以DeepSeek-R1、OpenAI的o1系列为代表的大型推理模型（LRMs）横空出世。它们不仅能生成答案，还能像人类一样“写草稿”“分步骤推导”，在数学</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538212&amp;idx=1&amp;sn=7d50d37f025fc1b90ed45feae0ce731c&amp;chksm=ea53fbb24e44870561ed70bf702b0d6d8b92ae68516192f8cae03812f360fa9f86c5222cb089&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Apr 2025 10:57:04 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[大模型在线辅导小模型，正确率提50%、推理效率涨90%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahqxXwfEf6HqS9H9usmkMaeRVjvq7SftBl7zSiaWBVRMILic4DIeQIfLdkicGXBb36MPsqjRskOfVCIw/300?wxtype=jpeg&amp;wxfrom=0"/><p>想一下，一个刚学数学的小学生（小模型SLM），虽然做题快，但遇到复杂问题就容易卡壳。而博士生导师（大模型LLM）知识渊博，但计算成本高。论文：Guiding Reasoning in Small La</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538212&amp;idx=2&amp;sn=0d9d56848222474b404512214395f834&amp;chksm=ea5a94a37cc547780a711319c98b4fe2836f2b0aef9a6ccb019d7e74c776f7acc3c326a20503&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Apr 2025 10:57:04 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里发布多模态UniME：硬负样本+知识蒸馏=性能天花板，准确率暴涨27%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/gKaxjIx6bahqxXwfEf6HqS9H9usmkMaeXbblbgm1ASLLeYrbicniaqAcUbZF7LBwfRGUn0lCXh8Z3muEK0VEVQeA/300?wxtype=jpeg&amp;wxfrom=0"/><p>为什么传统模型不够用？多模态模型（比如大家熟悉的CLIP）在图像-文本匹配任务中表现不错，但存在三大硬伤：文本截断：只能处理77个token，长文本直接“被砍头”；孤立编码：图片和文本分开处理，缺乏深</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&amp;mid=2247538212&amp;idx=3&amp;sn=63eb17827ac018078f3f025014f677e7&amp;chksm=ea538e9fd5840d02b3498f8822da1c5a780eb9907493e90ee3f531bf3a424a9d415e6ff044b4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Apr 2025 10:57:04 +0000</pubDate>
    </item>
  </channel>
</rss>