<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[NLP工作站]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[NLP工作站公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_a40a957ae2a2.jpg</url>
      

      <title>gh_a40a957ae2a2</title>
      

    </image>
    




















    <item>
      <title><![CDATA[LLM 预训练到头了吗？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5mgNw8j9VaKicnQ1LDoAiajibQxVzZ9IeSUXeaLb5wybPXk8RkGwDiaSEAoRozd7xMbw2ZNibHCfsdPdxg/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来的是好友@Binyuan的一篇想法，主要是对Ilya的“pre-training as we know it will end” 观点的看法。正文如下：最近，Ilya 在 NeurIPS</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491219&amp;idx=1&amp;sn=9800a9d53cebb0c0061401a57ede6c68&amp;chksm=ce24c0819166f4d8fb77259a63da2d46bde2bfa1d093425f0422f04320d7408c3ae4c9424e6b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 01 Jan 2025 03:11:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[回顾2024：与LLM又相伴一年的经历与思考]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5lxJofZY46QNM7TtqxicGBWr9hJMd8DZsTHLsaI6yDeARqcN8fOJ3Vfud8GRWHBsVmAx9icubfNWYJg/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是刘聪NLP。又到了一年一度年终总结时刻，不过今年这篇总结，跟往年的不同，今年只聊LLM。2024年是LLM蓬勃发展的第二年，只能说发展确实十分迅速，层出不穷的，但也让很多人看清了LLM现有</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491215&amp;idx=1&amp;sn=e5c6752701a334ca7414422abb857a43&amp;chksm=ce733e150badc2426a706503a9f6a7d156124e2629c566dcc8b5f461592a7c8a7f0a655c59c0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 31 Dec 2024 01:09:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[多模态大模型在表格解析任务上效果如何？亲身经历全是泪！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5n0ib2QaKK8tj3C5BlRssc8JBQlvrHpGDjic0QsqAJCicQKaMic25wWvpY8g6m12LrQ1Ticu7JYSa8spkw/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是刘聪NLP。前段时间一直都在尝试用多模态大模型进行落地应用，除了问答之外，那么最容易想到的就是文档解析了。一来多模态大模型本身就有强大的OCR功能，二来知识加工对于大模型落地来说也是重中之</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491195&amp;idx=1&amp;sn=04f9092b0bb06404ad149fea8f347d47&amp;chksm=ceb980b380b493c8d8d8a88a2af7b637b2319c245e3afcaf1743502540473e74c047635b2728&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 26 Dec 2024 01:20:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[QVQ-72B，如期而至！继QWQ后，通义千问又开源视觉推理大模型！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5lDrFs3QsIxrEGeoC2uqZbJXIrsmOD6TXVFLbkHjsBiaTDoVOIVgefdBZJ1NEDqo8ImicaPDvpMaL3A/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是刘聪NLP。没错，是的，对的，很棒，千问！QWQ之后，千问团队又开源了视觉推理大模型QVQ，是72B的呦。圣诞快乐，如期而至！HF: https://huggingface.co/Qwen</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491156&amp;idx=1&amp;sn=a015a5c0a756975238e25a2653dcd14a&amp;chksm=ced769e3ecd8d7f49d95cc6ffbb5f4962f6a7656d27b0e5dade92c13d5a66cd1288a102a9597&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 25 Dec 2024 00:00:10 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM 又过了一年！！！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5lDrFs3QsIxrEGeoC2uqZbJXIrsmOD6TXVFLbkHjsBiaTDoVOIVgefdBZJ1NEDqo8ImicaPDvpMaL3A/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来好友知乎@ybq的一篇文章，《LLM 又一年》。LLM 的第二年就要结束了，如果 2023 年的主题叫“从零到一”，那么 2024 年的主题无疑是“颠覆认知”。知乎：https://zh</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491145&amp;idx=1&amp;sn=2453ea40ade7d24756ac84f64455d6b9&amp;chksm=cef630874399c8ab916f3b5b9800c73129d6501e128071728a975630d0943d9852b782315eae&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 24 Dec 2024 15:55:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[技术人该积累什么，才能避免被AI淘汰？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5l8uuaWFH5xbhu792Ne2uhKvh3IncadZ7JAoicvBicy6P9iboQSD2536fRNxxuOaGZSOjAkyE5KWk1iaw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来好友知乎@ybq的一篇文章，聊聊技术人该积累什么，才能避免被AI淘汰？知乎：https://www.zhihu.com/question/7440697804/answer/612126</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491139&amp;idx=1&amp;sn=125a1465b92b8c4d5bce3b4fc4509cfc&amp;chksm=ce00a10de03085a2afbec2f627b427d4f7ce5feed1fed8935a6573c73c65f4037949acaf7deb&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 22 Dec 2024 16:15:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[对OpenAI o3模型的看法、思考与反思]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5kGd8Kic6V3Q4GWHWSHBn84J6ef95I0EXwDWAsQGX33V9kHjtmCA1RkpvoicibBwc1FUiatm23OavUbMA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来一篇博杰兄（@知乎 李博杰）关于o3 模型 思考的文章，如何看待 OpenAI 最新发布的 o3 模型？知乎：https://www.zhihu.com/question/7416922</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491134&amp;idx=1&amp;sn=11c8328f22c5728290a1412362191c6b&amp;chksm=ce8646ffc213474901b0f1f6543eea8a38faca1765fdbcf17bd9941e627fc8b0a9be5a8bdf6a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 22 Dec 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[别再用PostgreSQL了,Milvus才是多语言RAG的最佳搭档]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5ls4IqHnMkFMyTWSYRd9XreicblYv5a98M6PC2h1KHtEvXtoUkNGUTs8eL6AEORdflyxfCo32QNSgw/640?wxtype=jpeg&amp;wxfrom=0"/><p>你有没有想过,为什么我们的大脑能够轻松地理解"一朵花"、"a flower"和"一輪の花"其实指的是同一个事物?多语言 RAG 系统就像是在模仿人类大脑的这种神奇能力。它通过先进的向量技术,将不同语言</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491129&amp;idx=1&amp;sn=db7c194233b49d5007049b2b8ea6fab0&amp;chksm=cebaacca587be4a66585727726362f995233c7a04cf6e7176a8bf9b90ff903574d9b2e207aba&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 19 Dec 2024 09:26:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[实测！最新端侧全模态大模型Megrez-3B-Omni]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5kJeSoPVvAhMTU5wDqSOh8V8PZOfz79S9ic19FibJCwsgqmtAQXRiaSoPjNEa3qyvyegKMdUDawriaUow/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是刘聪NLP。中午看到无问芯穹开源了一个端侧全模态大模型-Megrez-3B-Omni，马上来测测看，效果如何。Github: https://github.com/infinigence/</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491123&amp;idx=1&amp;sn=9e23ddba2cfb63e29d04e9fb1bada27a&amp;chksm=cec997257c3da4c015cc929c7fc08b97a1184c0fe367fe196a71fd7264aebabc04c2145935a3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 16 Dec 2024 08:56:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[聊聊对强化微调（RFT）的理解及看法]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5nxYicQntALI7B9EibEjqOVGA8x15FwqM8CqvyspyLClPhJqehRzJiaHSO4zvIicoibx9qMarEZzogsO1g/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来一篇好友 知乎@ybq的文章，聊聊对RFT的理解及看法。作者：ybq 知乎：https://zhuanlan.zhihu.com/p/12328929529在看了 OpenAI 的直播，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491091&amp;idx=1&amp;sn=fb67e3779eb87f7838a833645e156d6d&amp;chksm=ce018b5f7662becaa1db8e148c256e021020066846df0d2052cb6c7ddfdbdb50b02f8c6ac83b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 13 Dec 2024 01:25:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一道涉及数学、生物、伦理的AI测试题，来测测各家大模型的推理能力]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5mVmvF2R449oFcibtwXcpibFlytBJujTnx1l2IwM1NNeANhl8MTkgibSbVwJo5ibd71JG1fsHGlW2VuCw/640?wxtype=jpeg&amp;wxfrom=0"/><p>声明：本题目仅用于测试大模型能力，不代表本人任何观点。起因是在@南乔的交流群里，看到了这样一道题，蛮有意思的，正好最近各家都在推o1推理大模型，来看看各家效果。测试题目：有一天，一个女孩参加数学考试只</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491085&amp;idx=1&amp;sn=5e0822a972a4fb59cd4e042e04e9db0d&amp;chksm=cef842ee5c4ccbe558f0cf1cf761d7380c2c0df993cff3e307853a57d522c6a6106f610df803&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 06 Dec 2024 03:43:13 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[长文 | RAG的实战指南及探索之路]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5ks86KiaXyRvOic8F9IiazvyjKbzibk5BQSQRMk9WtqoxolXkfLiamC1FNXxic7wwWsRH4spfdATTkAzdgQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来一篇知乎@孙鹏飞 的关于RAG实战的文章。作者：孙鹏飞 知乎：https://zhuanlan.zhihu.com/p/6822534961. 背景介绍RAG（Retrieval Aug</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491057&amp;idx=1&amp;sn=77c6c4792234bfa147871072ec968cea&amp;chksm=ce8c1c3fc85ac7dae34195bf1482d2245015bcd82627f870bf0505ce5268b07b5dfc2ea3fb86&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 03 Dec 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[瞎聊: SFT模型为何不如Pretrain模型]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5ksg05KCb1TzyLiakXMkkIohU75tOj3zoz8lyjcGTyoqFoh0Gawo2y0Y7DIqMJwAWic9DFSMmYlxPBg/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来好友知乎@ybq的一篇文章，聊聊SFT模型为何不如Pretrain模型。知乎：https://zhuanlan.zhihu.com/p/9649266595叠甲叠甲：本文纯瞎聊，单纯分享</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491019&amp;idx=1&amp;sn=0044d6689912b2f6b5542f705a5eb8b7&amp;chksm=ce78c7c92efe55c34b3a9bae64f20117b5c2eabc0d081313a33e51898ee05d9b8d1784741349&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 29 Nov 2024 01:40:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[该来的还是会来，Qwen团队开源推理大模型-QwQ！！！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5nKh7TDY49rZZBl25xYpoWX68iaBcCD2dcQRicddialEiauIcKeFZB7ysgYiaIJXoMG3JVJngLJoebEb9g/640?wxtype=jpeg&amp;wxfrom=0"/><p>它来了，Qwen团队开源推理大模型-QwQ最近国内的类o1系列模型疯狂发布，deepseek、kimi、skywork都发布了，现在Qwen也来了。今天Qwen团队也发布了推理大模型，不叫o1，叫Qw</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247491013&amp;idx=1&amp;sn=9f8b864c5e0173dcade717895adf2802&amp;chksm=ce0630af6cefb9205f359a87fe2164662ce6345a0c1029ba4067bca00dd888974c35ace585f5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 28 Nov 2024 01:51:16 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2024年，做大模型增量预训练（continue pretrain）的注意事项！！！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5nP6B4I4kUVz9sPiaUhDFUK9qpnIUnV1xvvia2ibnd7whxNZgQKJzYa70Nja0rdmf8B2MPspWXmjHp2w/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家带来一篇好友知乎@王焱的一篇关于大模型增量预训练的文章。作者：王焱 知乎：https://zhuanlan.zhihu.com/p/7077519011 背景去年，国内大模型赚钱最爽的一个方</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247490995&amp;idx=1&amp;sn=6cc70cba69f9701a737c81a1f2d4603d&amp;chksm=ce0f76bd54e714ab0383a52dc894f1faed8768894c4b47c61e251f46547aac51d6cc8f1b60d0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 27 Nov 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[测测Kimi新开的k0-math，你是数学模型，但我就测文本]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5nP6B4I4kUVz9sPiaUhDFUK9qpnIUnV1xvvia2ibnd7whxNZgQKJzYa70Nja0rdmf8B2MPspWXmjHp2w/640?wxtype=jpeg&amp;wxfrom=0"/><p>晚上发现kimi也更新了，之前网上流传的kimi在数学上对标o1的模型，可以测试了。感觉有点迫于deepseek的压力了，本来应该是国内第一个的，长推理、类o1的模型，现在变成了第二个。模型版本叫k0</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247490994&amp;idx=1&amp;sn=149786dfe60a72b72264f857a36cb871&amp;chksm=ced3d2f0321febeefb369663d9df25860d0f5e205314c4df9d1b31fef29a6317d2be8a71dac0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 25 Nov 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[写了一个月提示词（Prompt）后的感悟！！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5nP6B4I4kUVz9sPiaUhDFUK9qpnIUnV1xvvia2ibnd7whxNZgQKJzYa70Nja0rdmf8B2MPspWXmjHp2w/640?wxtype=jpeg&amp;wxfrom=0"/><p>Prompt工程的本质是将人类非线性思维转化为机器可理解的线性逻辑。这是一种从复杂到简单、从发散到收敛的思维重构过程。我们是需要将我们熟知的思维模式，翻译成机器能够“稳定”识别的模式。与人之间沟通不同</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247490972&amp;idx=1&amp;sn=23c65515cf04fd383b398cb541c9f366&amp;chksm=ce89864967cd309d6f617dde9de69cc14f7505dd49c9e6d424a17a3be882474d04639c4df774&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 25 Nov 2024 03:40:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[凑个热闹，测试一波DeepSeek新上的o1推理模型]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5kDibADpzOCECibyFLcIUOeqX0yV7utugBSxJ3fbBKXbgp1ZeLPE2A2wuQagM6RHvyM93Zek16WFUibw/640?wxtype=jpeg&amp;wxfrom=0"/><p>自从openai的o1出来之后，各大厂都在默默发力，几个月了，国内也开始有o1模型了。前两天，有kimi的 k0-math模型视频露出，昨天deepseek就来个大的，直接发布线上可测试的r1模型（每</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247490946&amp;idx=1&amp;sn=798edf700bcb8c7d7fb4f86632910c48&amp;chksm=ceca804c35df6034ced8cb6b7cb8cecd063c859cd2a2321ed37e99245ff8b8e705c5634fb39d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 21 Nov 2024 02:56:09 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[聊一聊做角色扮演大模型的经验]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5lTR4umCLS8NV3bh3MeX21Lb9qicm81VUXW2tkD1g2vNUxUhvcEdnzUNic5WTtZeWOu3rxZjl8JMGVw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在角色扮演这个领域也爬滚打了一段时间，分享一些自己的心得和思考。为了避免老板看到之后干掉我，有些细节就不展开了，多多包涵。作者：何先生 知乎：https://zhuanlan.zhihu.com/p/</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247490927&amp;idx=1&amp;sn=29e0bcd9677af512313b162e2ce5199e&amp;chksm=ce5cf18fa072df642c7c6bed5aeef98687f2c588ed8a383d47b4a8b6a0244189f329143dd6fc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 20 Nov 2024 02:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM实践系列-详聊OpenRLHF中的各种Loss]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5mZ5ljdNGNXWYMRuYDvGFibggAmG8libojRt9O4FSVdknRAqhuA4uiaiauKvUCFlDFAtpZcyvtclzibMrw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家来带好友知乎@ybq关于OpenRLHF的学习笔记，主要介绍其中的各种loss内容。作者：ybq 知乎：https://zhuanlan.zhihu.com/p/6290579087从这篇文</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&amp;mid=2247490917&amp;idx=1&amp;sn=253781eb66f7b7b16224fcb83309218f&amp;chksm=ce3088825fb675aed132cb7778a0eeee39ab313adaeb166491f8a59df616111b74bfb0478d76&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 18 Nov 2024 16:11:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
