<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[智谱开源最强旗舰GLM‑4.5：挑战全球前列的开源旗舰大模型，专为智能体应用打造。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enwVVjR2OrKwpn9VSkHhJHiavcjHQXgWGNzfttuXB3k7dzM4wc625Lbzr79ic8QdqP434KGiaQURcQTg/640?wxtype=jpeg&amp;wxfrom=0"/><p>在 2025 年 7 月28日，智谱正式推出 GLM‑4.5 系列大模型，这是继 GLM‑4 系列之后的又一次全面升级，也是智谱首次面向智能 Agent 应用场景定制的旗舰级基础模型。GLM‑4.5 </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494411&amp;idx=1&amp;sn=bf03f057f85c2584c882ad1d25893c0e&amp;chksm=fd8153ef19c6469610015ead3fcc18fced56226835b9e772b15fe3a19da0e5aed554bf70c0a2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 01 Aug 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[MeshAnything V2来了！30秒生成建模师级Mesh，最大可生成面数提升至1600。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elKcprhHqENugIHSUTwb3EOjn5XkV1zK43LvhbUTOvgJ8MMYraTDSoeD7ibTHT3bGcxX9ic5uAQfLGg/300?wxtype=jpeg&amp;wxfrom=0"/><p>GitHub已揽星1.9k的MeshAnything项目上新了V2版本，由来自南洋理工大学、清华大学、帝国理工学院、西湖大学等研究人员完成。MeshAnything V2相比V1，使用了最新提出的Ad</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494411&amp;idx=2&amp;sn=a413c7b5d67f8256a1cddc9dfbf4c2dc&amp;chksm=fd31b7d707ad0fea71296c50bb0e142523946287776b1285cb24b3ca4098ceed3b530291f69e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 01 Aug 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[突破GPU内存限制！VGGT-Long开源：首次将单目重建推至千米级、无边界室外环境]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/rFGeIHcFicWekTZib9BbIiaTmjnAkRWiaU6oZOyDb1XYo2PaGL8LwFicFIe8v6aTqzZb3Yia0FKibVHttq7YHJ5gL6DbA/300?wxtype=jpeg&amp;wxfrom=0"/><p>挖掘基础模型的潜力从单目RGB视频流中感知三维环境是自动驾驶的关键能力，然而现有方法在处理千米级长度且未标定的序列时仍面临困难。与小尺度的室内三维视觉任务不同，自动驾驶场景涉及长轨迹、稀疏的帧间关联、</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494411&amp;idx=3&amp;sn=cfe14310317924d22d48204a111fcf51&amp;chksm=fd5d38279d657f0a5978a3444b14246a5b7f780cecab3d3f35a8199801bddb684bc33b625cd2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 01 Aug 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[从单口相声到群口辩论：中山大学&amp;美团开源MultiTalk：多角色对话生成SOTA模型，语音-视觉对齐精度达98.7%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enfUCFX9WW23BajIFJBpRq3lz9nCs5icOy90Hv0zVbmIjdyTsfJWWDS7Fo3ugfyXkMKIEyJEtsAoHg/300?wxtype=jpeg&amp;wxfrom=0"/><p>由中山大学、美团、港科大开源的 MultiTalk 可实现多虚拟人对话视频生成。在语音与嘴形同步方面达到了SOTA性能，并支持通过prompt实现人物、物体与场景的交互。相关链接主页：https://</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494411&amp;idx=4&amp;sn=1368947368c0905b8dbb87f3fcf142d9&amp;chksm=fd625ea8204b5c5770529ec2a2ed262dd308411839f33fffc8018cea9628d5ab4b2f5705f8a0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 01 Aug 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[一沙一世界，一花一天堂！腾讯发布首个开源、可仿真、沉浸式3D世界生成模型 HunyuanWorld-1.0!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enPCjTfhE2exCxyco6laPao2lnHZq3ogpmmib5iasCmJnXmWuMH87RJ0LxT6CnuVbvj0gdyn5g39w6g/640?wxtype=jpeg&amp;wxfrom=0"/><p>计算机视觉与图形学中，从文本或图像创建可沉浸交互的 3D 世界挑战重重。现有基于视频的方法多样却缺 3D 一致性与渲染效率，基于 3D 的方法有几何一致性，但受训练数据和内存效率限制。腾讯开源的 Hu</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494385&amp;idx=1&amp;sn=e0adc354dad8f32f1aa8625e918a7a6b&amp;chksm=fdba33584d8f35ede3c538b7bbe57671aa61817a86cb7ac00d2b02fb29c7ccdd772a1811407f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 31 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[让SDXL实现50倍加速！中山&amp;字节最新对抗训练+双空间判别，单步生成新标杆！性能狂飙]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icoia1S5Ym7bibDIvuNABiayExIP2s1GG5jBXCzgctBupACNEyZWybv3icTdFsEZwl5CqqkmoQ0LiciazJ76Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>文章地址：https://arxiv.org/pdf/2507.18569 亮点直击对抗分布匹配（ADM）：提出一种新的对抗学习框架，利用扩散判别器在隐空间对齐真实和伪造分数估计器的预测，替代传统显式</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494385&amp;idx=2&amp;sn=880010a11bda7a1cf1f7ad4f6ffe35d1&amp;chksm=fdf917e4d34d7ef939f2504bae652668a1ccfce0b49f37667fc1697cb2ceccb17491c77b5ee6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 31 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[图像编辑革命，万物皆可插入！浙大/哈佛/南洋理工提出Insert Anything，告别PS抠图，AI让世界无缝生长。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enmjqTKh2qwkPiauc2Ejsn7Ficnb2ehPShfDudYtibS1fkY0Su3IFmdP3MkS9KDH1gsquQnXh8Ku6TPQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>浙江大学、哈佛大学、南洋理工大学联合提出了统一的图像插入框架Insert Anything，支持多种实际场景，包括艺术创作、逼真的脸部交换、电影场景构图、虚拟服装试穿、配饰定制和数字道具更换，下图展示</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494385&amp;idx=3&amp;sn=d73bf56eac35f71564b51bd7bb2bc07e&amp;chksm=fd6739e4c895ad762e694033c2459e3c761023feae006aed8a811449e550fa040051974e7786&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 31 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI创作从此无所不能！复旦大学提出UniCombine！多条件可控生成的终极武器！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5KrBKjz80g2WN9kgcLCdSvBgBqO9AvwpQCkInibAg65CoUM759Xzic4Ynw8E0DGia05YuibNc81chZQFg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer论文链接：https:/</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494385&amp;idx=4&amp;sn=67fdc97e51229b211bab7987341ac87e&amp;chksm=fd911a316444ef6ae795eccbd8f3a30b8c640fb7bbcc0976c60367a894c069909c7a856b32cd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 31 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[告别繁琐！ComfyUI LoRA Manager上线！配方管理+一键集成，模型查找与下载“零等待”！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enbjDSflysJFeRa1uPbPgmjKl0F8kLoBicOVibPKVBTiaPZejd9PaWA3NQ7pjbhFejD8lORouqAXsjOA/640?wxtype=jpeg&amp;wxfrom=0"/><p>unsetunsetComfyUI LoRA管理器unsetunset使用 ComfyUI 的终极 LoRA 伴侣彻底改变您的工作流程！ComfyUI 提供全面的工具集，简化 LoRA 模型的组织、下</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494334&amp;idx=1&amp;sn=00db8edd31cf655de3f724b1000f5b9b&amp;chksm=fdc04332369e001152aef44f7a5bf24614a77f2eabb26e0a4051d9d7e279aa51abaa4c483300&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[腾讯发布混元-3D 2.0: 首个开源高质3D-DiT生成大模型，几何结构更加精致，纹理色彩更加丰富。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enScf5fx9rxa1dXnvYHW4G815SyibP84GYLJGItGoKdb7k8ibSoFf2UCRYRf4VJVbpVm4IevhxibbLDw/300?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中已经和大家介绍过腾讯HunYuan-3D 1.0，感兴趣的小伙伴可以点击下面链接阅读~腾讯发布HunYuan-3D，支持文本到3D和图像到3D，10秒即可生成高分辨率细3D模型。HunY</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494334&amp;idx=2&amp;sn=738bb4f709c1d6052525dbf72ba0a14b&amp;chksm=fd288b3c805444807e18bd50c014ae1eacc1843882112ae669fbee8999d28c3a1f3d388545a2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[音乐版 ChatGPT 就在这里：Suno V3几秒钟内就可创建两分钟高质量的完整歌曲！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em1Jmm3JnCF4N6JNjZ7m3HZdP70FAps5gwtDs9CCtuIicp4h5fqd34SyiaBOOPtWEpDvK18SkJQYw2Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>仅在过去的一年里，生成式人工智能在生成文本、图像甚至视频方面取得了重大进展，特别是使用OpenAI的新Sora工具。但音频，尤其是音乐却滞后了。Suno似乎正在破解人工智能音乐的密码。Suno正在建设</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494334&amp;idx=3&amp;sn=b51c12184ec21aea60217c78eb807c50&amp;chksm=fd4ec0613c255553c091ad8969ed93da4fcb12246c5843239747ce3378922b321efea45e6ccc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494334&amp;idx=4&amp;sn=90a5212efeab7b7d79cdee969577df2c&amp;chksm=fd4aca19be2ebfd7ebe7cebb1851fe09c085ccade92814290166b789d4416d34e7d9db1b440d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[企业/个人开发者狂欢！字节跳动宣布开源 Coze Studio 和 Coze Loop，AI Agent 开发进入平民化时代。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enbjDSflysJFeRa1uPbPgmjAcxaantLMyv9tXrOibIpxUX3DfMiap9YULj2wC2qf84tE3egdxibsq5pQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>2025 年 7 月 25 日，字节跳动宣布将其AI Agent开发平台Coze的两大核心项目——Coze Studio和Coze Loop——正式开源，而且是还是 Apache 2.0 协议，任何组</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494333&amp;idx=1&amp;sn=d3760f4b6c7f5bdbcbbea7fabf36b5b4&amp;chksm=fd618878a4528110105609da9ad07e475d822fb9478ed5ac685ca0fdd1885ce20ee49a15ffaf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 29 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工 &amp; 牛津 &amp; 新加坡理工提出Amodal3R，可从遮挡 2D 图像重建完整 3D 资产，3D生成也卷起来了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enS6n92rGmqtJayOlicyqJq600IyDZicDbCN0IrvrTs03kGrs6dbzAyHZXniaUX6rcbNQPn1B25vgaJw/300?wxtype=jpeg&amp;wxfrom=0"/><p>Amodal3R 是一种条件式 3D 生成模型，能够从部分可见的 2D 物体图像中推测并重建完整的 3D 形态和外观，显著提升遮挡场景下的 3D 重建质量。给定图像中 部分可见的物体，Amodal3R</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494333&amp;idx=2&amp;sn=79c96ac4a3a61d21e5332a291de55e48&amp;chksm=fdd83c3764381455bbed871df26a3b69bd9ffcfe8547d6eff1b102e940eeab6eab984dbe2e85&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 29 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 字节提出个性化多人图像生成新方法ID-Patch，可生成多人合影、姿势可控。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emCuicERoV3guOMh64VYNrcA6VO1uBfS3aIicTCtKS3eFEBxCVDPwXCyj0Fye0L4toEplkN73YiaibibFw/300?wxtype=jpeg&amp;wxfrom=0"/><p>相信扩散模型（DMs）大家一定都不陌生了，目前已经成为文本生成图像的核心方法，凭借强大的图像生成能力，正重塑艺术创作、广告设计、社交媒体内容生产格局。现在，用一段文字生成个性化头像都不算啥新鲜事儿了。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494333&amp;idx=3&amp;sn=d8227928a5ef5b44ce9eceea9728c635&amp;chksm=fd7eb40ff74f6db1704796fbf5153b361f97167b85c1c9c953e6a43d5501ecf7be4bad0e09f7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 29 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦&amp;字节提出layout-to-image新范式，支持基于布局的MM-DiT架构下可控图像生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekXX8zYF4UxzjmCibmVsNeNfHdzvia0ykHy5vQljhxHZhBKib0DHCIdedbOAMsic8KZ423vtGia19o4Wow/300?wxtype=jpeg&amp;wxfrom=0"/><p>本篇分享论文CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494333&amp;idx=4&amp;sn=3754aaf24f5cba64798ec562dbe98725&amp;chksm=fd068feead28b39d3a2214cf3133378bab99c812b4bd1d8d7dcba65254c7afed805a3e665c1c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 29 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[图像编辑进入视频时代！字节Seed&amp;新国大提出VINCIE，视频驱动扩散模型，概念合成效率提升300%。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ennrAAVlvm6a2ndtb8NAAAehQOd2pAp8oOdmjraUibFjm5UwibicaIfLZpvl4licgd3FvIqQuDnyebylw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在图像编辑领域，如何让模型真正理解并响应动态变化的上下文需求，始终是横亘在技术落地前的关键挑战。传统方法依赖专家设计的任务流程与分割修复等辅助模型，不仅数据标注成本高昂，更难以应对复杂多变的编辑场景。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494319&amp;idx=1&amp;sn=aa34b2508066ad87d8d9064b459dc87d&amp;chksm=fdb4b1fd6e5bd129392456675effecdb0f11c52e90bb251b88e704ba9206a18558bd5d327de2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI创作从此无所不能！复旦大学提出UniCombine！多条件可控生成的终极武器！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5KrBKjz80g2WN9kgcLCdSvBgBqO9AvwpQCkInibAg65CoUM759Xzic4Ynw8E0DGia05YuibNc81chZQFg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer论文链接：https:/</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494319&amp;idx=2&amp;sn=ea12b270a8c396ccc909cbf5fa448cba&amp;chksm=fdf68739535c71b122e003c6d6a506b479b5d7921d9e436f4a996e7acd7104364bd3aa91ca0f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Poetry2Image：专为中文古诗词图像生成，忠于原诗意境和语义。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emh13jOMSY9oYmD0NHOx8BcYfwYJj74Cog1EPA8EQnekKhKwrxDasX2PxLvN7VqWDL8nRUrassVIw/300?wxtype=jpeg&amp;wxfrom=0"/><p>直接基于诗句中的文本进行图像生成通常会导致丢失图像中的关键元素。为了解决此问题，哈工大提出Poetry2Image，通过实施有针对性的图像校正解决这个问题，有效地捕捉这首诗所传达的语义和艺术精髓。Po</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494319&amp;idx=3&amp;sn=560c5696cf68d27a29d6bec67b566608&amp;chksm=fdcb990de8fcd4725a7d3e856ce9cc46e0c570107ac187d41fbcb06574ae2f103028894fbb94&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI 艺术工具通讯]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/5LJDib8HPR2rrZo6MvQMib3yaV5BITXF7CQorbzEicSeSiaBVwm9FpIicKhJ3TeBW7JsFmeMmjr8CqMt5ibicJRkEmjnQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>创刊号 🎉AI 领域的发展速度令人惊叹，回想一年前我们还在为生成正确手指数量的人像而苦苦挣扎的场景，恍如隔世 😂。过去两年对开源模型和艺术创作工具而言具有里程碑意义。创意表达的 AI 工具从未像现在这</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494319&amp;idx=4&amp;sn=e7b935214dcd48f6984c3d4ac725c67e&amp;chksm=fd60f6a187d887efa3dc3d30f239d861aec26d0611f30d4f7e18941212ba4ce942456d442e9f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI生成电影新革命！字节&amp;港中文等提出Captain Cinema，当「无限记忆」打破〈盗梦空间〉的第四面墙。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ennrAAVlvm6a2ndtb8NAAAeECKiaXibgWMfXuoBib9zKibRu3JMzPKAibRp6rfe0wartxB8M8rwGXMibnyw/640?wxtype=jpeg&amp;wxfrom=0"/><p>由约翰霍普金斯大学、字节跳动，斯坦福大学、香港中文大学联合提出的 Captain Cinema旨在创作具有专业电影级品质的多场景电影，同时 通过超长上下文记忆保持角色和场景的一致性。你可以成为导演，用</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494318&amp;idx=1&amp;sn=6bf1a40ac4cff47d4ae11f07db81ea05&amp;chksm=fd6cc02bc103bf5f3b7c1816d044b952140109d2610b6119aeefef7de4ab8e337cb152624230&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 27 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里最强代码模型Qwen3-Coder发布：多尺寸选择，开启编码新体验！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em9xtmb1LYQQ3l0fNDDWpS4R9kV73WWUJibv1BBPEzpA4GiczgHXPoApCNSWLpvhlypT6L0bvGhTEUA/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里推出了迄今为止最具代理性的代码模型Qwen3-Coder，Qwen3 -Coder有多种尺寸可供选择，首先推出的是最强大的版本：Qwen3-Coder-480B-A35B-Instruct。它具有</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494318&amp;idx=2&amp;sn=741183eb685a5164df579e59c95175da&amp;chksm=fd704d1e87a21bce0c21ab3ce316c858a888c4d2d670467e19ead99f2f0210aea9bb7f3954f0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 27 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 机器人双臂操控新突破！KStar Diffuser如何解决自碰撞与运动约束世纪难题？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icojvz9clmicqUEHWru0TSQwicibDxwd6pXeiac1QbZwUoibnWeMnE5ib2jBibpdEXVK5T4bFCwMWWK9BcS4dg/300?wxtype=jpeg&amp;wxfrom=0"/><p>文章链接：https://arxiv.org/pdf/2503.10743亮点直击与现有方法仅在笛卡尔空间中优化末端执行器姿态不同，提出了一种新颖的时空机器人图，显式地建模机器人物理配置，以指导生成动</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494318&amp;idx=3&amp;sn=9566b386d2548ecb07e7751571683f7e&amp;chksm=fd77915f965b801367f1bb7df7557f9c2b7d211d91e830e28d5d1be0845c2ba97eea2ac39a51&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 27 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[开源多模态生成模型新标杆！OmniGen2：支持视觉理解、文生图、图像编辑等任务，探索高级多模态生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek7H0AmSXtLibjgFibN8Hs8yrrhZa6JxHCHPbYCDGPOoQiaWTNCX0KMvXDq8E2VibCNrFhOQZicibkpSffw/300?wxtype=jpeg&amp;wxfrom=0"/><p>由北京人工智能研究院提出的 OmniGen2 是一个统一的多模态生成模型，它将强大的视觉理解、文本到图像的合成、基于指令的图像编辑以及主题驱动的上下文生成功能整合在一个框架内。它基于解耦架构，在保留高</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494318&amp;idx=4&amp;sn=074035dd45ebd4756116439aa1693984&amp;chksm=fd80af7055dbb4f117082a3681755809e29dd77faa5723971741f3cf745b737e01a46bcbd5fb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 27 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工提出 Ultra3D：6.7 倍加速突破效率瓶颈，1024 分辨率下登顶 3D 生成性能巅峰。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elVVXYVXeZKg9ibO6JibLeUheArjN7Xm2cOz07j7Uy5AIOxXedaPSFmFxlbPp39YvTiao133ztkagpEQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在 3D 内容生成领域，稀疏体素表示虽为高分辨率建模带来曙光，但现有框架却因两阶段扩散流程中注意力机制的二次复杂度，陷入计算效率低下的困境。不过，南洋理工大学的研究团队带来了突破性成果！他们提出的 U</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494284&amp;idx=1&amp;sn=cea7e65dd5ccf09f1386a29a33ca20a6&amp;chksm=fd6bc9e87837ba2224a45389d9a7239de6e52dfdb66d0a7b668eb6565ca4eda39e3c993d4c13&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Jul 2025 03:32:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[从文本到3D的“零训练”革命！英伟达&amp;康奈尔大学提出 ArtiScene：通过2D中介实现高保真3D场景合成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48HPJzGndEliaF6RR9oS2mcY6tk1tO13iaxO3UHzBrtzwlN2jFlpv4651g/300?wxtype=jpeg&amp;wxfrom=0"/><p>由英伟达和康奈尔大学提出的 ArtiScene 是一种无需训练、语言驱动的 3D 场景生成流程，它可以根据文本提示，设计出丰富多样、美观且易于编辑的场景，涵盖各种类别和风格。下图中展示了四种结果，并附</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494284&amp;idx=2&amp;sn=ff46e830afa0c44d78be7bd299fa033c&amp;chksm=fd06e608ed0cd51765151c788d5c3ed7732e30eedfcc856ac542417a3aa26193b7314662cde5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Jul 2025 03:32:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工力作Sparc3D：开启三维重建可微分优化与高效生成新纪元。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enhd8Da8niase1WltgKePj289UYQ2FkGK7uxrgpyoOIA6cIHk7jU4q6hvNUWTsCz3qI24ic8ibqQ8GjQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>由南洋理工大学推出 Sparc3D 是一个统一的框架，它将稀疏可变形行进立方体表示Sparcubes与新型编码器Sparconv-VAE相结合。Sparcubes 通过将有符号距离和变形场散射到稀疏立</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494284&amp;idx=3&amp;sn=f6b862e8a50403d358e378ff24a37c28&amp;chksm=fdd8635c097f9e6b788c992c4c125ae0d8d257e3f4c9d86e8d0b38cc6dcd48c6e63b5f016074&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Jul 2025 03:32:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[突破高分辨率3D建模算力瓶颈！南大&amp;复旦提出 Direct3D‑S2：8卡即可训练，革新 1024³ 分辨率3D生成格局！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elCicOusPT4UMbWRggQc9icnnibKbtwkfQH4wt4miaD9ltwEVcbK2fR9bxibXxuHKTtr0qEAr49WVVsnag/300?wxtype=jpeg&amp;wxfrom=0"/><p>介绍 在 3D 生成领域，高分辨率建模长期受算力限制，传统方法以符号距离SDF函数等体积表示生成 1024³分辨率 3D 形状，计算与内存压力巨大，成本高昂。而今天给大家介绍的 Direct3D‑S2</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494284&amp;idx=4&amp;sn=53ec9ec787d76685e3052c4bb6927525&amp;chksm=fd3337c0cbf2e4c68cc74937047ec62e4ffdb5025286a3aa547dac39e9dd17c1a05f99a44541&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Jul 2025 03:32:00 +0000</pubDate>
    </item>
  </channel>
</rss>