<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[AIGC Studio 联合机械工业出版社给读者免费送新书啦，开启 AIGC 智能教学新时代！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elzodISUKsiaVtsAvhTQ7mRrRLGsBHPcPFYjEQGy2R1696AH45yrdicaI7TibaZZoxAiauDh2ic1IMLl1w/640?wxtype=jpeg&amp;wxfrom=0"/><p>亲爱的读者们，我们正身处人工智能（AI）技术飞速发展的浪潮中，AI正以前所未有的速度重塑教育领域。你是否渴望了解AI如何赋能教学，提升学习效率？是否想掌握最新的AI教育工具，成为未来教育的引领者？现在</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493006&amp;idx=1&amp;sn=ef99a6d477ab0bfb69aaa25eed245895&amp;chksm=fd6ff6adb9bc8e77c3664fa305ee40e9abcb379a943670e2ab16dfe33ed3c136e6427333ac34&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 25 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[视觉生成领域新突破！无需引入任何外部表征组件：SRA助力Diffusion Transformer实现自我表征指导。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcnWs2mR9uePicbSxmgsNGY5eoKECJw2VJZHk986ZktQp6KeSnEV1SrSa2JVMKibe6QbJ5dpasib3Kw/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇文章来自公众号粉丝投稿，对于Diffusion transformer在视觉生成领域获取高质量表征不容易的问题，文章提出了一种SRA(Self-Representation Alignment)方</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492810&amp;idx=1&amp;sn=f9e0c5c2aad85a2d7583a90415a29409&amp;chksm=fd0ef80dadbe86088af659924fa73e14ef4cd85a9a05517fdcb28cdaacaac9000390d1697871&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 24 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Apple提出UniGen！多模态理解生成统一xii新架构！CoT - V提升图像生成质量！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5KqpbKjwyf8GDnoGZ1ANRZVHSofem5JIanFIxSibozXUibNxHviaUIPE6FTh1nw9lCf16QMqWDaqf7cg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：UniGen: Enhanced Training&amp;Test-Time Strategies for Unified Multimodal Understanding and </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492810&amp;idx=2&amp;sn=78a042467cb774887c54251d98689a96&amp;chksm=fd946423c2521902cc57950d1287e8d034ef55b11a3e2f1e0f9d8b7eeee9b548cbf4f4a32cc3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 24 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI视频生成新突破！字节提出一致性视频生成方法Phantom：通过跨模态对齐生成主题一致的视频，超多应用场景。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enmjqTKh2qwkPiauc2Ejsn7FjUDMtLDDzOxeRDTsjoBO7nWymp4ibfUg4ngicJhNSbFdrgXOp81mHMKg/300?wxtype=jpeg&amp;wxfrom=0"/><p>Phantom 是一个统一的视频生成框架，适用于单主题和多主题参考，基于现有的文本转视频和图像转视频架构构建。它通过重新设计联合文本-图像注入模型，利用文本-图像-视频三元组数据实现跨模态对齐。此外，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492810&amp;idx=3&amp;sn=8631834744a0367b70cbbaa3318d8d31&amp;chksm=fdc8db091d82c60cec2d7eea45943088786edb2aae642d9bd11d9212aec08b217189201e53f0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 24 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里联合上海 AI Lab 提出DMM！多个模型压缩成一个通用 T2I 模型！实现可控任意风格生成!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enKvWzJ9QLeWgYQiaKmEWxL1P5PicFV1tgibVmCBLQ8OGUjLpAzElnOKcfkVy2nedvyOpMtQl3n9DKicw/640?wxtype=jpeg&amp;wxfrom=0"/><p>阿里联合上海 AI Lab 提出了一种基于分数蒸馏的模型合并范式DMM，将多个模型压缩为一个多功能的 T2I 模型。DMM 能够在风格提示的控制下生成各种专业风格的图像（写实风格、亚洲肖像、动漫风格等</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492720&amp;idx=1&amp;sn=226943ef8dcad184d80aea9af3e66c76&amp;chksm=fdc6db95f92d04890c4c8ff8a3de9561aae0b2f6790b9f3e9141f5b06f4a3780c04a675396a4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里&amp;北邮提出基于Wan2.1的音频驱动数字人FantasyTalking，只需输入肖像、语音和文字即可生成动画。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en6YOGtn3XXJMye1oxLXOtQU407yMKvXQv0r7RibwF9tY6RoaWiaXTsGib66ALF1tYibibzZZ51ibfTjibCA/300?wxtype=jpeg&amp;wxfrom=0"/><p>由高德地图、阿里巴巴、北邮联合提出首个基于Wan2.1的音频驱动数字人FantasyTalking，只需输入肖像图像、语音和文字，即可生成表情丰富、肢体动作自然且具有身份特征的动画肖像。此外，Fant</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492720&amp;idx=2&amp;sn=3db9d9c49853f654b8c2e932bdf6a00c&amp;chksm=fd9cd3c30ef9959d1019ded712b0e3c8c4bcd48e24ca905d1b669a64585ca97b10124e654992&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大联合上海AI Lab提出视觉统一Diffusion架构DICEPTION！各种视觉任务一网打尽！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcb6XeOfGM7ic3jww1VGas5hyQ5UbdLhbhjcqHwrckdlwdXIvppjK9PlGZVkxMpOMiaT6tDJ32KOqiaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>数源AI 最新论文解读系列论文名：DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks论文链接：https://arx</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492720&amp;idx=3&amp;sn=eabe660b6826efd4bcd5177e7649126e&amp;chksm=fd623da5c814d23f87bfcabdc32e88c6af9567b359cf6169f58d4aad0baf2e71deddd81cd47f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[利用多模态模型赋能，SONY团队完成音乐到音乐视频描述生成大突破！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5Jn7gNOibialUb7ePwaNgQPKeSIN3Kfa1hwX15JM3vgCh8jl1Fm3ZyyqibhJ0YwwiaTxARyLn4ucxtkUw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：Cross-Modal Learning for Music-to-Music-Video Description Generation论文链接：https://arxiv.o</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492720&amp;idx=4&amp;sn=fafdd17b3cd673e2d4df703c9a9fde7c&amp;chksm=fdfec1ddee646414065722bf9c79272528050c348aa0f35d63a395fe3063e446845947914ef1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[谷歌黑科技炸场！LightLab：只需一张图+AI，光影编辑像呼吸一样简单，废片秒变电影级大片！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek5oLyjfCjICEWyMhWNvFXD4gjUliaiaZ2wibPXVQsv1Vm81gqpJibC7scwg1DqQJbMMmvr7P8D5cPMCA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中以及和大家介绍过需要关于图像&amp;视频重打光的方法，在今天的推送文章中，已经帮大家重新整理好了，欢迎大家点击阅读~今天给大家介绍谷歌提出的一种基于扩散模型的方法LightLab，可以实现对单</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=1&amp;sn=99354cc4985dc55c109838046f75fbb4&amp;chksm=fdacbfb75e0042cba268425f308b31bcb71945fd3ae2c0f3f770d26fc237ebabe2f3c9f892fa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IC-Light升级，支持视频重打光! RelightVid可在多视频场景中重照明，支持文本提示、背景视频和HDR输入！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek6QSiaic7OicOck7L6SeBvmG8KxGGhaK7IiaIoGtBJsFyM7LffJExAYwxgr09hKHicONPnN40NOq3Cib7A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中已经和大家介绍过ControlNet作者关于图像重打光的工作IC-light，这篇论文也是获得了ICLR2025的满分评分，感兴趣的小伙伴可以点击下面链接阅读！ICLR 2025满分论文，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=2&amp;sn=2950a56f26e8587c95186e0c9117dcdb&amp;chksm=fdff6c2ef47e1f54bb031474e3e4498163df8529daca24886b2d05817c3423e03251dc4a09e8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[耶鲁大学联合Adobe提出SynthLight：智能重塑人像照明，打造完美光影！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elPyLquFq9rYTicjFkPwyh9fFVDfMwbeuJFlesWohTUXxZRSXxUpCJVwUUib0mdhjaia5sa6Ciaibic8AQg/300?wxtype=jpeg&amp;wxfrom=0"/><p>耶鲁大学和Adobe提出一种用于人像重新照明的扩散模型SynthLight，该方法将图像重新照明视为重新渲染问题，其中像素会根据环境照明条件的变化而变化。在真实肖像照片上可以产生逼真的照明效果，包括颈</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=3&amp;sn=5e06c57c62bdf8160727f2911a8a7b18&amp;chksm=fd9f9180528417d6eb5a90537c53aa755d2edbc52acb91625e764569e4287f78b99d00f03150&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR 2025满分论文，ControlNet作者新作IC-light，控制生成图像照明，代码模型已开源。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enrJ6ibDPoiaQXzhMdZU8spAicfF8vgRl2qenVGz9ZkJkJgBYXd26ys3WTPnNDJtK81bRSE1Xia412nbQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>罕见！ICLR 2025 惊现了一篇满分论文，4个审稿人同时打出了[10,10,10, 10]，这是什么炸裂的存在？！这就是ControlNet的作者张吕敏，继ControlNet 后提出的IC-li</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=4&amp;sn=9b617f922e3e7a1681a2343d5385cc9a&amp;chksm=fda03aa51aca322b363f9343404c40c107837a266926ac84f76fc3ec2849b48616e1e8bdbe2b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[​IC-light V2：基于FLUX训练，支持风格化图像，细节远高于SD1.5。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXSHQPic5rJ8IBKiaNosJKl4zjuJBWaX5OT2Lf7ZUfribJIKIVPQd63HVFcGOR4owDUYVuia3JgZoueg/300?wxtype=jpeg&amp;wxfrom=0"/><p>IC-light V2：支持处理风格化图像“IC-Light”全称是“Imposing Consistent Light”，IC-Light 是一个操纵图像照明的项目。目前已经发布了两种类型的模型，两</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=5&amp;sn=ef2c28606afe86bf50fd73c138202ff6&amp;chksm=fd68bd28182b21beb89ddf7f52ae25c260dc958743a1b826470b8b1ecb395c9954533849f572&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[哔哩哔哩再放大招！开源最强文本转语音模型Index-TTS，超真实语音克隆，可纠正发音、控制停顿。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekBSQolZZOuH0N3j03LsQJ4iaV41U2U7rc7sOzwoFzlxLhwic4zicGU7C9cnmslZt0afXNjyErakLib2Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>哔哩哔哩最新开源的 Index-TTS  是一个 GPT 风格的文本转语音 (TTS) 模型。它能够使用拼音纠正汉字发音，并通过标点符号控制任意位置的停顿。经过数万小时的数据训练，该方法达到了最佳性能</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492718&amp;idx=1&amp;sn=00c7ca18ac08958d965bbcd3710a12b9&amp;chksm=fde412718fec897012e790e372142c946f89f22af26088cea3ef285ffd0a9c2ed66ada2cfc30&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[哔哩哔哩开源目前最强大的动漫视频生成模型Index‑AniSora，给二次元世界的献礼！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enSiccUGiblNd9QEDRcSD96fBURrzeY4wTV8PKUrO1z3OVvvWGu3cqBYg8yJTxdEbtefDDE4WXaYsLA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的是哔哩哔哩献给二次元世界的礼物——Index‑AniSora，目前最强大的开源动漫视频生成模型。它支持一键生成多种动漫风格的视频镜头，包括番剧片段、国创动画、漫画改编、VTuber 内</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492718&amp;idx=2&amp;sn=dd2451579f3a4d06f3c2f267cdb7f0fe&amp;chksm=fd74914dbcbccfc25938c9500780811abf1fdc4419d32f571103ac8c7693367367a76ca73551&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[单图生成3D头像+AI编辑+多模态驱动？阿里LAM让虚拟人“活”了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en9libmJyfFzq4ma8I0IqAGY3dib7yN0HLOdysDOE9mgQUibQDzEyr5tB9daDg9fq9JmJqBeOgnB0zgQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>LAM 是一个能从一张图片中一次前向推理重建可动画3D高斯人头的模型，不依赖多视角训练或额外渲染网络，支持跨平台、低延迟、实时渲染，是虚拟人、AI聊天头像与AIGC人物生成的重大突破。特点总结如下：从</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492718&amp;idx=3&amp;sn=68581704414edfc6e54f258fe6b8d9bb&amp;chksm=fd1086ac22cf95442d58ca39ae94c81e106809c6206ad0a4d0b09ee1ab64bc336610a674686a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[3D人脸黑科技！Pixel3DMM：单张RGB图像秒变3D人脸，姿势表情精准还原，几何精度碾压竞品15%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXFXA8pZKAq59wibWEHiaviafiabtefYD9pHZ4MPj0OpAkqBJmnicoxT1Oib952Bqw8Vt7paicb51B2WQfw/300?wxtype=jpeg&amp;wxfrom=0"/><p>慕尼黑工业大学和伦敦大学学院提出了一款经过微调的 DINO ViT模型 Pixel3DMM，用于逐像素表面法线和 UV 坐标预测。从上到下，下图展示了 FFHQ 输入图像、估计的表面法线、根据预测的 </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492718&amp;idx=4&amp;sn=94f33d4bcd95ff84571c62fdc2a21909&amp;chksm=fdc8e99f181a2016d4e6daa373a48c9d088af9ac5f4c749e048444db6cc095435a4ff5e3e2b0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[给二次元世界的献礼！哔哩哔哩开源目前最强大的动漫视频生成模型Index‑AniSora！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enSiccUGiblNd9QEDRcSD96fBURrzeY4wTV8PKUrO1z3OVvvWGu3cqBYg8yJTxdEbtefDDE4WXaYsLA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的是哔哩哔哩献给二次元世界的礼物——Index‑AniSora，目前最强大的开源动漫视频生成模型。它支持一键生成多种动漫风格的视频镜头，包括番剧片段、国创动画、漫画改编、VTuber 内</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492684&amp;idx=1&amp;sn=11915e42a737f6e39e8f2c198f1e09c9&amp;chksm=fd7a0ff4ea5d1b64ed619b8a816a6d4e5c270b3c63208be5b3f730ae1a5dae347b3a74336407&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 20 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[大模型再现黑马！英伟达开源Llama-Nemotron系列模型，效果优于DeepSeek-R1。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXFXA8pZKAq59wibWEHiaviafoC1ibJ7eE1fvbrtrICXG1kaXfiaqibBmibzznCUHyiaB4NGTibwK6pmBM0hA/300?wxtype=jpeg&amp;wxfrom=0"/><p>近日，英伟达推出了 Llama-Nemotron 系列模型（基于 Meta AI 的 Llama 模型构建）—— 一个面向高效推理的大模型开放家族，具备卓越的推理能力、推理效率，并采用对企业友好的开放</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492684&amp;idx=2&amp;sn=53439f12162dc38eaa0a30abfc172de5&amp;chksm=fd60209f088ba27da940c7793d562cdbb5d59ff8b667d3fffa235517abd3b6af4986c07650ae&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 20 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[3D 生成新 SOTA！SECERN AI 提出 方法 SVAD，单张图像合成超逼真3D Avatar！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elmzbxIf6OS3v7M1woTicaJczQ6xAAgVU8NYrMphwhLiaiajhcsCMja0TDYcr6RulFp9C6Yt1mtcbiamA/300?wxtype=jpeg&amp;wxfrom=0"/><p>SECERN AI提出的3D生成方法SVAD通过视频扩散生成合成训练数据，利用身份保留和图像恢复模块对其进行增强，并利用这些经过优化的数据来训练3DGS虚拟形象。SVAD在新的姿态和视角下保持身份一致</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492684&amp;idx=3&amp;sn=138e8a677e24ed57e53a688c5d73ce14&amp;chksm=fd5949abc21cb8519a499fe29b8595a728100de796632607638319a4878f7d07362b3119fb17&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 20 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[炸裂！ComfyUI 原生支持 HiDream-I1，全新文本转图神器来了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eloBQe14a8ohz069lCGESt2ibyrQzlh4BO0Xa83u0NI5WzuIBs5KCqMafPkjLwMiapJ0TVwXCaj6ibIQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>ComfyUI 原生支持 HiDream-I1，全新文本转图神器来了！大家好，这不是演习！ComfyUI 终于官宣——原生支持 HiDream-I1 模型啦！对于熟悉图像生成的小伙伴来说，这可是一件值</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492684&amp;idx=4&amp;sn=af6ae14a7dc045db56ab03f34130ec73&amp;chksm=fd7c7c55b693485bf5cee443a91e8c84077dfe8006c93711954c4076f1dc09020c5aff106ce7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 20 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[谢赛宁团队提出BLIP3-o：融合自回归与扩散模型的统一多模态架构，开创CLIP特征驱动的图像理解与生成新范式!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek5oLyjfCjICEWyMhWNvFXDN37WVtXa4JeBibibTSdNGmBP0wSFhuUAJkiaz9qNwiccNW4SuNJ7FvduuQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>BLIP3-o 是一个统一的多模态模型，它将自回归模型的推理和指令遵循优势与扩散模型的生成能力相结合。与之前扩散 VAE 特征或原始像素的研究不同，BLIP3-o 扩散了语义丰富的CLIP 图像特征，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492683&amp;idx=1&amp;sn=c6f9575a6347b0fd4f905f7ec64b54a8&amp;chksm=fd600ee6b8313e076506c578485f83d8dbd2740bd8b48a901f5bbd2e1cdad50b87393ed53add&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 19 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节提出从单一主题发展到多主题定制的通用框架UNO，通过情境生成释放更多可控性。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elo3s89icGNibsPQVXGhctg9WDrsYXyWyFSyqXzUDm6eOsD3G2Z7XbSMUPZrQw19LsCTpuzPx9KiaCWg/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节跳动的智能创作团队提出了一个从单一主题发展到多主题定制的通用框架UNO，从少到多的泛化：通过情境生成释放更多可控性。能够将不同的任务统一在一个模型下。在单主题和多主题驱动的生成中都能实现高度一致性</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492683&amp;idx=2&amp;sn=81496eb2cd96b6ff6e0fa8df5de1039d&amp;chksm=fdc359d002107ded0b9eb0d26dafd4996bb6ab8b6de68d9e2245f5f1e088aad1d08f3d00bc2a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 19 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[太强了！浙大联合上海AI Lab提出视觉统一Diffusion架构DICEPTION！各种视觉任务一网打尽！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcb6XeOfGM7ic3jww1VGas5hyQ5UbdLhbhjcqHwrckdlwdXIvppjK9PlGZVkxMpOMiaT6tDJ32KOqiaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>数源AI 最新论文解读系列论文名：DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks论文链接：https://arx</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492683&amp;idx=3&amp;sn=fe89aee75d03d8a8c22dd34f825f3347&amp;chksm=fd6a9663871f0b1d4837d084092f26a9cefa277e1abfe0013b771ff7f1d53142fe7659a98c06&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 19 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ComfyUI插件安装失败率90%？教你4种方法0踩坑]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/ACyQFjNqyE4krxeSSRPxaSdK57W0WTcKsAosmf2OLF7QQMa3t5LdSSntjJPTIZQgLlJTibnRCJGn820Ex6Odf4g/300?wxtype=jpeg&amp;wxfrom=0"/><p>点击蓝字关注我吧！ComfyUI 作为一款高度模块化、节点式的 AI 图像生成工具，近年来在国内外社区圈粉无数。但要说它最常见的“劝退点”，那一定非插件安装莫属。很多新手兴冲冲地下载完工具，还没来得及</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492683&amp;idx=4&amp;sn=989d17002404a904cfe61c5f8937f2eb&amp;chksm=fdb16e7819c41c7376d16cd0c578ed36308137b733fae9c185c6fded42c06266a538d74f218b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 19 May 2025 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>