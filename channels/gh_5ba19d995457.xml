<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[超越SOTA！浙大&amp;斯坦福提出 DiffLocks，单图头发 3D 重建精度提升30%，首次支持非洲式卷发生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48OPjpkTnf2mQjjRuiawRZ5BrVMBtgVJ4QGZRnabuCSKficVh97iaqr4QzQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>由浙江大学、斯坦福大学等联合提出的DiffLocks，给定一张 RGB 图像，DiffLocks 使用扩散模型生成精确的 3D 发束。该模型基于一个包含 RGB 图像和相应 3D 发束的新型合成头发数</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493763&amp;idx=1&amp;sn=0bf1f3f786dc31b81e0847855e288227&amp;chksm=fd51247ca97c2bbfe87bb4030530f09d84032465bba763ccf1aace713c6d488d7bbb65ed9e1b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 25 Jun 2025 16:02:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[3D人脸黑科技！Pixel3DMM：单张RGB图像秒变3D人脸，姿势表情精准还原，几何精度碾压竞品15%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXFXA8pZKAq59wibWEHiaviafiabtefYD9pHZ4MPj0OpAkqBJmnicoxT1Oib952Bqw8Vt7paicb51B2WQfw/300?wxtype=jpeg&amp;wxfrom=0"/><p>慕尼黑工业大学和伦敦大学学院提出了一款经过微调的 DINO ViT模型 Pixel3DMM，用于逐像素表面法线和 UV 坐标预测。从上到下，下图展示了 FFHQ 输入图像、估计的表面法线、根据预测的 </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493763&amp;idx=2&amp;sn=374f9b5b2b28daf20d4c0e5f8da51fcd&amp;chksm=fd6a5064474934cbf9708d754546571b75685a26cf7a2e09702d3b403017228c644eb9066a8e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 25 Jun 2025 16:02:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[谢赛宁团队提出BLIP3-o：融合自回归与扩散模型的统一多模态架构，开创CLIP特征驱动的图像理解与生成新范式!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek5oLyjfCjICEWyMhWNvFXDN37WVtXa4JeBibibTSdNGmBP0wSFhuUAJkiaz9qNwiccNW4SuNJ7FvduuQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>BLIP3-o 是一个统一的多模态模型，它将自回归模型的推理和指令遵循优势与扩散模型的生成能力相结合。与之前扩散 VAE 特征或原始像素的研究不同，BLIP3-o 扩散了语义丰富的CLIP 图像特征，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493763&amp;idx=3&amp;sn=d062a5824d4a2c5e5ac49b61d8aae7e2&amp;chksm=fd8bae8b28d224cb36bc812e6595032b1895d5b1ede8ed58f97e8f8c76847c7c3eac75ea017e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 25 Jun 2025 16:02:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大联合上海AI Lab提出视觉统一Diffusion架构DICEPTION！各种视觉任务一网打尽！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcb6XeOfGM7ic3jww1VGas5hyQ5UbdLhbhjcqHwrckdlwdXIvppjK9PlGZVkxMpOMiaT6tDJ32KOqiaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>数源AI 最新论文解读系列论文名：DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks论文链接：https://arx</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493763&amp;idx=4&amp;sn=c5ff46e216b20fbf10814dfed6641a19&amp;chksm=fd16369ffb227699b070236f25f240b1643557b49629436c74f5a5c2b49a123f00cd48122c56&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 25 Jun 2025 16:02:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[多模态任务大一统！蚂蚁推出Ming-Omni：图像、文本、语音三模态无缝融合，一网打尽复杂任务！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48J6PaQ6vvBzg7xSUic6gUlNictqMXib837dJ3ia9U0lib3hc3BMuQHGGd8fA/640?wxtype=jpeg&amp;wxfrom=0"/><p>Ming-lite-omni 是 Ming-omni 的轻量版本，源自 Ling-lite，具有28亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493761&amp;idx=1&amp;sn=327e2892d147905881517d651605f33a&amp;chksm=fd58910f7804665cc7e1427c6a98181ed23263280d170e407a53cb7528e63a7fa7dcc373c5da&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 24 Jun 2025 15:33:18 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[StepFun提出Step-Video-T2V！300亿参数视频生成大模型！可生成204帧视频！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcJicm2I75ZP1rkl1ZMqicoKfreYnRFLqFBbibqBpPJl9LzNL6OUXy1tmllZuicN8KGIYIbPRjfSZnnOw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名：Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model论</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493761&amp;idx=2&amp;sn=172ad152f29a13d0cc614df4f6ff5282&amp;chksm=fdea51744a0d30335d6423e7e277c9f2d770ba23696a24d760204ee894d0f20d827250e308e8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 24 Jun 2025 15:33:18 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大联合上海AI Lab提出视觉统一Diffusion架构DICEPTION！各种视觉任务一网打尽！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcb6XeOfGM7ic3jww1VGas5hyQ5UbdLhbhjcqHwrckdlwdXIvppjK9PlGZVkxMpOMiaT6tDJ32KOqiaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>数源AI 最新论文解读系列论文名：DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks论文链接：https://arx</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493761&amp;idx=3&amp;sn=32a7e26cdaa841f1b6dc17ae83dc7e21&amp;chksm=fd00b75aa70a08f01d71e61c9ff79aa966358bb841b5e52823725da164c0b068345a3f28e2d8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 24 Jun 2025 15:33:18 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[LBM：用于图像到图像直接快速转换，支持可控照明、图像恢复、物体移除等功能！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eniaAibjBDYoftj8VvjntaLlazzrjyAuCaxtUgTmwTpbpXdlUbj1mP1pmA9QicicVlSzvQAT83J2fYzAA/300?wxtype=jpeg&amp;wxfrom=0"/><p>LBM是一种新型、多功能且可扩展的方法，它依赖于潜在空间中的桥匹配来实现快速的图像到图像转换。该方法仅使用一个推理步骤即可在各种图像到图像任务中达到最佳效果。除了效率之外，该方法在不同图像转换任务（例</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493761&amp;idx=4&amp;sn=c459c00d57cc052c08c86f1d594c6ef1&amp;chksm=fdf0720a8936003c010c7d03db8c0b59013587bda38aa67f0d23de32e341b8bb4565d0124e2e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 24 Jun 2025 15:33:18 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[TMM 2025 | 超越SOTA！AdaMesh用10秒视频生成个性化语音动画，表情生动性提升40%。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48sD3er0mq7FL0guJnKjkMSPVhPLjFIJ4elWF7POpyFVoOcRqfjLRoPw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在虚拟角色与数字人技术中，如何生成兼具真实感与个性化的语音驱动面部动画仍是关键挑战。现有方法往往依赖海量数据或通用模型，难以捕捉用户独特的说话风格（如微表情、头部动态）。为此，由清华大学深圳国际研究生</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493722&amp;idx=1&amp;sn=50c3f9227adeff4dfca60ed06c734781&amp;chksm=fd944c47d51fa331253f7dc46543c4da2886b67017deee3483730b56c6d8858a3444cdd7d45b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 23 Jun 2025 16:04:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里EMO2重磅升级！手部动作生成+超逼真表情，音频驱动人像视频生成再进化！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en9libmJyfFzq4ma8I0IqAGYiaHtTElCkzOGD9sY0N1Qp8FDJqnDN5BkTWSW0TSu1sYeAgQzRiaicMcRw/300?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中已经和大家介绍过阿里提出的音频驱动的人像视频生成方法EMO，感兴趣的小伙伴可以点击下面链接阅读~阿里最新EMO：只需要提供一张照片和一段音频，即可生成会说话唱歌的AI视频此外公众号的底部</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493722&amp;idx=2&amp;sn=1b35700fce90619c337670929cd351e5&amp;chksm=fd10aa0f0914f8214324b9116402e3e998924161ef50973c232c364f4e14e04925d19dc67d93&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 23 Jun 2025 16:04:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[PlayerOne横空出世：港大×达摩院重塑虚拟世界交互范式，动作捕捉驱动AAA级场景自由探索。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enfUCFX9WW23BajIFJBpRq3xvD6IHNj8gocPOicHAPyQsE13dEpzsl31yyrObIKhz86FlHOmK6LtVg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天介绍的文章来自公众号读者投稿，由香港大学与阿里达摩院联合研发的PlayerOne模型正式亮相。该技术突破传统虚拟场景构建范式，通过单张图像输入即可生成高保真动态虚拟世界，并支持用户以实时动作捕捉实</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493722&amp;idx=3&amp;sn=91262785aa5549cd654c402eed5ff9a9&amp;chksm=fd5e5b3d05d289f8aa1e0ddabb096c327fee2eaf1304d876d2cf28a2e1e1eca4244ec06289a1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 23 Jun 2025 16:04:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[3D 生成新 SOTA！SECERN AI 提出 方法 SVAD，单张图像合成超逼真3D Avatar！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elmzbxIf6OS3v7M1woTicaJczQ6xAAgVU8NYrMphwhLiaiajhcsCMja0TDYcr6RulFp9C6Yt1mtcbiamA/300?wxtype=jpeg&amp;wxfrom=0"/><p>SECERN AI提出的3D生成方法SVAD通过视频扩散生成合成训练数据，利用身份保留和图像恢复模块对其进行增强，并利用这些经过优化的数据来训练3DGS虚拟形象。SVAD在新的姿态和视角下保持身份一致</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493722&amp;idx=4&amp;sn=6a94dc88ad1a0ca7ae70e6360408d4c5&amp;chksm=fd1e9c79bcf1556e2ed52396d0e7c17952ae6698b8c4250808d28659c4550353b53c997c70c2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 23 Jun 2025 16:04:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[电商广告新利器！字节提出 DreamActor-H1，让产品与模特“一键生成”高保真交互视频。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48RPbNymvllhYib3H7VZyk223eWwAma3ovH2vTKZCY7Hg7zPurzD9kpqQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>DreamActor-H1 是一个基于扩散变换器 (DiT) 的创新框架，能够根据配对的人与产品图像生成高质量的人与产品演示视频。DreamActor-H1 基于大规模混合数据集进行训练，并结合多类别</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493721&amp;idx=1&amp;sn=fc232eceb4cf740b972e6ace540bf0c2&amp;chksm=fde8e6f3d61c5b1cc9f3b8e6ebf746e43b9fda435c77337f89c5dcc3ba4e8f790da42988fc3e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 22 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[如何使用DeepSeek进行科研图表绘制？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/vI9nYe94fsFkhhJibgYhskdb4vjUEaTlFuY2pp216d97E3UsjQZuBJkB8oBHK2OrmMP1t3zaSDLBxT6GhVGv5rQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>有时候我们写论文或者看 blog，看到别人画的很好看的结构图，觉得自己肯定画不了这么好。但是现在可以让大模型来帮我们结构图。一共需要用到两个工具：大模型、Draw.io。下面的示例会使用 Claude</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493721&amp;idx=2&amp;sn=1b6dc32b2de2032c5cb92223cb590e3d&amp;chksm=fd8dfab3a425aafcbe539086f2a332d1de3600e88d319c089caf8e839ef3d4ed51b7d35402e5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 22 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[港大&amp;Adobe联合提出图像生成模型PixelFlow，可直接在原始像素空间中运行，无需VAE即可进行端到端训练。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em57kq23EbSGQ52kUcSia6n8oTIJOicficBicZpibaJQgm7tEpQJ6psVkrLse6pjDUwqiaktvnGSEiaL6xPg/300?wxtype=jpeg&amp;wxfrom=0"/><p>香港大学和Adobe联合提出了一种直接在原始像素空间中运行的图像生成模型PixelFlow，这种方法简化了图像生成过程，无需预先训练的变分自编码器 (VAE)，并使整个模型能够端到端训练。通过高效的级</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493721&amp;idx=3&amp;sn=2a38c255968f417d1569c66c9e709743&amp;chksm=fda677b972ed81ac21350aab9ae8034898721cca28db4fc2718fd7f96e3bddb592fbf03d4f87&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 22 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR2025 | 同济提出无需训练的肖像动画框架FaceShot，让表情包、动漫人物、玩具等“开口说话”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emXysHeAOso1q4PjdgGCNECN5vlsQZr9AOKKvriaYqbhSHH5y8IBJg25HQaMqclHrVZ7Dp9ObVuiaww/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天和大家分享同济大学的最新研究FaceShot: 一举打破肖像动画模型“驱动真人”的局限，FaceShot 的动画效果可应用于各个领域的角色，包括 3D 动漫、表情符号、2D 动漫、玩具、动物等等。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493721&amp;idx=4&amp;sn=8a8aa68fc4b333e67931da9c5c047fb6&amp;chksm=fd24a7bad3dcddc39de7bda470320384ea8de354b198803efac15ee21230d477aff7b07bfa32&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 22 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[碾压开源与商业模型！腾讯开源一致性视频生成框架HunyuanCustom：可同时实现音频同步与视频编辑！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emrh9ic75wTHs9dqezVrp8ttuUVmic8QW7affiaZjHpsxtibwNLZzmTDHNHfticnBcXHEWnBic45icRHnibicQ/640?wxtype=jpeg&amp;wxfrom=0"/><p> 腾讯提出了一个多模态定制视频生成框架HunyuanCustom，该框架强调主题一致性，同时支持图像、音频、视频和文本条件。基于HunyuanVideo，该模型首先通过引入基于LLaVA的文本图像融合</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493506&amp;idx=1&amp;sn=a71a9afcf4d59706d82def94d30062d4&amp;chksm=fd0cf713ea60a30628071066acfec1a2fda65841130aa16cabffff9d62b83e725fe539ec28d5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 21 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[首个开源免费的商用级别克隆数字人模型，1080显卡可用，数字人不再有门槛。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/l2VB7h1M5NZ6PJdJzjbpnsg9jnqH5fs4qjKNm58tibo7Gf2j3EvibGIAmfBuElquWUKVKjavt3IGQo0JLLcHDsDQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>数字人的市场是极大的。从23年开始爆火，大家问我比较多的几个问题，哪个好用，哪个便宜，能不能自己做一个。能，确实是能自己做。但是造价太高太高，这样一个商用音频驱动口型的模型，如果从一开始就在做的话，可</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493506&amp;idx=2&amp;sn=107557bf2cc236890406c944fd06b89f&amp;chksm=fd7e048382ebd94bf8c5f8b30e274181d087ecb8b903a5b3953b0a4d4dbd1aefed383a59c94f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 21 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[StochSync：可在任意空间中生成高质量360°全景图和3D网格纹理]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enXFFuKUPQcYLlG3aibBhJtN7xgJCpXZE7HoaWiahrDNLktV0doUSl1wRalx4MZej02YkgNsTVfSbpg/300?wxtype=jpeg&amp;wxfrom=0"/><p>StochSync方法可以用于在任意空间中生成图像，尤其是360°全景图和3D网格纹理。该方法利用了预训练的图像扩散模型，以实现zero-shot生成，消除了对新数据收集和单独训练生成模型的需求。St</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493506&amp;idx=3&amp;sn=f116028a702569a36487fc84230f9c25&amp;chksm=fdd37dea8268c0c36f3721d362ba5d5f368f5e47faa089003c658e5060abb72641266cfd2dc9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 21 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[一文了解：大模型 Agent 开发框架有哪些？它们的区别是什么？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/CibEZ9gjHpIr8DNP5MR3eP0zlA9JsT0hBHpz7mIT9yRpuDv80p9ANexpSib1fao2maWhK8nPVqlRMv4h1P6M8ia8Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>——大模型时代，如何选择适合的 Agent 开发工具？引言随着大模型技术的爆发，AI Agent（智能代理）逐渐成为落地应用的核心载体。它不仅能理解语言，还能自主规划、调用工具、执行任务，真正让大模型</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493506&amp;idx=4&amp;sn=81aed2f49d75f3509726bb4b6fdae9fc&amp;chksm=fdc9bc81033258221e7ee597e472b01da5a22283494e3a0bb23d001d8e24df82cd4e0bc86e2b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 21 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[腾讯开源 HunyuanVideo-Avatar，一张图+一段音频实现图中人物、动物甚至虚拟角色开口说话！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em4gibISNFQR95biapR4RJ7Lq5BIttmnJoy6onMGT6hEJiblmfujJkZFpZjpO6usAYRtw7aj1tZbJZYw/640?wxtype=jpeg&amp;wxfrom=0"/><p>腾讯混元团队提出的 HunyuanVideo-Avatar 是一个基于多模态扩散变换器（MM-DiT）的模型，能够生成动态、情绪可控和多角色对话视频。支持仅 10GB VRAM 的单 GPU运行，支持</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493505&amp;idx=1&amp;sn=192f1cb676680ac1a83d15098a6df491&amp;chksm=fd19a8a2b3c783792481eca39abcdb73e4c9b8f44776312af769afac0480e8a12e9ef9950796&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 20 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[6秒音频即可克隆AI语音！FLOAT数字人生成语音/口型/表情，情感同步超惊艳，文中附工作流。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elmzbxIf6OS3v7M1woTicaJcmBGicWjwiauMpFknBOofINibzHjBSIibjwDHKYvhnzulS1E2KIPicobCywA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的 FLOAT 是一种基于流匹配的音频驱动的说话肖像视频生成方法，可以增强语音驱动的情感运动。该方法唇形同步质量高，生成速度还很快。6秒音频完美生成语音/口型/表情。情绪转移由于 FLO</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493505&amp;idx=2&amp;sn=dd438da6df94216717c29c1e27d81417&amp;chksm=fd635dffacf389be8dfa57e48a72233ac69ee978378b2879554c240d89cbdfd170ea450c1dec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 20 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[开源数字人克隆神器HeyGem：1秒视频生成4K超高清AI形象，用AI重塑数字人创作生态！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elzodISUKsiaVtsAvhTQ7mRrgxstWFTNfP8vOAkR5RI8GOy83ObgNDrZJL0p3TTnAIBViacS7PlySow/300?wxtype=jpeg&amp;wxfrom=0"/><p>在虚拟形象与数字内容需求激增的当下，传统3D数字人制作的高昂成本（动辄数十万美元）与复杂流程，让许多行业望而却步。而今天，一款由Duix.com团队打造的开源AI项目HeyGem，正以颠覆性技术打破这</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493505&amp;idx=3&amp;sn=cd549827e46ad001a6cf9b863bd62dfd&amp;chksm=fd2421513f0013e32a1c21c12ccb1130de7072158c9577117a863ecb11c542b9192f6304bd75&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 20 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[加利福尼亚大学提出TULIP！视觉-语言模型的新王者！AI性能全面碾压CLIP！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5KsLicuyg3oA0dGOnwBictNTE782KtlqwlaVEmKrVyKAO0YzauujiaGWFqaYjHzZqKD5rLk8dQLKZtEg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：TULIP: Towards Unified Language-Image Pretraining论文链接：https://arxiv.org/pdf/2503.15485开源</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493505&amp;idx=4&amp;sn=5fcb5d89678ab6a2588f377a71b17a86&amp;chksm=fd632de2cd7242c0e4de70e0848d100a02cd66f6f44634129fd70ae974e23a25d68b1150a75c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 20 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[PlayerOne横空出世：港大×达摩院重塑虚拟世界交互范式，动作捕捉驱动AAA级场景自由探索。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enfUCFX9WW23BajIFJBpRq3xvD6IHNj8gocPOicHAPyQsE13dEpzsl31yyrObIKhz86FlHOmK6LtVg/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天介绍的文章来自公众号读者投稿，由香港大学与阿里达摩院联合研发的PlayerOne模型正式亮相。该技术突破传统虚拟场景构建范式，通过单张图像输入即可生成高保真动态虚拟世界，并支持用户以实时动作捕捉实</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493504&amp;idx=1&amp;sn=2a1317de9928f83ef6684a66995d2d8d&amp;chksm=fd2923ee7c7c853d47c4e8ac9a8f96fd2cf54bbd8a12efcbebdd606c81b3cfa579ca4ab4b09d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 19 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦联合百度发布Hallo4：让AI肖像“活”起来！新型扩散框架实现高保真音频驱动动画生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em4gibISNFQR95biapR4RJ7Lq56s1kIaYWsxKESfb9riaHUQVlW3JfPib9AP6mL8Hk0Ec5R0f43HYJ8aw/300?wxtype=jpeg&amp;wxfrom=0"/><p>复旦联合百度发布扩散框架Hallo4，实现了准确的唇音同步、自然的面部表情，并能够稳健地处理各种角色身份和环境场景中快速的语音节奏和突然的上身运动。相关链接论文：https://arxiv.org/p</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493504&amp;idx=2&amp;sn=9be5793da41f19e0b244c36280adeee7&amp;chksm=fd31034ec8502b4043e11ed3f27a27dcaf1ca16c58bd92e848e733e65a6bfbeff53baa8c7dd2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 19 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 字节提出个性化多人图像生成新方法ID-Patch，可生成多人合影、姿势可控。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emCuicERoV3guOMh64VYNrcA6VO1uBfS3aIicTCtKS3eFEBxCVDPwXCyj0Fye0L4toEplkN73YiaibibFw/300?wxtype=jpeg&amp;wxfrom=0"/><p>相信扩散模型（DMs）大家一定都不陌生了，目前已经成为文本生成图像的核心方法，凭借强大的图像生成能力，正重塑艺术创作、广告设计、社交媒体内容生产格局。现在，用一段文字生成个性化头像都不算啥新鲜事儿了。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493504&amp;idx=3&amp;sn=54634b736df796435a94901240aa3a11&amp;chksm=fd27535cb515d93d0d3fefd2e6e600ec0295643e3195f67aa42370e6e92e0cf62aa358775fd9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 19 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[腾讯混元&amp;InstantX开源InstantCharacter，跨角色外观、姿势和风格个性化生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eloBQe14a8ohz069lCGESt2mVMulTo5LC5G2oFcJtOgsuJWSCokK4anUcgT9xP5mIuHTqbM9wOvIw/300?wxtype=jpeg&amp;wxfrom=0"/><p>腾讯混元联合InstantX团队提出全新角色定制生图框架 InstantCharacter，与当前的SoTA方法GPT4o取得了相当的结果，然而，GPT4o并未开源。相比之下，InstantChara</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493504&amp;idx=4&amp;sn=e1afe7cc00faf1a30ca9437dc0646003&amp;chksm=fdfbe9ba9a684cfbd005c4fff4124310510f18c22527121fa7ebdced9fb656e2d8fa5158efe2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 19 Jun 2025 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>