<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[南洋理工提出 Ultra3D：6.7 倍加速突破效率瓶颈，1024 分辨率下登顶 3D 生成性能巅峰。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elVVXYVXeZKg9ibO6JibLeUheArjN7Xm2cOz07j7Uy5AIOxXedaPSFmFxlbPp39YvTiao133ztkagpEQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在 3D 内容生成领域，稀疏体素表示虽为高分辨率建模带来曙光，但现有框架却因两阶段扩散流程中注意力机制的二次复杂度，陷入计算效率低下的困境。不过，南洋理工大学的研究团队带来了突破性成果！他们提出的 U</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494284&amp;idx=1&amp;sn=cea7e65dd5ccf09f1386a29a33ca20a6&amp;chksm=fd6bc9e87837ba2224a45389d9a7239de6e52dfdb66d0a7b668eb6565ca4eda39e3c993d4c13&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Jul 2025 03:32:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[从文本到3D的“零训练”革命！英伟达&amp;康奈尔大学提出 ArtiScene：通过2D中介实现高保真3D场景合成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48HPJzGndEliaF6RR9oS2mcY6tk1tO13iaxO3UHzBrtzwlN2jFlpv4651g/300?wxtype=jpeg&amp;wxfrom=0"/><p>由英伟达和康奈尔大学提出的 ArtiScene 是一种无需训练、语言驱动的 3D 场景生成流程，它可以根据文本提示，设计出丰富多样、美观且易于编辑的场景，涵盖各种类别和风格。下图中展示了四种结果，并附</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494284&amp;idx=2&amp;sn=ff46e830afa0c44d78be7bd299fa033c&amp;chksm=fd06e608ed0cd51765151c788d5c3ed7732e30eedfcc856ac542417a3aa26193b7314662cde5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Jul 2025 03:32:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工力作Sparc3D：开启三维重建可微分优化与高效生成新纪元。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enhd8Da8niase1WltgKePj289UYQ2FkGK7uxrgpyoOIA6cIHk7jU4q6hvNUWTsCz3qI24ic8ibqQ8GjQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>由南洋理工大学推出 Sparc3D 是一个统一的框架，它将稀疏可变形行进立方体表示Sparcubes与新型编码器Sparconv-VAE相结合。Sparcubes 通过将有符号距离和变形场散射到稀疏立</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494284&amp;idx=3&amp;sn=f6b862e8a50403d358e378ff24a37c28&amp;chksm=fdd8635c097f9e6b788c992c4c125ae0d8d257e3f4c9d86e8d0b38cc6dcd48c6e63b5f016074&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Jul 2025 03:32:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[突破高分辨率3D建模算力瓶颈！南大&amp;复旦提出 Direct3D‑S2：8卡即可训练，革新 1024³ 分辨率3D生成格局！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elCicOusPT4UMbWRggQc9icnnibKbtwkfQH4wt4miaD9ltwEVcbK2fR9bxibXxuHKTtr0qEAr49WVVsnag/300?wxtype=jpeg&amp;wxfrom=0"/><p>介绍 在 3D 生成领域，高分辨率建模长期受算力限制，传统方法以符号距离SDF函数等体积表示生成 1024³分辨率 3D 形状，计算与内存压力巨大，成本高昂。而今天给大家介绍的 Direct3D‑S2</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494284&amp;idx=4&amp;sn=53ec9ec787d76685e3052c4bb6927525&amp;chksm=fd3337c0cbf2e4c68cc74937047ec62e4ffdb5025286a3aa547dac39e9dd17c1a05f99a44541&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Jul 2025 03:32:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里最强代码模型Qwen3-Coder发布：多尺寸选择，开启编码新体验！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em9xtmb1LYQQ3l0fNDDWpS4R9kV73WWUJibv1BBPEzpA4GiczgHXPoApCNSWLpvhlypT6L0bvGhTEUA/640?wxtype=jpeg&amp;wxfrom=0"/><p>阿里推出了迄今为止最具代理性的代码模型Qwen3-Coder，Qwen3 -Coder有多种尺寸可供选择，首先推出的是最强大的版本：Qwen3-Coder-480B-A35B-Instruct。它具有</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494276&amp;idx=1&amp;sn=3f35ab38b7dfcaecfb3d4cf5f71e1a6a&amp;chksm=fdc98f94baf937334eeeae91b2e2862025cf8ee8faa4e3d1ae24936745d219b7f454aa812e2e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 24 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[蚂蚁推出Ming-Omni：图像、文本、语音三模态无缝融合，一网打尽复杂任务！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48J6PaQ6vvBzg7xSUic6gUlNictqMXib837dJ3ia9U0lib3hc3BMuQHGGd8fA/300?wxtype=jpeg&amp;wxfrom=0"/><p>Ming-lite-omni 是 Ming-omni 的轻量版本，源自 Ling-lite，具有28亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494276&amp;idx=2&amp;sn=13b58957989b6c7996897f74ea0b4537&amp;chksm=fdee885b4470a35a7a57728982c0a99c972f5688b045203496c0a60c36e3a9d4c51697c250c1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 24 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[FlashFace: 具有高保真身份保存的人像个性化方法，效果超越InstantID，人脸定制化更逼真了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekGB20aoopfDW9Ia72SmdXIpHxTUXCIsjaFY2ob7ymtpQcesNrLicxwJME93bc3QPGgQ9eWOpYd9VA/300?wxtype=jpeg&amp;wxfrom=0"/><p>FlashFace技术是由香港大学、阿里巴巴集团、蚂蚁集团共同研发的一项实用工具，用户可以通过提供一张或几张参考面部图像和文本提示，就可以轻松地即时个性化自己的相片。与现有的人像定制方法相比，Flas</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494276&amp;idx=3&amp;sn=5e655dd02f2c65ebc304037a9602a42a&amp;chksm=fddbae1be1204cdd3d0ce922bab8d040d579cbec5d1469d7c896fea0ac1f7fc337c4ae85c177&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 24 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[PlayerOne横空出世：港大×达摩院重塑虚拟世界交互范式，动作捕捉驱动AAA级场景自由探索。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enfUCFX9WW23BajIFJBpRq3xvD6IHNj8gocPOicHAPyQsE13dEpzsl31yyrObIKhz86FlHOmK6LtVg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天介绍的文章来自公众号读者投稿，由香港大学与阿里达摩院联合研发的PlayerOne模型正式亮相。该技术突破传统虚拟场景构建范式，通过单张图像输入即可生成高保真动态虚拟世界，并支持用户以实时动作捕捉实</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494276&amp;idx=4&amp;sn=2537b65d5d366cb381b9607b06e537de&amp;chksm=fd3edcf7d003087fe7490544d733902b449a303ac723fbd753a562b46cd331564a295d4635d0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 24 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 中海洋提出虚拟试穿通用框架OmniVTON：首个无需训练，首创多人试穿，解锁虚拟试穿新玩法！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enDjkDdGWUtr9tUMAgfKEsSXEyFsBSBt07vqsaSfHCBGu6HTtsWZZULSH22C3xOTlJseL1bvGtnmQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍了很多关于虚拟试穿的文章，本公众号也总结了虚拟试衣专题在公众号菜单栏，感兴趣的小伙伴可以在公众号内搜索“虚拟试衣”阅读～在基于图像的虚拟试穿（VTON）领域，In-Shop监督方法虽</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494244&amp;idx=1&amp;sn=2fc765ccb9a7b8019ebb0ec2dc018654&amp;chksm=fdcb271a70b4c13e10a9ab7851d3abd7ad0340fb29e9929b8a7b0dfb1e63b75c281d8cc369d2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 23 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR2025 | 同济提出无需训练的肖像动画框架FaceShot，让表情包、动漫人物、玩具等“开口说话”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emXysHeAOso1q4PjdgGCNECN5vlsQZr9AOKKvriaYqbhSHH5y8IBJg25HQaMqclHrVZ7Dp9ObVuiaww/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天和大家分享同济大学的最新研究FaceShot: 一举打破肖像动画模型“驱动真人”的局限，FaceShot 的动画效果可应用于各个领域的角色，包括 3D 动漫、表情符号、2D 动漫、玩具、动物等等。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494244&amp;idx=2&amp;sn=10671ddeb51b359b6fb549e68e8f66fb&amp;chksm=fdd033b40bf2d4cda9a79fdac0a598e29f2528ce8d5cf1f18115545c171b43de3c11323c6c77&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 23 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[6秒音频即可克隆AI语音！FLOAT数字人生成语音/口型/表情，情感同步超惊艳，文中附工作流。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elmzbxIf6OS3v7M1woTicaJcmBGicWjwiauMpFknBOofINibzHjBSIibjwDHKYvhnzulS1E2KIPicobCywA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的 FLOAT 是一种基于流匹配的音频驱动的说话肖像视频生成方法，可以增强语音驱动的情感运动。该方法唇形同步质量高，生成速度还很快。6秒音频完美生成语音/口型/表情。情绪转移由于 FLO</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494244&amp;idx=3&amp;sn=2659d8631c4ec20000486087ce5584c6&amp;chksm=fd788a70fa1deab6dda5fbb4844df4a106b07e5a7cca2ab8a5fabffcdc2a21276804422afcb7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 23 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[StochSync：可在任意空间中生成高质量360°全景图和3D网格纹理]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enXFFuKUPQcYLlG3aibBhJtN7xgJCpXZE7HoaWiahrDNLktV0doUSl1wRalx4MZej02YkgNsTVfSbpg/300?wxtype=jpeg&amp;wxfrom=0"/><p>StochSync方法可以用于在任意空间中生成图像，尤其是360°全景图和3D网格纹理。该方法利用了预训练的图像扩散模型，以实现zero-shot生成，消除了对新数据收集和单独训练生成模型的需求。St</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494244&amp;idx=4&amp;sn=5f01342a8e09cc6cd151dec09614e840&amp;chksm=fd72ec70f1aec85839092d6060000696d78918f5298dc6f5a4d8c02bc25e59e65f92c956fe63&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 23 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR2025 | 图像生成加速新突破！南开大学提出Loopfree，1步编码+4步解码，重塑图像生成速度与质量。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibWvnfl5ezdTp2XA9hLiadicTgKTsPJdJ0Nwj7SmRVCWh0AKGke0VtfXQIEC1J9YlyOvDSiaticOjguw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在图像生成领域，文本到图像（T2I）扩散模型近年来取得了显著进展，但推理速度与图像质量之间的权衡始终是制约其广泛应用的一大难题。南开大学等在CVPR2025上发表了《Loopfree: 时间无关的统一</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494233&amp;idx=1&amp;sn=a6bf23792f504600c2769a8739f886c4&amp;chksm=fda1d9b0f1f3cbdde94fd8322537ef191ea7293d7fccb8c92f06560e21d0b3982d101d69d095&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 22 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[单图生成3D头像+AI编辑+多模态驱动？阿里LAM让虚拟人“活”了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en9libmJyfFzq4ma8I0IqAGY3dib7yN0HLOdysDOE9mgQUibQDzEyr5tB9daDg9fq9JmJqBeOgnB0zgQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>LAM 是一个能从一张图片中一次前向推理重建可动画3D高斯人头的模型，不依赖多视角训练或额外渲染网络，支持跨平台、低延迟、实时渲染，是虚拟人、AI聊天头像与AIGC人物生成的重大突破。特点总结如下：从</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494233&amp;idx=2&amp;sn=110208784ecb00375803f6f89f900a04&amp;chksm=fd33e67efc6a25cf56bd577096dbb1b364f371f7561fec32d811329d2e8d252ab001aca5bb5e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 22 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[从文本到3D的“零训练”革命！英伟达&amp;康奈尔大学提出 ArtiScene：通过2D中介实现高保真3D场景合成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48HPJzGndEliaF6RR9oS2mcY6tk1tO13iaxO3UHzBrtzwlN2jFlpv4651g/300?wxtype=jpeg&amp;wxfrom=0"/><p>由英伟达和康奈尔大学提出的 ArtiScene 是一种无需训练、语言驱动的 3D 场景生成流程，它可以根据文本提示，设计出丰富多样、美观且易于编辑的场景，涵盖各种类别和风格。下图中展示了四种结果，并附</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494233&amp;idx=3&amp;sn=18ee43b1ea84497d0abb49d1d00ffebb&amp;chksm=fd954c15347c4dbf8c253de7e064bc558fbac050060142d9f77b17f533df56f68f404377e7df&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 22 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[碾压开源与商业模型！腾讯开源一致性视频生成框架HunyuanCustom：可同时实现音频同步与视频编辑！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emrh9ic75wTHs9dqezVrp8ttuUVmic8QW7affiaZjHpsxtibwNLZzmTDHNHfticnBcXHEWnBic45icRHnibicQ/300?wxtype=jpeg&amp;wxfrom=0"/><p> 腾讯提出了一个多模态定制视频生成框架HunyuanCustom，该框架强调主题一致性，同时支持图像、音频、视频和文本条件。基于HunyuanVideo，该模型首先通过引入基于LLaVA的文本图像融合</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494233&amp;idx=4&amp;sn=58e033ee81cc1426fe3fe06738f8ef40&amp;chksm=fd3ba2e905594ab15b73bf279989575a953da8a2e08271c7c3444b3a7205b9b58136f9445afc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 22 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[数字人新突破！FantasyPortrait：高德阿里北邮携手，开启肖像动画“幻境”之旅！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em0AmDO6mb5DLoAgCkVm29EcIEITkjGUbAiczNzPZIsINt6VeribQHhib2bppp56iayPt5g3HIad5VbUQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>由高德、阿里、北邮联合提出的FantasyPortrait，提供一张肖像图像和一段参考动态视频，在交叉重演过程中生成逼真的动画肖像。无论单人还是多人，它都能实现高保真的面部动态和自然的头部动作。uns</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494222&amp;idx=1&amp;sn=904751cfd592fadccc037e19273db616&amp;chksm=fd84a750404b7dbc3d5210257a9ea4a0212063429e1d05131c559a84c8437bf574be7c63c6c9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 21 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[美团LLIA登场，让照片“开口说话”不卡顿：低延迟、高帧率，音频驱动肖像视频进入实时交互时代！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48pVRHuhqxC0rZetfMaQO9vH1icmibhHy1HIl7coV7ubJl4gO4TZRJKv4w/300?wxtype=jpeg&amp;wxfrom=0"/><p>美团提出了一种基于扩散模型的音频驱动人像视频生成框架LLIA。该方法实现了低延迟、流畅且真实的双向通信。在NVIDIA RTX 4090D显卡上，该模型在384×384分辨率下最高帧率可达78 FPS</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494222&amp;idx=2&amp;sn=cdc624f0856408d57ded40e2ed138c32&amp;chksm=fd95350b8f98fc6228bca765d5a7ace30abd87797ad445a05234438b63e493a3eaae13dede2d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 21 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里开源FluxText：轻松应对多语言多场景文本编辑挑战, 海报|广告|游戏|文案编辑轻松搞定！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek8r1mbBOtrNsSTkhqjMvnHA3l9PourHLbs4icMEsVsuPWfFWhxiaESVMM2sxiac1nib3sib54N9KicTMtg/300?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中已经和大家介绍过许多关于文字编辑和生成的方法，感兴趣的小伙伴可以点击下面链接阅读！字体控狂喜！Liblib AI 黑科技 RepText：无需理解文字，AI就能 1:1 复刻多国语言视觉</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494222&amp;idx=3&amp;sn=0d10391e6defb53fbfc547f601de6d1e&amp;chksm=fd877b2c5c4752e5248ce77dd7ca549c7b7feeb821642035bc7d16f02bd1c386163297aa224c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 21 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[200M参数吊打商业巨头！浙大-哈佛开源ICEdit，用1%资源实现图像编辑自由！一句指令生成海报级修图方案。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekBUypVTojw9NicChAveibQcTLDAqzMNvNlVcY8Qyp0Uw8xV4We94zZVeNGOiaduoVu37rdJoZlxoibZg/300?wxtype=jpeg&amp;wxfrom=0"/><p>浙江大学联合哈佛大学提出一种高效的基于指令的图像编辑框架ICEdit，与以往的方法相比，ICEdit仅需1%的可训练参数（200M）和0.1% 的训练数据（50k），就展现出强大的泛化能力，能够处理各</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494222&amp;idx=4&amp;sn=e4cbcdb102fa7be477357cc758bcd231&amp;chksm=fd2b2555e57a3f02298aa07bca96a235a2d2b5a81ae8b9e3578b68222d7fe72156fedceeebdf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 21 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[让静态照片“活”过来！OmniAvatar：LoRA 赋能，解锁音频驱动全身人像视频生成新密码。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elHjoNmnhIZ1WlEINYQPhBVwHtIicGD7DRHo7WEJn1mYDaiaRkEf8ES1uzXk9uBREDlwHfWKdLcqs2w/640?wxtype=jpeg&amp;wxfrom=0"/><p>OmniAvatar：“全能”的数字人视频生成。OmniAvatar 是一个基于LoRA的高效的音频驱动全身人像视频生成系统，支持从音频 + 单张图像 + 提示语生成自然、表达丰富的视频，仅需一条音频</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494186&amp;idx=1&amp;sn=0d7d10fc2a1e18ca1fe882705684930d&amp;chksm=fde724b63482dc0fe4d936b20ffc41ed53261d8e8f37d2b7454180dc0ed9df7dbb1914d0ec01&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 20 Jul 2025 16:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[一文搞懂大模型时代的Agent：方法论、应用与挑战]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/B1OJ3jLyfic5px0fSndBD2GIVIzRDOXKqBib4cgWWxqiceO15Zp3U2X3KkEx0iaZKvQMuTmjTliagvqkof4ibrME5SSQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>“写在前面【从零走向AGI】旨在深入了解通用人工智能（AGI）的发展路径，从最基础的概念起，逐步构建完整的知识体系。项目地址：https://github.com/AI-mzq/From-Zero-t</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494186&amp;idx=2&amp;sn=362b98ad57267d32169520a377b98f05&amp;chksm=fd414362c66861b7a7d09a9b51f3b940e0de793d1294fe2d728cf03869f6e12b34928d976bed&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 20 Jul 2025 16:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Tora2震撼发布！阿里巴巴首创视频多角色「外观+运动」同步定制，告别视频角色错位！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icojC01mC63A6Ct1xfHLt8tlNgDeAiauMvOdnh0sPX1YebicwQ3j1aGaeib56Sjst9hO8c37azBjrtMKBQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>亮点直击Tora2，首个支持多实体定制化的视频扩散Transformer框架。如图1所示，Tora2 支持外观和运动轨迹的双重控制。引入了一种解耦的个性化提取器（Decoupled Personali</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494186&amp;idx=3&amp;sn=09be64fea36efc9997fa9262e7744a13&amp;chksm=fd1f3d05b093c0ee9795012d314d34ae6af79843f26f848c9e17e4cc20e16a273a676b617616&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 20 Jul 2025 16:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[用ComfyUI复原老照片，像素级重生太惊艳了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/ACyQFjNqyE62umia43diaibQHp3ufyS7wiaxIEibtXWGdYtMcl71rnkX5MWXjibQMdn9RUbu6m0NMMfLoryHo7Tqzluw/300?wxtype=jpeg&amp;wxfrom=0"/><p>过去的一张张老照片，承载着无数回忆，也记录着一个时代的光影。但随着时间的流逝，那些泛黄、破损、模糊的老照片正一点点被遗忘。幸运的是，AI图像处理的浪潮正悄然改变这一切。而在这股浪潮中，一个名字正在悄然</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494186&amp;idx=4&amp;sn=4bde151136f623d010c7b412f80c2074&amp;chksm=fd089ca651bb54ac76be20ebc4b7dd573f4a2d01dda60eda4f94e63220aec8a98ca01c7fb778&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 20 Jul 2025 16:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[人机交互动画制作新突破！文本驱动扩散框架HOIDiNi：一句话驱动虚拟人高精度操作物体。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elKibiah2ldIaVhKqwicV967dT1sIM1Fp2cOZVSwYwa6MN7rFE0yC7GRXAazMHfQ7D56MYzIJaS7HqJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在人机交互、动画制作等领域，如何生成逼真且符合严格约束的人体 - 物体交互（HOI）一直是科研人员努力攻克的难题。今天给大家分享一个文本驱动扩散框架——HOIDiNi，它不仅满足 HOI 的严格约束，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494184&amp;idx=1&amp;sn=e56ebc05c812853d918c61980edd5b6d&amp;chksm=fd743772efcdf7809f5308858d0419a91abed715a1f9bf1295d98b1381c2cbc332a41e9e48a8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 19 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[中科院提出图像定制方法MCA-Ctrl，无需调优的即可使用文本和复杂的视觉条件实现高质量的图像定制。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enKvWzJ9QLeWgYQiaKmEWxL1KFwReqeCxk2s3eVTGWZJ0ibicT6EGpGzrsJ4W6jtTm4fLh17icB9CJNyg/300?wxtype=jpeg&amp;wxfrom=0"/><p>中国科学院计算技术研究所研究团队提出了多方协作注意力控制方法( MCA - Ctrl )，这是一种无需调优的方法，能够使用文本和复杂的视觉条件实现高质量的图像定制。MCA-Ctrl 可用于文本驱动的主</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494184&amp;idx=2&amp;sn=ef454362b62ef73bd1fedd431456c3a4&amp;chksm=fd4ea415cba8098f581a5a35813b32afef6a24ab428edf91232dc03b24b24dee9b1d47095748&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 19 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[200M参数吊打商业巨头！浙大-哈佛开源ICEdit，用1%资源实现图像编辑自由！一句指令生成海报级修图方案。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekBUypVTojw9NicChAveibQcTLDAqzMNvNlVcY8Qyp0Uw8xV4We94zZVeNGOiaduoVu37rdJoZlxoibZg/300?wxtype=jpeg&amp;wxfrom=0"/><p>浙江大学联合哈佛大学提出一种高效的基于指令的图像编辑框架ICEdit，与以往的方法相比，ICEdit仅需1%的可训练参数（200M）和0.1% 的训练数据（50k），就展现出强大的泛化能力，能够处理各</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494184&amp;idx=3&amp;sn=a1d21f3d1831bacaff99250cecc9324d&amp;chksm=fddc0da1a9a9606592dd0555c618e6c35b68b286c1f54a4ca6127d2989b9fda2bd9a42793765&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 19 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节提出 DreamActor-H1，让产品与模特“一键生成”高保真交互视频。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48RPbNymvllhYib3H7VZyk223eWwAma3ovH2vTKZCY7Hg7zPurzD9kpqQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>DreamActor-H1 是一个基于扩散变换器 (DiT) 的创新框架，能够根据配对的人与产品图像生成高质量的人与产品演示视频。DreamActor-H1 基于大规模混合数据集进行训练，并结合多类别</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494184&amp;idx=4&amp;sn=bc7834ff61e4d94e8d0b051c070ecaf4&amp;chksm=fd02b3b195ea602d0055dfb50bc7bf122606c0439775a83a8d6708007a29ca2e8553c9da99db&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 19 Jul 2025 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>