<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    























    <item>
      <title><![CDATA[CVPR 2024 Spotlight | 解锁图像编辑新境界, 北大、腾讯提出DiffEditor，让精细编辑更简单！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emtHS7t5ic0uQWb1AOhKNDRVPEH3Xqks3oDpG6kIgEczYPnVPsI98I9LqibzYz8fUXeJrNILLDFicB0A/640?wxtype=jpeg&amp;wxfrom=0"/><p>在图像生成领域，大型文本到图像（T2I）扩散模型近年来取得了革命性的突破。然而，将这些强大的生成能力转化为精细的图像编辑任务，仍面临诸多挑战。CVPR 2024, 来自北京大学深圳研究生院与腾讯PCG</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=1&amp;sn=78e02ad557acbddd73965e30230fe090&amp;chksm=fdbe7557a5166b386b9963284767c09132e19c70d858552f3ead19a6560e9aefc86fe035e88b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[Adobe与MIT推出自回归实时视频生成技术CausVid。AI可以边生成视频边实时播放！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDwNBzSGR9jz4olCuaibHPBcISVDNbZVjdKgcIl4GiaczxalR4zb0LKJQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>传统的双向扩散模型（顶部）可提供高质量的输出，但存在显著的延迟，需要 219 秒才能生成 128 帧的视频。用户必须等待整个序列完成才能查看任何结果。相比之下CausVid将双向扩散模型提炼为几步自回</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=2&amp;sn=c6502c899b6e3f852dfd2041911215b9&amp;chksm=fd54050113a0b64568ab964cb7a0254d15670d27d222e071ba613a4e6d1dea7fec1a8fb80b56&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[多模态图像生成模型Qwen2vl-Flux，利用Qwen2VL视觉语言能力增强FLUX，可集成ControlNet]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYHx9RLCm1u1zJr61WGBPZZicviaGPyXN8y5ZTaZE9jpPcdNSX1nmUlib5g/300?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen2vl-Flux 是一种先进的多模态图像生成模型，它利用 Qwen2VL 的视觉语言理解能力增强了 FLUX。该模型擅长根据文本提示和视觉参考生成高质量图像，提供卓越的多模态理解和控制。让 F</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=3&amp;sn=27a5753da03f6a835735475d04049920&amp;chksm=fd110660ccaf10ce849fe434a13dc1af783f42a485f7a20baaef50b5258309848cfe0c36dc61&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[Google发布新AI工具Whisk：使用图像提示代替文本，快速完成视觉构思。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2end3mWUdomxapVIqKPBfrWChHLuwfMCvRvq1l8Kl6qfOOlq9YUxzPEdzMibDSUo0R5owCSicTJLumBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>Google发布了新的AI工具Whisk，Whisk 是 Google Labs 的一项新实验，可使用图像进行快速而有趣的创作过程。Whisk不会生成带有长篇详细文本提示的图像，而是使用图像进行提示。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=4&amp;sn=a9f392d276409795dbe3f70c902346be&amp;chksm=fd8cb3e29c053652af42eda3f111fb346358a870517a14972adf84936712f347e75edb5b911c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[Snap | 港科大提出端侧文生图模型SnapGen，参数仅SD十分之一，1.4秒内生成1024分辨率图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDuiaVeqQlxjbAyWQVBK9zTWxMkxd3V2yvSqRYageKwuRPwuQj7g2Iog/640?wxtype=jpeg&amp;wxfrom=0"/><p>这项工作提出了一种新颖且高效的 T2I 模型SnapGen，SnapGen 是第一个可以在1.4秒内在移动设备上合成高分辨率图像（1024x1024 ） 的图像生成模型（379M ） ，并在 GenE</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=1&amp;sn=1432436e0f71b9b7e486560668eeed7a&amp;chksm=fd9bc6a0e991e310061f042c8137126e01921ff048fc146bdb79d645fa89ad4db423f53e59fc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OminiControl：一个新的FLUX通用控制模型，单个模型实现图像主题控制和深度控制。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYEn3PFyv0qTxQYEgq8VntmUj91vEEYPJjMADiamfkH94icSBs7fF1Tn1A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中和大家介绍过Flux团队开源了一系列工具套件，感兴趣的小伙伴可以点击下面链接阅读~AI图像编辑重大升级！FLUX.1 Tools发布，为创作者提供了更强大的控制能力。OminiContro</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=2&amp;sn=5f893661ace33b3910b950d9030b7a1c&amp;chksm=fdaad180939f5106e2305580c604ad3fa7e7f1ee76410babf76cc5e46cf65de4c08bb3da09a7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里发布新ID保持项目EcomID, 可从单个ID参考图像生成定制的保ID图像，ComfyUI可使用。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwazEmicf1F7ZjrxQSj4JNP3x3icluxM84Et2UYGdsdxfDOXnd9OlZYFCwg/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里妈妈发布了一个新的ID保持项目EcomID，旨在从单个ID参考图像生成定制的保ID图像，优势在于很强的语义一致性，同时受人脸关键点控制。EcomID 方法结合了 PuLID 和 InstantID</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=3&amp;sn=2a131da1305f4c755047f499f0b2fb72&amp;chksm=fd9be0eebc204661357981b7050c377b021814104b3d4c883a27a20527b26204b3694c9f62ed&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[港大和字节提出长视频生成模型Loong，可生成一分钟具有一致外观、动态和场景过渡的视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eny4Iriba5NSXkHvLxicLITJD5gFTkWLoBqMSNfUicQxXgibIh9n6vokK5ia5EOh7ZDJLVHGEsbaLz86XA/300?wxtype=jpeg&amp;wxfrom=0"/><p>HKU, ByteDance｜⭐️港大和字节联合提出长视频生成模型Loong，该模型可以生成外观一致、运动动态大、场景过渡自然的分钟级长视频。选择以统一的顺序对文本标记和视频标记进行建模，并使用渐进式</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=4&amp;sn=a3dc9baea4db7b2ce69c09edf47daebe&amp;chksm=fdce4e89e36260261732b1375680ed14de82d2c716f27c5952d048065d3058b47cb8a2e759f1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Github热门机器学习笔记:「从零构建大型语言模型」]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emtHS7t5ic0uQWb1AOhKNDRVQe1ibV5hcbvDj7icpDN1BtRicibpaHbuszyA75wydLlzCvmBKSLia5XJSLQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家推荐一份GitHub上很火的机器学习学习笔记《从零构建大型语言模型》，目前已经收获1.4K stars，，这份笔记完美展示了从零构建LLM的技术路线图，既有理论深度，又包含实践要点。每个核心</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489955&amp;idx=1&amp;sn=53339b99584662616f0959a58cfa0826&amp;chksm=fd13928f93ecb22328315109c5122112f3f0833c72c28c0bef286629b12231eef78a5f1bb74d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 17 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里发布新ID保持项目EcomID, 可从单个ID参考图像生成定制的保ID图像，ComfyUI可使用。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwazEmicf1F7ZjrxQSj4JNP3x3icluxM84Et2UYGdsdxfDOXnd9OlZYFCwg/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里妈妈发布了一个新的ID保持项目EcomID，旨在从单个ID参考图像生成定制的保ID图像，优势在于很强的语义一致性，同时受人脸关键点控制。EcomID 方法结合了 PuLID 和 InstantID</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489955&amp;idx=2&amp;sn=a5e8d093a053334b4260f36de4dbbb5e&amp;chksm=fd179381eb1331749a1f8d687b7629f5efad372be3d703e037180fe1de802588fc987c7ee8ce&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 17 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ComfyUI服装设计，一个工作流搞定！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/ACyQFjNqyE6jwFqazZJ5fESF3UlicH6GlH8oBF0nugTB4K9GZwyblhme7GVxoiaXUrN6Ce0c5ibFPOkwd5AttyiaCw/300?wxtype=jpeg&amp;wxfrom=0"/><p>ComfyUI：为你的图像创作赋能的强大工具在AI技术迅猛发展的今天，Stable Diffusion成为了图像生成领域中的一颗明星，而基于此开发的ComfyUI更是为用户提供了一个强大而直观的工具，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489955&amp;idx=3&amp;sn=a98d0e628800bb695eb296da98f95ef2&amp;chksm=fd7693c1962a51b2ea5fc4b9d061dcac402bf1341fe3a0699247908b91cae002efc070b30c3f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 17 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[引领图像编辑领域的新潮流！Edicho：实现跨图像一致编辑的新方法(港科&amp;蚂蚁&amp;斯坦福)]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicEKO56ibffMsiau7qrg2gcpibqTFwwB20Tz8hX6wXGcTC684He5MiazvvzA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在图像处理领域，如何实现跨图像的一致编辑一直是技术挑战。传统方法往往局限于单张图像的编辑，难以保证多张图像间编辑效果的一致性。香港科技大学、蚂蚁集团、斯坦福大学和香港中文大学联合提出Edicho，这一</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=1&amp;sn=8f3541833f1b3a1ab71d132b9ae84666&amp;chksm=fd42cdd0435be406b12120fb005b4c4372e9c0c9dc338439136155dc6b190b9bf016091492f3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图像编辑大一统？多功能图像编辑框架Dedit:可基于图像、文本和掩码进行图像编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekDYMeOJw6PMrPrgUmBfVvIibC8Suae7poAtMSSVAkicNMibK5CyJB4RLSAKFiajeuqXiaiaib0vMibRiaSKCQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个基于图像和文本的编辑的框架D-Edit，它是第一个可以通过掩码编辑实现图像编辑的项目，近期已经在HuggingFace开放使用，并一度冲到了热门项目Top5。使用 D-Edit 的编</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=2&amp;sn=21834e17f83cf54755f4fe2b99836471&amp;chksm=fd51575dfac0bef5eadf6ca456bcf7fbfa3693f52424e4337ed0c4c56ce40fe82a1f5cab5119&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[扩散模型 vs GAN，谁将主宰文生图的未来？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/V4HEViaySCn8Onq8lDttiaFTpUiabu4PiassM06oMOYp7S6fggSCrREIZ3IoecSjrFkOF5jdiaPODhTx0ALBn3icp8Zw/300?wxtype=jpeg&amp;wxfrom=0"/><p>点击蓝字 关注我们导读你知道，就是那种能根据你的文字描述，一键生成图片的神奇技术。想象一下，你只需动动手指，输入一段文字，就能得到一张与之匹配的图片，是不是很酷？从最早的基于GAN（生成对抗网络）的技</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=3&amp;sn=6461fb360bdc70552893d69b1260caf0&amp;chksm=fd41c645caa30cdbe04e69fdb1d738877236baa432454be61ed3b7a23ec777a8f3e93e15345a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[长篇故事可视化方法Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=4&amp;sn=25eaecb6ed069f0bc5b3a09873f7d441&amp;chksm=fd735b674728b3e39d42ad141a023a6429bdebef7d1773a6852d410430d7c958b942d115abd5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[中科大提出新视频流制作动画解决方案RAIN，可实现真人表情移植和动漫实时动画。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekaVfDRjALdOCj5889F1MpAKe7VdTw8TPT3jvjm7A5B4CnAk0BFdqibXzkZls0gnHjLqpeExdtPhpw/640?wxtype=jpeg&amp;wxfrom=0"/><p>中科大提出了一种新的视频流制作动画解决方案RAIN，能够使用单个RTX 4090 GPU 实时低延迟地为无限视频流制作动画。RAIN 的核心思想是有效地计算不同噪声水平和长时间间隔的帧标记注意力，同时</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=1&amp;sn=ad177e495ad3e2052d95dbe49a68ce5b&amp;chksm=fd643ea4d20d09f8ca2c1910bdd5e4a3cfc2a6a287761384b05e51950c1277878f85052cb161&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[理想汽车提出3DRealCar：首个大规模3D真实汽车数据集!]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eldrtUtjxDXrVGVcbe8sY5fn25qjoY7k2Bsz6XJV8GAUvw3FQiaWMcbeiadZBbw9ZE2f1znye2jstGw/300?wxtype=jpeg&amp;wxfrom=0"/><p>理想提出3DRealCar，这是第一个大规模 3D 实车数据集，包含 2500 辆在真实场景中拍摄的汽车。3DRealCar的目标是可以成为促进汽车相关任务的宝贵资源。3DRealcar包含各种颜色、</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=2&amp;sn=2c49fe640f47cd28e9c19753d4fc21e5&amp;chksm=fd179b43565b066cdd786bfcea59e1768886f733e4cb57ba7d5e6b1857ec3c46e7b20b1fe2c6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[UIUC提出InstructG2I：从多模态属性图合成图像​，结合文本和图信息生成内容更丰富有趣！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZqBubdMgx3yBMfDK8JGL4YYX3hw4kJVRCHjFaqvVYYc7nEPXjibpCEug/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的这项工作是伊利诺伊大学厄巴纳-香槟分校的研究者们提出的一个新任务 Graph2Image，其特点是通过调节图信息来合成图像，并引入了一种名为InstructG2I的新型图调节扩散模型来</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=3&amp;sn=04ea94ae2fb61f64de0600a942236967&amp;chksm=fdc660d4cc39437f66c5fa2a96ccd4b26b472eb21bf811ccf8010a8612e5d12eec6af0d81fc3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[SD和Sora们背后的关键技术！一文搞懂所有 VAE 模型（4个AE+12个VAE原理汇总）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icojhGksxPcuzzcs6IGgx9ulYoFvA0ibAbJu9UfI318Eq0Y8x4wk2aoFzldXC9DKiclmSIkGiclenOCY2g/300?wxtype=jpeg&amp;wxfrom=0"/><p> 点击下方卡片，关注“AIGC Studio”随着Stable Diffusion和Sora等技术在生成图像和视频的质量与帧率上取得显著提升，能够在一个低维度的压缩空间进行计算变得越发重要。这种方法不</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=4&amp;sn=ef3daf262bc2df101417b200b505e198&amp;chksm=fd2f5a19b6f43147db8be066445396516a176e6a9d7284afcf5956c730473c668c1b3e39bbdd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adobe与MIT推出自回归实时视频生成技术CausVid。AI可以边生成视频边实时播放！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDwNBzSGR9jz4olCuaibHPBcISVDNbZVjdKgcIl4GiaczxalR4zb0LKJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>传统的双向扩散模型（顶部）可提供高质量的输出，但存在显著的延迟，需要 219 秒才能生成 128 帧的视频。用户必须等待整个序列完成才能查看任何结果。相比之下CausVid将双向扩散模型提炼为几步自回</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489895&amp;idx=1&amp;sn=e23d2324419a090406755f4485d19462&amp;chksm=fdf458d4f29287fde5d74485fbc12cacd9075a7e81b3e97935a12290ff06e2eb2312da594be2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2024 AI TimeLine 回顾（独家视角）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUnCMiaBjZZszIFmMq4b6euJ14V5ibkAcCZdmtRw3lykFcu3iafSO7319RSVydS3scYAgM0oASkKTw/300?wxtype=jpeg&amp;wxfrom=0"/><p>2024 AI TimeLine 回顾（独家视角）2024年，生成式人工智能已远远超越了仅仅作为一个流行词的范畴，它在实际应用和技术创新方面取得了显著进展，成为推动社会进步和产业变革的重要力量。以下是</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489895&amp;idx=2&amp;sn=1dce7fa3a57fc7211c9377baaf0250f6&amp;chksm=fd8587a28797f79d5bdb7df064e27c1ca2580f1014d00e93dadee1de1c98b8189382011c5b16&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ScribbleDiff：使用涂鸦精细引导扩散，实现无需训练的文本到图像生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en4dVnOT75Vve5gBZeAMAcqnHFQnQNTu2jZ3gdtvtEhgfeuBiawdPpo4eRXb4xIj7t0TCyfMVB3Rhg/300?wxtype=jpeg&amp;wxfrom=0"/><p>ScribbleDiff可以通过简单的涂鸦帮助计算机生成图像。比如你在纸上随意画了一些线条，表示你想要的图像的轮廓。ScribbleDiff会利用这些线条来指导图像生成的过程。首先，它会分析这些涂鸦，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489895&amp;idx=3&amp;sn=50249778fb8c43c58af4c0b24772fec9&amp;chksm=fd5d1fbd23d530c55d38ffaacd478644a92c459f9d7c4c24be060d7f1177c52a5533fb21751a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Retinex-Diffusion：让图像照明更加自然、细腻、富有层次感。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enUPMyjgiaKvq6tT4kEsgP5AtNIVia1UOcToyf7rIe0yncY9LBCDwGDAqtMbZTdicZDWLLWnnDLGEPhA/300?wxtype=jpeg&amp;wxfrom=0"/><p>这项研究主要是针对如何智能控制图像中的光照，采用了一种不需要重新训练模型的新方法。简而言之，研究人员利用一种叫作Retinex理论的方法，先识别出图像中的光照元素，然后用这些元素来指导图像生成模型。具</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489895&amp;idx=4&amp;sn=cc61032505cb7c4ea2a2bd594760b149&amp;chksm=fd8e59cf1e33a28ea7571075e4a6538562fd910a3f26a52e6f1f84f5ac4ea218676da97fb29c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
