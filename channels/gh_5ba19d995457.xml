<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[阿里发布新开源视频生成模型Wan-Video, 支持文生图和图生图,最低6G就能跑, ComfyUI可用!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emELibL5kVkYibicFiaR0laM2LPanMoANDKaC5QVXWXiawVrJIjXcdbschfSzanarf8EbFZpGTOSicjTZAQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>Wan-Video 模型介绍：包括 Wan-Video-1.3B-T2V 和 Wan-Video-14B-T2V 两个版本，分别支持文本到视频（T2V）和图像到视频（I2V）生成。14B 版本需要更高</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490584&amp;idx=1&amp;sn=fe694eb011d3353ffb9d928dbf0a53be&amp;chksm=fd0246e0ed834cd8ed25b326c914a22c27d26419b3e1c5afe597548661aebcc2404db6482a7d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 05 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[电商领域利器来了！港大&amp;阿里提出MimicBrush，可模仿参考图进行零样本图像编辑。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enu15BEdxn8DpEdshmGPMicepiaSYu1INiahHv6ZdWxcTRjT1UexEYITfITVC8uhS6hWlib4Wodyfrr3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里和港大提出的MimicBrush可以通过对参考图模仿进行零样本图像编辑。将一张图片的某一部分融合到领一张图片上去。用在电商商品展示上或者单纯的图片编辑和内容迁移很有用。从官方演示来看效果也很好。M</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490584&amp;idx=2&amp;sn=c93414ea4529a07a782a7c6ceb1fa9d3&amp;chksm=fd61c34ae8594dfa5892c10f52deee502175222b53c497cf671f77ec6a7a90952aa7da2342af&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 05 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[澳门大学提出DC-ControlNet！解耦控制条件！灵活性和精度超过ControlNet！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5IJObOoyvhRkCaPGyos7d8xL9KBFJiaWYgoicVEkmuuB7slvPLj3SIW9jx5pace0iagDibDDTLU1P3Lwg/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名：DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490584&amp;idx=3&amp;sn=ac5e2ddabb690e93ebeda1861f99f902&amp;chksm=fda3e3054ad4d470503d95e966f1fbc17c31a99236066de6ece904408b7f30b82aa4b0a08b72&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 05 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICIR2025 | CubeDiff：无需考虑失真，重新利用基于扩散的图像模型来生成360°全景图]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emrL9coT0EQdTjZR7WCoOG6qavvqaKicyhfbe1wrRfKuEmZbfJ8LvrOgQJMgZYG5CztqNUPPASQbtg/300?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中已经给大家介绍过许多关于3D生成的文章，感兴趣的同学可以点击公众号菜单栏查看3D生成专栏，创作不易，欢迎大家点点赞和在看~CubeDiff是一种使用基于扩散的图像模型生成 360° 全景</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490584&amp;idx=4&amp;sn=5eaf557b0b77ce14365259b921883d5f&amp;chksm=fd163121ab3ab307f4b1af978e490b0d9dbb2c1c9efc6401b1479c8312233ad38589e210823f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 05 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICIR2025 | CubeDiff：无需考虑失真，重新利用基于扩散的图像模型来生成360°全景图]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emrL9coT0EQdTjZR7WCoOG6qavvqaKicyhfbe1wrRfKuEmZbfJ8LvrOgQJMgZYG5CztqNUPPASQbtg/640?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中已经给大家介绍过许多关于3D生成的文章，感兴趣的同学可以点击公众号菜单栏查看3D生成专栏，创作不易，欢迎大家点点赞和在看~CubeDiff是一种使用基于扩散的图像模型生成 360° 全景</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490543&amp;idx=1&amp;sn=5612888e956f0106578eaa019f577656&amp;chksm=fd0f27c47f012bf005fca5132a31a6f4bdb2bdf08c9532581d8e2caecc5e65a7d0536a9e8a1e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 04 Mar 2025 16:12:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[StochSync：可在任意空间中生成高质量360°全景图和3D网格纹理]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enXFFuKUPQcYLlG3aibBhJtN7xgJCpXZE7HoaWiahrDNLktV0doUSl1wRalx4MZej02YkgNsTVfSbpg/300?wxtype=jpeg&amp;wxfrom=0"/><p>StochSync方法可以用于在任意空间中生成图像，尤其是360°全景图和3D网格纹理。该方法利用了预训练的图像扩散模型，以实现zero-shot生成，消除了对新数据收集和单独训练生成模型的需求。St</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490543&amp;idx=2&amp;sn=4bcb6a9ceb33d74506d10c9192481efa&amp;chksm=fd5c46beedb75bf24522af9c98149de73eddff0c393a8e2fc377b363c26fc67f9fb470f05c98&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 04 Mar 2025 16:12:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[单幅图像合成 360° 3D场景的新方法：PanoDreamer，可同时生成全景图像和相应的深度信息。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el8quKicUEibqsQFrF8ttU8UZJIaZuVsUww2Z2IDLp7MYqLaIsWuo5dAG2Y1iaHf8AibCjXj4KFPbTv3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文介绍了一种从单幅图像合成 360° 3D 场景的新方法。该方法以连贯的方式生成全景图及其相应的深度，解决了现有最先进方法（如 LucidDreamer 和 WonderJourney 的局限性。这</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490543&amp;idx=3&amp;sn=c6643842afa2c65148191fa5b03e636c&amp;chksm=fd8c0d2e25438e364d8a0ca539bdd0b1ce2b1421a1834815485a82078508ddfba2251035e97f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 04 Mar 2025 16:12:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[TRELLIS：用于创建多功能、高质量的360°全景图生成方法，实现可扩展多功能3D生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsrcRtOibx92ZhEt1Z0UdQ7JsQibr17Y0WaD8O08DMM3XIor43XOZVMvXg/300?wxtype=jpeg&amp;wxfrom=0"/><p>清华大学、中国科学技术大学、微软研究院联合提出T RELLIS，这是一个大型 3D 资产生成模型，可根据文本或图像提示（使用 GPT-4o 和 DALL-E3）以各种格式生成高质量的 3D 资产，可在</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490543&amp;idx=4&amp;sn=2cbb023aa7f1bd83b5de17d2b9cc6526&amp;chksm=fd1e2aa8a4311c9ff6b1fd788f0da9698d8cce11c373dffdadee68df671f607c2aa9cb27b584&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 04 Mar 2025 16:12:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南开提出1Prompt1Story，无需训练，可通过单个连接提示实现一致的文本到图像生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzPNo4OCBUcdtmaQJs6N0wWib04FeoPN0dUqhYsAJa4gIVOeKpt04Ox35gLkECLB9LEJGNnrDPu9g/640?wxtype=jpeg&amp;wxfrom=0"/><p>（1Prompt1Story）是一种无训练的文本到图像生成方法，通过整合多个提示为一个长句子，并结合奇异值重加权（SVR）和身份保持交叉注意力（IPCA）技术，解决了生成图像中身份不一致的问题，同时保</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490542&amp;idx=1&amp;sn=9b01d1603a7aaab7939445b11ab9f806&amp;chksm=fd066915a0549dba0c9c7d5746bbc6edef622c274fb22ef8633ee6ff8d34a71ef17349e6a028&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 03 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CineMaster: 用于电影文本到视频生成的 3D 感知且可控的框架。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekMLBRnvtbr9hh7W1ccXtbHEFc26iarR5N2r9CO0FlrI8VbA3yicts8jmfYqQu9BHcgSCETWsJ0PawQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>CineMaster是一种 3D 感知且可控的文本到视频生成方法允许用户在 3D 空间中联合操纵物体和相机，以创作高质量的电影视频。相关链接主页：cinemaster-dev.github.io论文介</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490542&amp;idx=2&amp;sn=e2a44ffbea977468ca18383d19f3541a&amp;chksm=fde5816f23c1435055648f7dbc3b30f0aef226dab2924544c45ec5570c9151dc4417eabe8b10&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 03 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[震惊！东京大学提出ARTalk！语音驱动3D面部动画大突破！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5JgxI2td6MuHkKtXCMBGlVyNoBucDYsH1Jct1PGOib0q03Jn6GdpLz4QjrI8emN5ohoxj6WzEHv54w/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model论文链接：https://arxiv.org/p</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490542&amp;idx=3&amp;sn=0c74f66577b3a85fdb1b1d79a6156a01&amp;chksm=fd33a5202de54545ce92731a21000648f8a910655c727cacd2df47288dc38a82cf2fa56b52d5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 03 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Magic Mirror：可从单个参考图像生成电影级质量身份一致性和自然运动视频]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emrL9coT0EQdTjZR7WCoOG6gAxgXB4PynfsscmlUfdakUvCDVQnWbSz48ZDHyhvW76iaaN3BpfbNqQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>Magic Mirror 可以生成合成身份配对的视频数据。该框架利用视频扩散模型，能够在保持身份一致性的同时，生成具有电影级质量和动态运动的视频。Magic Mirror 根据 ID 参考图像生成文本</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490521&amp;idx=1&amp;sn=9338c7246b5b4cc587818d0c2258e9a9&amp;chksm=fd14567aabbed1399ced8b0d2fe2de3d132991286ea51a5c471f3bffc0879b728b211d329c8d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 02 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[超级智能“试衣镜”！GarDiff：高保真保持目标人物特征和服装细节，虚拟试穿技术新SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUN5oqyRgSButjKACUwRIef94PhQmUMcfJSkj4W9NicELKlw377icJuhpfjx2VUNPWKHMM0Gqib5Eg/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍了很多关于虚拟试穿的文章，本公众号也总结了虚拟试衣专题在公众号菜单栏，感兴趣的小伙伴可以在公众号内搜索“虚拟试衣”阅读～今天给大家介绍一个最新的虚拟试穿技术GarDiff，它可以分析</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490521&amp;idx=2&amp;sn=2f6ae2eff14abfd89788ed4e5be1c032&amp;chksm=fd5fdde867001d56db3b13bbaac5154ff39d93f0619a4781d1da9eef14a1c45c4b1e9bc8f381&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 02 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[港大&amp;Adobe提出通用生成框架UniReal：通过学习真实世界动态实现通用图像生成和编辑。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en3n1j1LLVnKmKxjJUkVMkfcOvKg4alghJicfViaQXPN3cGVy3SYtSRiaWE0jyTlhQNs1mRy2lUoe0Pw/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的文章来自公众号粉丝投稿，由香港大学，Adobe提出的统一图像生产与编辑方法UniReal，将多种图像任务统一成视频生成的范式，并且在大规模视频中学习真实的动态与变化，在指令编辑、图像定</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490521&amp;idx=3&amp;sn=2ea722547f91ac965281f729fa88b406&amp;chksm=fde03d60420daace88c2d1035962c36da3e5834b246add52daa116ffc3b1514817f4b5163273&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 02 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[多模态图像生成模型Qwen2vl-Flux，利用Qwen2VL视觉语言能力增强FLUX，可集成ControlNet]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYHx9RLCm1u1zJr61WGBPZZicviaGPyXN8y5ZTaZE9jpPcdNSX1nmUlib5g/300?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen2vl-Flux 是一种先进的多模态图像生成模型，它利用 Qwen2VL 的视觉语言理解能力增强了 FLUX。该模型擅长根据文本提示和视觉参考生成高质量图像，提供卓越的多模态理解和控制。让 F</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490521&amp;idx=4&amp;sn=94f253805cdbd408e79c2addde107210&amp;chksm=fdc959a905b64dacf7d3942a97a5149e6860ed5605a6bd869672eb07c874c326c24e33bf3650&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 02 Mar 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CineMaster: 用于电影文本到视频生成的 3D 感知且可控的框架。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekMLBRnvtbr9hh7W1ccXtbHEFc26iarR5N2r9CO0FlrI8VbA3yicts8jmfYqQu9BHcgSCETWsJ0PawQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>CineMaster是一种 3D 感知且可控的文本到视频生成方法允许用户在 3D 空间中联合操纵物体和相机，以创作高质量的电影视频。相关链接主页：cinemaster-dev.github.io论文介</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490520&amp;idx=1&amp;sn=b217a6ebc742080bf81796c7b1d162f3&amp;chksm=fd91b7e8d4d682b2260cb1e4b5840f5c283885f577ac646ff12dc8073a9b37d7e095ed7e33f2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 01 Mar 2025 16:14:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[多模态图像生成模型Qwen2vl-Flux，利用Qwen2VL视觉语言能力增强FLUX，可集成ControlNet]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYHx9RLCm1u1zJr61WGBPZZicviaGPyXN8y5ZTaZE9jpPcdNSX1nmUlib5g/300?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen2vl-Flux 是一种先进的多模态图像生成模型，它利用 Qwen2VL 的视觉语言理解能力增强了 FLUX。该模型擅长根据文本提示和视觉参考生成高质量图像，提供卓越的多模态理解和控制。让 F</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490520&amp;idx=2&amp;sn=6bf63f8435ec46ef939e7efe6be4eab5&amp;chksm=fdf7179b55ec937165122670235b77f765b563fca797aebb071e5e5da28a1b6f5cc8c1430ecd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 01 Mar 2025 16:14:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[北航 | 第一个多功能即插即用适配器MV-Adapter：轻松实现多视图一致图像生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en3n1j1LLVnKmKxjJUkVMkfSL2lH1ru1uCJuUuA21YKHU5ia5SLlWu0BztQtHU3YSeZIYv3K9nGSHQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>北航提出了第一个多功能的即插即用适配器MV-Adapter。可以在不改变原有网络结构或特征空间的情况下增强T2I模型及其衍生模型。MV-Adapter 在 SDXL 上实现了高达768分辨率的多视图图</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490520&amp;idx=3&amp;sn=f9665d89e6f81c9dde852d80d8f13cb4&amp;chksm=fd6fe66eaec98737528c0dabbb57943ea004b1a494a053c058eed9885c6c4c74553d59150972&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 01 Mar 2025 16:14:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大 | 腾讯 | 华为提出视频生成框架VideoMaker，可由参考图实现Zero-shot定制化视频生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekaVfDRjALdOCj5889F1MpALuqg2wbFklkt9TIVHLSyQfTQ65do3Pe4Szhc0sWs0dMVTLfiavGbvRQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>浙大联合腾讯和华为提出了一种新的定制化视频生成框架——VideoMaker，利用VDM的内在能力，实现高质量的zero-shot定制化视频生成。该方法通过直接输入参考图像到VDM中，利用其固有的特征提</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490520&amp;idx=4&amp;sn=199e3c48ba0303fb009601951f9095dc&amp;chksm=fd6b19616f89fcb8f99d323d5487af5c993ef74ef8dc5ab141118c2e84983ac3c4aff5ca958f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 01 Mar 2025 16:14:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大提出视频生成方法VidSketch：可从手绘草图和简单的文本描述生成高质量视频动画。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekMLBRnvtbr9hh7W1ccXtbHgw4aIUWZuDmaia40dSE9oXFEapMjdUy4355HjGBNbH33XBwB6vDxFhg/640?wxtype=jpeg&amp;wxfrom=0"/><p>浙大提出的VidSketch是第一个能够仅通过任意数量的手绘草图和简单的文本提示来生成高质量视频动画的应用程序。该方法训练是在单个 RTX4090 GPU 上进行的，针对每个动作类别使用一个小型、高质</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490519&amp;idx=1&amp;sn=79506d6c5873f4175f9df966ed1a80b0&amp;chksm=fda06817e332ac2250802f0818d3a4e66a6ff27eb46e41c260e83e1d4701d4c19184c7021053&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 28 Feb 2025 16:06:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[NVIDIA提出新框架ImageRAG！RAG+AIGC提升图像生成质量！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5JK3j8AP855QOPLGKEpd37E3bPLWmIOj4bSM2oUxbcSEQ3NFVFyqRhEKjhBGvFkPMAwAaMsbszianQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今日论文推荐论文名：ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation论文链接：https://arxiv.</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490519&amp;idx=2&amp;sn=ce3026b5dc7d195a19f51fe317ba1385&amp;chksm=fd147c133a8857326f4b8c8e6df4d5078903fc221cd5359c784e4fc58a5f118b817c03fd35a5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 28 Feb 2025 16:06:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[电商领域利器来了！港大&amp;阿里提出MimicBrush，可模仿参考图进行零样本图像编辑。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enu15BEdxn8DpEdshmGPMicepiaSYu1INiahHv6ZdWxcTRjT1UexEYITfITVC8uhS6hWlib4Wodyfrr3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里和港大提出的MimicBrush可以通过对参考图模仿进行零样本图像编辑。将一张图片的某一部分融合到领一张图片上去。用在电商商品展示上或者单纯的图片编辑和内容迁移很有用。从官方演示来看效果也很好。M</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490519&amp;idx=3&amp;sn=554d23556191e02c38f336b2e29245b2&amp;chksm=fd99c66c9c358101f2bdb8d4e9d28aab08764a11e762a9e442bd5e1d1680274c2ade741e9264&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 28 Feb 2025 16:06:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Meta提出Fast3R！多视角快速3D重建新SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAdQicl6tFbm7xyZxj3Q91AjRBzA4Vrr5253RCiabI6uiaibZMIdTyoq6TZdXe8AotLRlkE2pkJyqGQLfQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名：Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass论文链接：https://arxiv.org/pdf/</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490519&amp;idx=4&amp;sn=a905ede7d9d76c9eb1adde9699ad71ef&amp;chksm=fdb0c30ddfa289234019713b54d6e1b9b4fe9f44d346186c9fe1e1512781ae161dd27d7a2d2c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 28 Feb 2025 16:06:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[理想汽车提出LDGen！颠覆多语言图像生成的革命性突破，美学与精准度的双重飞跃！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5J2tFEUOXbXSxBx5j1nBibbqn6qQJanWaRSGLup9KjBMUuBeGtw9u98Ciby4QTibN7lhYm3tXW8VRYtQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Repres</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490518&amp;idx=1&amp;sn=0d857ab42f69e5602facfafbb7c204a6&amp;chksm=fdb56048253a4a043578daf9389e6cbb4080331c1c96ab2b82fe7346b5fbcb6b3c6a48d7a558&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 27 Feb 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[[ComfyUI]阿里WanX2.1：最强开源视频模型易主！静待社区生态开源直逼闭源，Vbench榜首第一]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/1BRxta5juGR4iczdl1w1JCwWwhHibiaD9umzM0GtLxG8EhR3dUBt02neC4QvYYvoFicjM1FhMAten8AeAnfhcCtHYA/300?wxtype=jpeg&amp;wxfrom=0"/><p> 阿里WanX2.1：文生和图生视频模型ComfyUI体验WanX 2.1简介在昨天的文章（阿里Wan2.1：最强开源视频，本地部署优先体验！Vbench榜首第一，超越Sora&amp;混元&amp;Gen3&amp;Pik</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490518&amp;idx=2&amp;sn=9c99b1e013fc8f80eb22c83791380f17&amp;chksm=fd756053974066717edb6b862bda3c4f25fc0dc9b540dc4da291aff994aeafe6e17d3d19b792&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 27 Feb 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[国科大提出SayAnything！高保真语音驱动说话人视频生成神器！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5K5KErGYAcboNZ5g1ickzvcVkQLCcDMsCQcyM5NbvALsSFic6AIQm1WH4bTtNZ1icfTrh6g2j7b1kRhQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名：SayAnything: Audio-Driven Lip Synchronization with Conditional Video Diffusion论文链接：https://arxiv</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490518&amp;idx=3&amp;sn=08f579474fa9039499e89b2edb4eef86&amp;chksm=fd335b9684d893bd98ef6e53003dcfa7f998338818814130cf59e08b6ec23795b01114a3fe72&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 27 Feb 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[深度长文｜一文读懂多模态大模型：强化学习技术全面解读 SFT、RLHF、RLAIF、DPO]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/B1OJ3jLyfic5T2TV2orhwficRDibGUTU3xn1xoCPq7bx5xr0CM1pbwH7Q9gYicdPZRiaIOKTsVKHicyuQGUA524jqibYQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>点击下方卡片，关注「魔方AI空间」公众号本文从强化学习如何增强大语言模型（LLMs）的视角，进行系统性全面解读，涵盖强化学习的基础知识、流行的RL增强LLMs、基于奖励模型的RL技术（RLHF和RLA</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490518&amp;idx=4&amp;sn=c1b8d72b7aeed8f82ef479147ca2c62b&amp;chksm=fd9877c87e2dd912188fcd009b0994a3529a6a99f84b7d11506f4c693dcd7eff289a31f90974&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 27 Feb 2025 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>