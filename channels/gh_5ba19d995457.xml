<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    

























    <item>
      <title><![CDATA[英伟达开源4K图像生成模型Sana，可在16G显存电脑部署，支持ComfyUI和LoRA训练。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek6Zafxy9AicSRodyIcwlSHNT9mr6NOzfTpJPhveE41Xmh1RVMhhibAgXAt3qSb6eFx0HfpEYX74THA/640?wxtype=jpeg&amp;wxfrom=0"/><p>英伟达开源了一个可以直接生成 4K 图片的模型 Sana。 Sana-0.6B 可以在 16GB 的笔记本电脑 GPU 上部署。生成 1024 × 1024 分辨率的图像只需不到 1 秒钟。官方已经支</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490058&amp;idx=1&amp;sn=f671f319d7fd3f82c07be81aa55e5d2e&amp;chksm=fd2fa9352628b4ff1d6bf2f915de5fba7754aa2146ab0844f895cf296f9e8054e23e35342a70&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 27 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[复旦&amp;字节提出layout-to-image新范式，支持基于布局的MM-DiT架构下可控图像生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekXX8zYF4UxzjmCibmVsNeNfHdzvia0ykHy5vQljhxHZhBKib0DHCIdedbOAMsic8KZ423vtGia19o4Wow/300?wxtype=jpeg&amp;wxfrom=0"/><p>本篇分享论文CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490058&amp;idx=2&amp;sn=781b6dc047987911723ab4d92e23378b&amp;chksm=fd8118d00f71769b5a87ce76c4599f7c3d0195c96a246327b08ec706f264bdfc71f7019ce276&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 27 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[SHMT：通过潜在扩散模型进行自监督分层化妆转移（阿里&amp;武汉理工）]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsjQl0sGle0TkYDcmMMuGmbtLXibkDVicOAa1tpYmub1EJgQJfZ41lm6WQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>当前的妆容转移技术面临两个主要挑战：缺乏成对数据，导致模型训练依赖于低质量的伪配对数据，从而影响妆容的真实感；不同妆容风格对面部的影响各异，现有方法难以有效处理这种多样性。今天给大家介绍的方法是由阿里</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490058&amp;idx=3&amp;sn=04b9f8f32aa3b7145c1821b47d4ea26b&amp;chksm=fd8a91cc96f2a2e1f3d41311d6c21bd95ff9b7cd6d1a34f76e7e1317e3ab7127dc8ba3c8fafa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 27 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[提出街景定位大模型AddressCLIP：一张图实现街道级精度定位！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eldKGCwibmhq5RSxC5rV78dDcVpQDWZ2qUibtJW2qRF8ehlmicnuSw3n5MdOVQ0NTovfOnPib1RNDwBibQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>中科院自动化所和阿里云一起推出了街景定位大模型AddressCLIP，只要一张照片就能实现街道级精度的定位。比如给模型看一张北京南锣鼓巷的街景之后，它直接给出了具体的拍摄位置，并列举了附近的多个候选地</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490058&amp;idx=4&amp;sn=187ced6c7d7f40ec09a755763c6b9670&amp;chksm=fdbb265fbde436ab4fc4e9a040a1e60d68e1f8fe5cb9ed097502d404f8fbc9ee9218b3981df6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 27 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[单张照片生成3D头部模型！Adobe提出FaceLift，从单一人脸图像重建360度头部模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4qXFbiaEqoEPejUQNwuqLOrpIE3WmoKJBxjrMnCoHDn3huArYyaCa7Ew/640?wxtype=jpeg&amp;wxfrom=0"/><p>FaceLift是Adobe和加州大学默塞德分校推出的单图像到3D头部模型的转换技术,能从单一的人脸图像中重建出360度的头部模型。FaceLift基于两阶段的流程实现:基于扩散的多视图生成模型从单张</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490057&amp;idx=1&amp;sn=abf129eb3d79e47e5868f82fb2c27131&amp;chksm=fd9941c812f75472c02b9919c97a24ac51e9f744f06c2d95ff899d806b3168f8ee11988e2aea&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一键试衣or一键脱衣？TryOffAnyone：从人像输入中生成高质量平铺服装。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUnCMiaBjZZszIFmMq4b6eoaD7GTE6xt5Kkbz15pCGnYFBHiaXPN7qYcV79yBTU689jcMRI5dJd0g/300?wxtype=jpeg&amp;wxfrom=0"/><p>TryOffAnyone 是一种新颖的单阶段框架，旨在从穿着衣服的人的输入图像和覆盖服装区域的相应服装掩码合成高质量的平铺布料图像。在 VITON-HD 等基准数据集上实现了最先进的性能。该方法在为全</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490057&amp;idx=2&amp;sn=bc8fef13bc2549981e44048cb0ce7664&amp;chksm=fdc3bef4062c2ca60abc1de2e59d7633505ea3632ac5e653b4afaee22b45b08a5c37c1a8f94b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adobe发布TurboEdit：可以通过文本来编辑图像，编辑时间<0.5秒！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elKcprhHqENugIHSUTwb3EOiaaqictMa8fmmNEDqsoISMhGDZH4oZmh7vtMn5sov6khPdhIypPkhDZQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍Adobe研究院新的研究TurboEdit，可以通过文本来编辑图像，通过一句话就能改变图像中的头发颜色、衣服、帽子、围巾等等。而且编辑飞快，<0.5秒。简直是图像编辑的利器。相关链接项目</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490057&amp;idx=3&amp;sn=8c33be1915dfe03902e10b066e2fc7c6&amp;chksm=fdbf1488eda57bc8c82735506307c1fbdb6ce67b2ad1e96e7c7c2ee7ce0e7550abae3df32d22&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[浙大 | 腾讯 | 华为提出视频生成框架VideoMaker，可由参考图实现Zero-shot定制化视频生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekaVfDRjALdOCj5889F1MpALuqg2wbFklkt9TIVHLSyQfTQ65do3Pe4Szhc0sWs0dMVTLfiavGbvRQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>浙大联合腾讯和华为提出了一种新的定制化视频生成框架——VideoMaker，利用VDM的内在能力，实现高质量的zero-shot定制化视频生成。该方法通过直接输入参考图像到VDM中，利用其固有的特征提</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490056&amp;idx=1&amp;sn=515a850881d7ef3822914ea6560d3fe1&amp;chksm=fdca9bd4cd455e689d4221795ed3a48e51a7a8bd31df2e309570e49c5780f65d6bf29cb81bdd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 00:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一键脱衣？TryOffAnyone：从人像输入中生成高质量平铺服装。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUnCMiaBjZZszIFmMq4b6eoaD7GTE6xt5Kkbz15pCGnYFBHiaXPN7qYcV79yBTU689jcMRI5dJd0g/300?wxtype=jpeg&amp;wxfrom=0"/><p>TryOffAnyone 是一种新颖的单阶段框架，旨在从穿着衣服的人的输入图像和覆盖服装区域的相应服装掩码合成高质量的平铺布料图像。在 VITON-HD 等基准数据集上实现了最先进的性能。该方法在为全</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490056&amp;idx=2&amp;sn=c2ea44a0f6e1f07f65487d61e6412a7d&amp;chksm=fd112f0f1371c0eaf18562e06b386ac72123601926151ffbf5be041b5380504cf2629aaaf778&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 00:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekB7CXUYR45xqh1P2Q9zWuxgmicJiaO6JPkkhoaibkSARt6qftWXI9ofZjt9NK9vuibg0UrfhA2kTPRaQ/300?wxtype=jpeg&amp;wxfrom=0"/><p> 腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT今天介绍的文章来自公众号粉丝投稿，腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT，给定一个人像图像和一个衣物图像，就可以生成一个</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490056&amp;idx=3&amp;sn=c251f6741179b8376358db838c84bd6b&amp;chksm=fd4e02ee03cd7d095d3a8e55f9b6f2b344a8493200479a2c7d298e8bf5cab6b1c928d230c38b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 00:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Stability AI 联合UIUC提出单视图3D重建方法SPAR3D，0.7秒完成重建并支持交互式用户编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4zxt8tZpVIWNwGIWEeh5iaYqC8uJSSWGn5hv37BppQMhwNVjgbEMmOgw/640?wxtype=jpeg&amp;wxfrom=0"/><p>Stability AI 联合 UIUC 提出一种简单而有效的单视图 3D 重建方法 SPAR3D，这是一款最先进的 3D 重建器，可以从单视图图像重建高质量的 3D 网格。SPAR3D 的重建速度很</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=1&amp;sn=5b72ce7f8c6cf6dfe853a76491eee0f6&amp;chksm=fd22c717e1473e7228e5c739a177c2666bce39b6a4d4da032608ccd5a6c4c8ef5d8bf1f49e72&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[北大提出定制化漫画生成新框架DiffSensei，可生成具有动态多角色控制的漫画图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgKtXcy3S4XBxj4sIBiacegBSAicARmN6YDuAjO6tUqgQ6TNNE8CbF3pFw/300?wxtype=jpeg&amp;wxfrom=0"/><p>由北京大学、上海人工智能实验室、南洋理工大学提出了一种新框架DiffSensei可以实现定制化漫画生成，解决现有方法在多角色场景中对角色外观和互动控制不足的问题。DiffSensei结合了基于扩散的图</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=2&amp;sn=91ab2fe971de657581119fbf62674822&amp;chksm=fdacb1e433a0cd2848fe0a808085f5ac9e709fc2854da5931ec18d031054f488c631f6353854&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[DeepSeek-V3 正式发布，已在网页端和 API 全面上线，性能领先，速度飞跃。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsVQ8iaD3eY0RibaYkYGTf6mgibWMibTiaiccjeVMM6rnIOdfQ4sLtwbuJJ1iaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>DeepSeek-V3 在推理速度上相较历史模型有了大幅提升。在目前大模型主流榜单中，DeepSeek-V3 在开源模型中位列榜首，与世界上最先进的闭源模型不分伯仲。unsetunset简介unset</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=3&amp;sn=03adf783346dd287860d17558659a1da&amp;chksm=fdb7b81e6fc16379e42c4f653aee55baa5f32697384b4c16ce412e3f76bb95d4bc23a620c02d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=4&amp;sn=ec1a6b75781ab0ae69edcebe84350dfc&amp;chksm=fd98b3a29816bca11506813a9fd296bd34fb3c13bbc70b066f2b62632f247fe404f9ad23bf8b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里通义实验室提出AnyStory：开启个性化文本到图像生成的新篇章！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2empB05GMsROweibZLQdTRdryE5VRCZQVY9U7yt38iaOH1WTKFoooIOYJGiaeHWGaeL3ZuZ4sUW8zKnjg/640?wxtype=jpeg&amp;wxfrom=0"/><p>在这个数字化时代，生成式AI技术正以前所未有的速度改变着我们的创作方式。近期，阿里通义实验室发表了一篇题为《AnyStory: Towards Unified Single and Multi-Sub</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489981&amp;idx=1&amp;sn=0075a635bd4b70f8897e039ae41f4edd&amp;chksm=fdbca53cb21f66e8c140636186df570a919d004fb7dad68805c216d7df9401cf1b535bb5e0cd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 23 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LineArt：无需训练的高质量设计绘图生成方法，可保留结构准确性并生成高保真外观。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emtHS7t5ic0uQWb1AOhKNDRVgJ9ReODibzN7pbSG8HiaFPKVEMD45h5nQHic2uaYSruibSNfMdcXWxiclIA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一种无需训练的基于扩散模型的高质量设计绘图外观迁移方法LineArt，该方法可以将复杂外观转移到详细设计图上的框架，可促进设计和艺术创作。现有的图像生成技术在细节保留和风格样式一致性方面</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489981&amp;idx=2&amp;sn=96eadb560acd5e28c21979f1e786ec60&amp;chksm=fdba8083198186e9dc773f5cb71a677e6c9bebf19ae4b4d32caea3c4cb225e55aa3950d2e51b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 23 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Snap | 港科大提出端侧文生图模型SnapGen，参数仅SD十分之一，1.4秒内生成1024分辨率图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDuiaVeqQlxjbAyWQVBK9zTWxMkxd3V2yvSqRYageKwuRPwuQj7g2Iog/300?wxtype=jpeg&amp;wxfrom=0"/><p>这项工作提出了一种新颖且高效的 T2I 模型SnapGen，SnapGen 是第一个可以在1.4秒内在移动设备上合成高分辨率图像（1024x1024 ） 的图像生成模型（379M ） ，并在 GenE</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489981&amp;idx=3&amp;sn=9f8f2602584e2a3efdfd94bc889c1009&amp;chksm=fda8ecb605ed337ae24c0a3a01d0b24fd535ed81eb62e4c2b1e0cfb3d9bf8e0f8c5c51eb818d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 23 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NVIDIA提出虚拟试衣新方法EARSB，让时尚与科技完美融合！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2empB05GMsROweibZLQdTRdryhxs0ChYaSb8Q7MXgpODuC675ClEZrLFicPt5eWjzKjxOHWcsP2ic7rBA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在数字化浪潮席卷全球的今天，科技正以前所未有的方式融入我们的生活，包括我们追求时尚的方式。想象一下，无需亲临实体店，只需轻点屏幕，就能轻松试穿心仪的衣物，这不再是遥不可及的梦想。NVIDIA联合波士顿</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489980&amp;idx=1&amp;sn=e5644f7af10968f5b61df062c593f41b&amp;chksm=fdcfcea6689a2282559a810ef16a95cda0124d7b8d32316260107809ec3403ba7823608111c7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 22 Jan 2025 16:06:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[中科大提出新视频流制作动画解决方案RAIN，可实现真人表情移植和动漫实时动画。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekaVfDRjALdOCj5889F1MpAKe7VdTw8TPT3jvjm7A5B4CnAk0BFdqibXzkZls0gnHjLqpeExdtPhpw/300?wxtype=jpeg&amp;wxfrom=0"/><p>中科大提出了一种新的视频流制作动画解决方案RAIN，能够使用单个RTX 4090 GPU 实时低延迟地为无限视频流制作动画。RAIN 的核心思想是有效地计算不同噪声水平和长时间间隔的帧标记注意力，同时</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489980&amp;idx=2&amp;sn=81ee380588888072855c759aeb304672&amp;chksm=fdd8deb7ca6ae1c3a8e752fdb7c425f438867b074eceddaf70aaf8009764afe1499c36077906&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 22 Jan 2025 16:06:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[引领图像编辑领域的新潮流！Edicho：实现跨图像一致编辑的新方法(港科&amp;蚂蚁&amp;斯坦福)]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicEKO56ibffMsiau7qrg2gcpibqTFwwB20Tz8hX6wXGcTC684He5MiazvvzA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在图像处理领域，如何实现跨图像的一致编辑一直是技术挑战。传统方法往往局限于单张图像的编辑，难以保证多张图像间编辑效果的一致性。香港科技大学、蚂蚁集团、斯坦福大学和香港中文大学联合提出Edicho，这一</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489980&amp;idx=3&amp;sn=eb9ea7d88bf97a720a844ec403f8d9a1&amp;chksm=fd4872d652b148f3a01d4c8a01137f892c1849ec2aebeb00ebde9310fcac1e05071713852744&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 22 Jan 2025 16:06:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[解决文生图质量和美学问题，字节跳动提出VMix：多维度美学控制方法，一键提升图像美学。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcic6UBBbFKhnQGPNFnt0uNP0icQj44gWuVMkaK2nsqNia2kicW5icETqKKVMA/640?wxtype=jpeg&amp;wxfrom=0"/><p>为了解决扩散模型在文生图的质量和美学问题，字节跳动&amp;中科大研究团队提出VMix美学条件注入方法，通过将抽象的图像美感拆分成不同维度的美学向量引入扩散模型，从而实现细粒度美学图像生成。论文基于提出的方法</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=1&amp;sn=ae5928d9f36e2133a8fbb5688b2bfc92&amp;chksm=fd98b7372d9f557c10bd9f7b0bbc3b9a88d9e4decba29ba49f776126836c75b897a68574251f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图像超分辨新SOTA！南洋理工提出InvSR,利用大模型图像先验提高SR性能, 登上Huggingface热门项目。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnndGUoqrp8dTsfo3BKVIVGVNf5sWoXGauJCgAEaaCQm9Qb7QfuM34qZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>南洋理工大学的研究者们提出了一种基于扩散反演的新型图像超分辨率 (SR) 技术，可以利用大型预训练扩散模型中蕴含的丰富图像先验来提高 SR 性能。该方法的核心是一个深度噪声预测器，用于估计前向扩散过程</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=2&amp;sn=5bdb432efc836f876133e0e0bae8b7e1&amp;chksm=fd7d483c20654bbcbc1411092a35c0cf3d267b20147fafd7584c2ec4ae3543f1b15307885901&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[小米SU7璀璨洋红限定色360°全景图首次曝光？TRELLIS给你答案，实现可扩展多功能3D生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsrcRtOibx92ZhEt1Z0UdQ7JsQibr17Y0WaD8O08DMM3XIor43XOZVMvXg/300?wxtype=jpeg&amp;wxfrom=0"/><p>清华大学、中国科学技术大学、微软研究院联合提出T RELLIS，这是一个大型 3D 资产生成模型，可根据文本或图像提示（使用 GPT-4o 和 DALL-E3）以各种格式生成高质量的 3D 资产，可在</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=3&amp;sn=e151853f9bd46153b897b4f0e2401ca4&amp;chksm=fdc19fd286937aae1c4f8a3bf173aadfb77794b141bf0d4ea4c3dd8877cded8585316d16e77e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[你要跳舞么？复旦&amp;微软提出StableAnimator：可实现高质量和高保真的ID一致性人类视频生成]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEg0pWxDqMplvkr6CkMHxsZoRegYlaQmYz6ah0rQewI1UFbTMjpYhWh4Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>由复旦、微软、虎牙、CMU的研究团队提出的StableAnimator框架，实现了高质量和高保真的ID一致性人类视频生成。StableAnimator 生成的姿势驱动的人体图像动画展示了其合成高保真和</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=4&amp;sn=5824b4904804b29870c71c2ac413eaf1&amp;chksm=fd7aca3517eaf161b05840738e5c62bc3bdfb2c03399c1a5fd9b44c3eed4cd517cf78f17db37&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
