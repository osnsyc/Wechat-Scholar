<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    
























    <item>
      <title><![CDATA[解决文生图质量和美学问题，字节跳动提出VMix：多维度美学控制方法，一键提升图像美学。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcic6UBBbFKhnQGPNFnt0uNP0icQj44gWuVMkaK2nsqNia2kicW5icETqKKVMA/640?wxtype=jpeg&amp;wxfrom=0"/><p>为了解决扩散模型在文生图的质量和美学问题，字节跳动&amp;中科大研究团队提出VMix美学条件注入方法，通过将抽象的图像美感拆分成不同维度的美学向量引入扩散模型，从而实现细粒度美学图像生成。论文基于提出的方法</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=1&amp;sn=ae5928d9f36e2133a8fbb5688b2bfc92&amp;chksm=fd98b7372d9f557c10bd9f7b0bbc3b9a88d9e4decba29ba49f776126836c75b897a68574251f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[图像超分辨新SOTA！南洋理工提出InvSR,利用大模型图像先验提高SR性能, 登上Huggingface热门项目。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnndGUoqrp8dTsfo3BKVIVGVNf5sWoXGauJCgAEaaCQm9Qb7QfuM34qZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>南洋理工大学的研究者们提出了一种基于扩散反演的新型图像超分辨率 (SR) 技术，可以利用大型预训练扩散模型中蕴含的丰富图像先验来提高 SR 性能。该方法的核心是一个深度噪声预测器，用于估计前向扩散过程</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=2&amp;sn=5bdb432efc836f876133e0e0bae8b7e1&amp;chksm=fd7d483c20654bbcbc1411092a35c0cf3d267b20147fafd7584c2ec4ae3543f1b15307885901&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[小米SU7璀璨洋红限定色360°全景图首次曝光？TRELLIS给你答案，实现可扩展多功能3D生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsrcRtOibx92ZhEt1Z0UdQ7JsQibr17Y0WaD8O08DMM3XIor43XOZVMvXg/300?wxtype=jpeg&amp;wxfrom=0"/><p>清华大学、中国科学技术大学、微软研究院联合提出T RELLIS，这是一个大型 3D 资产生成模型，可根据文本或图像提示（使用 GPT-4o 和 DALL-E3）以各种格式生成高质量的 3D 资产，可在</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=3&amp;sn=e151853f9bd46153b897b4f0e2401ca4&amp;chksm=fdc19fd286937aae1c4f8a3bf173aadfb77794b141bf0d4ea4c3dd8877cded8585316d16e77e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[你要跳舞么？复旦&amp;微软提出StableAnimator：可实现高质量和高保真的ID一致性人类视频生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEg0pWxDqMplvkr6CkMHxsZoRegYlaQmYz6ah0rQewI1UFbTMjpYhWh4Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>由复旦、微软、虎牙、CMU的研究团队提出的StableAnimator框架，实现了高质量和高保真的ID一致性人类视频生成。StableAnimator 生成的姿势驱动的人体图像动画展示了其合成高保真和</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=4&amp;sn=5824b4904804b29870c71c2ac413eaf1&amp;chksm=fd7aca3517eaf161b05840738e5c62bc3bdfb2c03399c1a5fd9b44c3eed4cd517cf78f17db37&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[CVPR 2024 Spotlight | 解锁图像编辑新境界, 北大、腾讯提出DiffEditor，让精细编辑更简单！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emtHS7t5ic0uQWb1AOhKNDRVPEH3Xqks3oDpG6kIgEczYPnVPsI98I9LqibzYz8fUXeJrNILLDFicB0A/640?wxtype=jpeg&amp;wxfrom=0"/><p>在图像生成领域，大型文本到图像（T2I）扩散模型近年来取得了革命性的突破。然而，将这些强大的生成能力转化为精细的图像编辑任务，仍面临诸多挑战。CVPR 2024, 来自北京大学深圳研究生院与腾讯PCG</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=1&amp;sn=78e02ad557acbddd73965e30230fe090&amp;chksm=fdbe7557a5166b386b9963284767c09132e19c70d858552f3ead19a6560e9aefc86fe035e88b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adobe与MIT推出自回归实时视频生成技术CausVid。AI可以边生成视频边实时播放！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDwNBzSGR9jz4olCuaibHPBcISVDNbZVjdKgcIl4GiaczxalR4zb0LKJQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>传统的双向扩散模型（顶部）可提供高质量的输出，但存在显著的延迟，需要 219 秒才能生成 128 帧的视频。用户必须等待整个序列完成才能查看任何结果。相比之下CausVid将双向扩散模型提炼为几步自回</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=2&amp;sn=c6502c899b6e3f852dfd2041911215b9&amp;chksm=fd54050113a0b64568ab964cb7a0254d15670d27d222e071ba613a4e6d1dea7fec1a8fb80b56&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[多模态图像生成模型Qwen2vl-Flux，利用Qwen2VL视觉语言能力增强FLUX，可集成ControlNet]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYHx9RLCm1u1zJr61WGBPZZicviaGPyXN8y5ZTaZE9jpPcdNSX1nmUlib5g/300?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen2vl-Flux 是一种先进的多模态图像生成模型，它利用 Qwen2VL 的视觉语言理解能力增强了 FLUX。该模型擅长根据文本提示和视觉参考生成高质量图像，提供卓越的多模态理解和控制。让 F</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=3&amp;sn=27a5753da03f6a835735475d04049920&amp;chksm=fd110660ccaf10ce849fe434a13dc1af783f42a485f7a20baaef50b5258309848cfe0c36dc61&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Google发布新AI工具Whisk：使用图像提示代替文本，快速完成视觉构思。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2end3mWUdomxapVIqKPBfrWChHLuwfMCvRvq1l8Kl6qfOOlq9YUxzPEdzMibDSUo0R5owCSicTJLumBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>Google发布了新的AI工具Whisk，Whisk 是 Google Labs 的一项新实验，可使用图像进行快速而有趣的创作过程。Whisk不会生成带有长篇详细文本提示的图像，而是使用图像进行提示。</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=4&amp;sn=a9f392d276409795dbe3f70c902346be&amp;chksm=fd8cb3e29c053652af42eda3f111fb346358a870517a14972adf84936712f347e75edb5b911c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Snap | 港科大提出端侧文生图模型SnapGen，参数仅SD十分之一，1.4秒内生成1024分辨率图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDuiaVeqQlxjbAyWQVBK9zTWxMkxd3V2yvSqRYageKwuRPwuQj7g2Iog/640?wxtype=jpeg&amp;wxfrom=0"/><p>这项工作提出了一种新颖且高效的 T2I 模型SnapGen，SnapGen 是第一个可以在1.4秒内在移动设备上合成高分辨率图像（1024x1024 ） 的图像生成模型（379M ） ，并在 GenE</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=1&amp;sn=1432436e0f71b9b7e486560668eeed7a&amp;chksm=fd9bc6a0e991e310061f042c8137126e01921ff048fc146bdb79d645fa89ad4db423f53e59fc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OminiControl：一个新的FLUX通用控制模型，单个模型实现图像主题控制和深度控制。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYEn3PFyv0qTxQYEgq8VntmUj91vEEYPJjMADiamfkH94icSBs7fF1Tn1A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中和大家介绍过Flux团队开源了一系列工具套件，感兴趣的小伙伴可以点击下面链接阅读~AI图像编辑重大升级！FLUX.1 Tools发布，为创作者提供了更强大的控制能力。OminiContro</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=2&amp;sn=5f893661ace33b3910b950d9030b7a1c&amp;chksm=fdaad180939f5106e2305580c604ad3fa7e7f1ee76410babf76cc5e46cf65de4c08bb3da09a7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里发布新ID保持项目EcomID, 可从单个ID参考图像生成定制的保ID图像，ComfyUI可使用。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwazEmicf1F7ZjrxQSj4JNP3x3icluxM84Et2UYGdsdxfDOXnd9OlZYFCwg/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里妈妈发布了一个新的ID保持项目EcomID，旨在从单个ID参考图像生成定制的保ID图像，优势在于很强的语义一致性，同时受人脸关键点控制。EcomID 方法结合了 PuLID 和 InstantID</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=3&amp;sn=2a131da1305f4c755047f499f0b2fb72&amp;chksm=fd9be0eebc204661357981b7050c377b021814104b3d4c883a27a20527b26204b3694c9f62ed&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[港大和字节提出长视频生成模型Loong，可生成一分钟具有一致外观、动态和场景过渡的视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eny4Iriba5NSXkHvLxicLITJD5gFTkWLoBqMSNfUicQxXgibIh9n6vokK5ia5EOh7ZDJLVHGEsbaLz86XA/300?wxtype=jpeg&amp;wxfrom=0"/><p>HKU, ByteDance｜⭐️港大和字节联合提出长视频生成模型Loong，该模型可以生成外观一致、运动动态大、场景过渡自然的分钟级长视频。选择以统一的顺序对文本标记和视频标记进行建模，并使用渐进式</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=4&amp;sn=a3dc9baea4db7b2ce69c09edf47daebe&amp;chksm=fdce4e89e36260261732b1375680ed14de82d2c716f27c5952d048065d3058b47cb8a2e759f1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Github热门机器学习笔记:「从零构建大型语言模型」]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emtHS7t5ic0uQWb1AOhKNDRVQe1ibV5hcbvDj7icpDN1BtRicibpaHbuszyA75wydLlzCvmBKSLia5XJSLQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家推荐一份GitHub上很火的机器学习学习笔记《从零构建大型语言模型》，目前已经收获1.4K stars，，这份笔记完美展示了从零构建LLM的技术路线图，既有理论深度，又包含实践要点。每个核心</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489955&amp;idx=1&amp;sn=53339b99584662616f0959a58cfa0826&amp;chksm=fd13928f93ecb22328315109c5122112f3f0833c72c28c0bef286629b12231eef78a5f1bb74d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 17 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里发布新ID保持项目EcomID, 可从单个ID参考图像生成定制的保ID图像，ComfyUI可使用。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwazEmicf1F7ZjrxQSj4JNP3x3icluxM84Et2UYGdsdxfDOXnd9OlZYFCwg/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里妈妈发布了一个新的ID保持项目EcomID，旨在从单个ID参考图像生成定制的保ID图像，优势在于很强的语义一致性，同时受人脸关键点控制。EcomID 方法结合了 PuLID 和 InstantID</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489955&amp;idx=2&amp;sn=a5e8d093a053334b4260f36de4dbbb5e&amp;chksm=fd179381eb1331749a1f8d687b7629f5efad372be3d703e037180fe1de802588fc987c7ee8ce&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 17 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ComfyUI服装设计，一个工作流搞定！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/ACyQFjNqyE6jwFqazZJ5fESF3UlicH6GlH8oBF0nugTB4K9GZwyblhme7GVxoiaXUrN6Ce0c5ibFPOkwd5AttyiaCw/300?wxtype=jpeg&amp;wxfrom=0"/><p>ComfyUI：为你的图像创作赋能的强大工具在AI技术迅猛发展的今天，Stable Diffusion成为了图像生成领域中的一颗明星，而基于此开发的ComfyUI更是为用户提供了一个强大而直观的工具，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489955&amp;idx=3&amp;sn=a98d0e628800bb695eb296da98f95ef2&amp;chksm=fd7693c1962a51b2ea5fc4b9d061dcac402bf1341fe3a0699247908b91cae002efc070b30c3f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 17 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[引领图像编辑领域的新潮流！Edicho：实现跨图像一致编辑的新方法(港科&amp;蚂蚁&amp;斯坦福)]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicEKO56ibffMsiau7qrg2gcpibqTFwwB20Tz8hX6wXGcTC684He5MiazvvzA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在图像处理领域，如何实现跨图像的一致编辑一直是技术挑战。传统方法往往局限于单张图像的编辑，难以保证多张图像间编辑效果的一致性。香港科技大学、蚂蚁集团、斯坦福大学和香港中文大学联合提出Edicho，这一</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=1&amp;sn=8f3541833f1b3a1ab71d132b9ae84666&amp;chksm=fd42cdd0435be406b12120fb005b4c4372e9c0c9dc338439136155dc6b190b9bf016091492f3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图像编辑大一统？多功能图像编辑框架Dedit:可基于图像、文本和掩码进行图像编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekDYMeOJw6PMrPrgUmBfVvIibC8Suae7poAtMSSVAkicNMibK5CyJB4RLSAKFiajeuqXiaiaib0vMibRiaSKCQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个基于图像和文本的编辑的框架D-Edit，它是第一个可以通过掩码编辑实现图像编辑的项目，近期已经在HuggingFace开放使用，并一度冲到了热门项目Top5。使用 D-Edit 的编</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=2&amp;sn=21834e17f83cf54755f4fe2b99836471&amp;chksm=fd51575dfac0bef5eadf6ca456bcf7fbfa3693f52424e4337ed0c4c56ce40fe82a1f5cab5119&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[扩散模型 vs GAN，谁将主宰文生图的未来？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/V4HEViaySCn8Onq8lDttiaFTpUiabu4PiassM06oMOYp7S6fggSCrREIZ3IoecSjrFkOF5jdiaPODhTx0ALBn3icp8Zw/300?wxtype=jpeg&amp;wxfrom=0"/><p>点击蓝字 关注我们导读你知道，就是那种能根据你的文字描述，一键生成图片的神奇技术。想象一下，你只需动动手指，输入一段文字，就能得到一张与之匹配的图片，是不是很酷？从最早的基于GAN（生成对抗网络）的技</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=3&amp;sn=6461fb360bdc70552893d69b1260caf0&amp;chksm=fd41c645caa30cdbe04e69fdb1d738877236baa432454be61ed3b7a23ec777a8f3e93e15345a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[长篇故事可视化方法Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=4&amp;sn=25eaecb6ed069f0bc5b3a09873f7d441&amp;chksm=fd735b674728b3e39d42ad141a023a6429bdebef7d1773a6852d410430d7c958b942d115abd5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[中科大提出新视频流制作动画解决方案RAIN，可实现真人表情移植和动漫实时动画。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekaVfDRjALdOCj5889F1MpAKe7VdTw8TPT3jvjm7A5B4CnAk0BFdqibXzkZls0gnHjLqpeExdtPhpw/640?wxtype=jpeg&amp;wxfrom=0"/><p>中科大提出了一种新的视频流制作动画解决方案RAIN，能够使用单个RTX 4090 GPU 实时低延迟地为无限视频流制作动画。RAIN 的核心思想是有效地计算不同噪声水平和长时间间隔的帧标记注意力，同时</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=1&amp;sn=ad177e495ad3e2052d95dbe49a68ce5b&amp;chksm=fd643ea4d20d09f8ca2c1910bdd5e4a3cfc2a6a287761384b05e51950c1277878f85052cb161&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[理想汽车提出3DRealCar：首个大规模3D真实汽车数据集!]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eldrtUtjxDXrVGVcbe8sY5fn25qjoY7k2Bsz6XJV8GAUvw3FQiaWMcbeiadZBbw9ZE2f1znye2jstGw/300?wxtype=jpeg&amp;wxfrom=0"/><p>理想提出3DRealCar，这是第一个大规模 3D 实车数据集，包含 2500 辆在真实场景中拍摄的汽车。3DRealCar的目标是可以成为促进汽车相关任务的宝贵资源。3DRealcar包含各种颜色、</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=2&amp;sn=2c49fe640f47cd28e9c19753d4fc21e5&amp;chksm=fd179b43565b066cdd786bfcea59e1768886f733e4cb57ba7d5e6b1857ec3c46e7b20b1fe2c6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[UIUC提出InstructG2I：从多模态属性图合成图像​，结合文本和图信息生成内容更丰富有趣！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZqBubdMgx3yBMfDK8JGL4YYX3hw4kJVRCHjFaqvVYYc7nEPXjibpCEug/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的这项工作是伊利诺伊大学厄巴纳-香槟分校的研究者们提出的一个新任务 Graph2Image，其特点是通过调节图信息来合成图像，并引入了一种名为InstructG2I的新型图调节扩散模型来</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=3&amp;sn=04ea94ae2fb61f64de0600a942236967&amp;chksm=fdc660d4cc39437f66c5fa2a96ccd4b26b472eb21bf811ccf8010a8612e5d12eec6af0d81fc3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[SD和Sora们背后的关键技术！一文搞懂所有 VAE 模型（4个AE+12个VAE原理汇总）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icojhGksxPcuzzcs6IGgx9ulYoFvA0ibAbJu9UfI318Eq0Y8x4wk2aoFzldXC9DKiclmSIkGiclenOCY2g/300?wxtype=jpeg&amp;wxfrom=0"/><p> 点击下方卡片，关注“AIGC Studio”随着Stable Diffusion和Sora等技术在生成图像和视频的质量与帧率上取得显著提升，能够在一个低维度的压缩空间进行计算变得越发重要。这种方法不</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=4&amp;sn=ef3daf262bc2df101417b200b505e198&amp;chksm=fd2f5a19b6f43147db8be066445396516a176e6a9d7284afcf5956c730473c668c1b3e39bbdd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
