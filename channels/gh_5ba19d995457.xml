<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[一句话开启高效图像编辑新时代！TeleAI SmartFreeEdit，打造图像编辑新方案，解决推理指令与分割难题。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elKibiah2ldIaVhKqwicV967dTrx4b0lHrzlxzbCo5bstbMH7ERVSrgFAwvtWptIDnolIg4HTBCicomtw/640?wxtype=jpeg&amp;wxfrom=0"/><p>TeleAI 推出了一个图像理解编辑修复模型 SmartFreeEdit，用来解决图像编辑中推理指令和分割的挑战，从而提升 AI 编辑的实用性。该方法可以有效地处理一些语义编辑操作，包括添加、移除、更</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494076&amp;idx=1&amp;sn=3a2ffe969eea7afc02b2f07198fb3b0c&amp;chksm=fdc155f8fa74a5d8d61ffc7bcfffe1f173fc8bf8979ff04175df45de50371b2ac7b75780f375&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 12 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[LightRAG：颠覆传统AI问答，一张“知识网”让大模型真正开窍！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/CibEZ9gjHpIoKghGBpvdlrGNMQe58LqzvwhL0dbqckzE76dLAvn3Dke60UcVLuSc0LPCD5hAicc6gnAXYGUK9Edw/300?wxtype=jpeg&amp;wxfrom=0"/><p>还在为AI回答支离破碎而头疼？LightRAG用一张“知识网”让大模型真正理解复杂关系你是否遇到过这样的场景：向企业知识库提问“新能源汽车电池技术路线对供应链的影响”，却得到一堆割裂的电池参数和物流术</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494076&amp;idx=2&amp;sn=a8716122909cbe6c69f83ae6c5ebc470&amp;chksm=fd84457f2bea89286f7934ce365532d3dd0b88800c874097310bb15aa480f3e3a84a0975e462&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 12 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Google DeepMind 发布最强视频生成工具 Veo 3, 可为作品添加音效、环境噪音、对话，文中附体验链接。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcnWs2mR9uePicbSxmgsNGYEOvC44lWnQUfBAMbv2Kgy7vDib4ee4tlF1R091cfagJqdQWc10PdkUA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天跟大家介绍谷歌的视频生成模型 Veo 3，可为作品添加音效、环境噪音甚至对话，所有音频均可原生生成。它还能提供一流的音质，在物理效果、真实感和快速响应方面均表现卓越。相比 Veo2 的改变Veo </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494076&amp;idx=3&amp;sn=6a2e08fd410d4fed83b28c0d13d370b3&amp;chksm=fddfe3bc70777b3c25b56a20e49d521d9cbc4adf34edcae47a6136269837eb4364c7f149dc05&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 12 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[海报设计革命,开源媲美部分商用！港科大&amp;美团等提出PosterCraft：让AI实现「构图自由」]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icojx7obK332sQIu5tPib8HDKTSVyn11kSVibT6WeicxNgpicMIibvCvN1yUSHKhAxtClydJbaPlds1MjvRw/300?wxtype=jpeg&amp;wxfrom=0"/><p>Paper：https://arxiv.org/abs/2506.10741 Daily Paper: https://huggingface.co/papers/2506.10741 Github：</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494076&amp;idx=4&amp;sn=2ac829988f28de7d346aa337a18e04c3&amp;chksm=fd85bdac2a9df76839045dde8929b3d28bc878110a39135a266dd9491b07796acbc6945d5fba&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 12 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[突破高分辨率3D建模算力瓶颈！南大&amp;复旦提出 Direct3D‑S2：8卡即可训练，革新 1024³ 分辨率3D生成格局！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elCicOusPT4UMbWRggQc9icnnibKbtwkfQH4wt4miaD9ltwEVcbK2fR9bxibXxuHKTtr0qEAr49WVVsnag/640?wxtype=jpeg&amp;wxfrom=0"/><p>介绍 在 3D 生成领域，高分辨率建模长期受算力限制，传统方法以符号距离SDF函数等体积表示生成 1024³分辨率 3D 形状，计算与内存压力巨大，成本高昂。而今天给大家介绍的 Direct3D‑S2</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494075&amp;idx=1&amp;sn=147c356db39943cc7fb08a357242e08d&amp;chksm=fd0dcd08de36c9a7a51e3bd58b16e7238105081c501f88ab990bb137a3d65c330eb3ad6ca5cf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 11 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[不是P图！用ComfyUI复原老照片，像素级重生太惊艳了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/ACyQFjNqyE62umia43diaibQHp3ufyS7wiaxIEibtXWGdYtMcl71rnkX5MWXjibQMdn9RUbu6m0NMMfLoryHo7Tqzluw/300?wxtype=jpeg&amp;wxfrom=0"/><p>过去的一张张老照片，承载着无数回忆，也记录着一个时代的光影。但随着时间的流逝，那些泛黄、破损、模糊的老照片正一点点被遗忘。幸运的是，AI图像处理的浪潮正悄然改变这一切。而在这股浪潮中，一个名字正在悄然</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494075&amp;idx=2&amp;sn=c28265ab8a4b4dc5a7167d017884f464&amp;chksm=fd5f516b3b48b87be4f9dd969d1575f54e6e625858c876cdb210d57ac40a8ece741e3750d468&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 11 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[香港中文大学 MMLab 提出文生图模型 T2I - R1，文生图进入 R1 时刻！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enKvWzJ9QLeWgYQiaKmEWxL1XALE57UTLoPLT6xjsxJW5tiaoo8VJdc0HMQQUAGdaNID9L9wkBMspWA/300?wxtype=jpeg&amp;wxfrom=0"/><p>香港中文大学 MMLab 提出了一种基于双层次 CoT 推理框架与强化学习的新型文本生成图像模型 T2I-R1，该模型结合了语义级和 token 级的链式思维（CoT）推理过程，并通过强化学习进行增强</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494075&amp;idx=3&amp;sn=6264bd1bdcf5b4c107eba06604c08d6c&amp;chksm=fd187835686e151b8cf89afe32632b05a5f5078b1ce9f8beae5f5f9086f685c53445f82366ae&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 11 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Apple提出UniGen！多模态理解生成统一xii新架构！CoT - V提升图像生成质量！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5KqpbKjwyf8GDnoGZ1ANRZVHSofem5JIanFIxSibozXUibNxHviaUIPE6FTh1nw9lCf16QMqWDaqf7cg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：UniGen: Enhanced Training&amp;Test-Time Strategies for Unified Multimodal Understanding and </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494075&amp;idx=4&amp;sn=2fe977012559e15117cf37e638495192&amp;chksm=fd74501aff7793888a8fc6a827263500ba8d75a14c2bbc58c3531e7944eaad72bd6ad9c59ea4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 11 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[手机上实时跑3D数字人？阿里开源MNN-TaoAvatar，打造本地离线智能数字人新标杆。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek8O7Mx0HicAQzYnr62ZEJLwjCaTF4Xg9G65OJibjU82ibBkicGN8Zdt93yM9ibvqO1zSU5WvX0ehVP4QA/640?wxtype=jpeg&amp;wxfrom=0"/><p>TaoAvatar 是由阿里巴巴淘天 Meta 技术团队研发的 3D 真人数字人技术，这一技术能在手机或 XR 设备上实现 3D 数字人的实时渲染以及 AI 对话的强大功能，为用户带来逼真的虚拟交互体</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494074&amp;idx=1&amp;sn=95271422a4ac0638179f300d2647b01e&amp;chksm=fdb5ef90e7380d3348a1cf29a189c4388ab52586a87b704ea96971c1fa6b7cabdab23c3e9040&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 10 Jul 2025 16:06:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 字节提出个性化多人图像生成新方法ID-Patch，可生成多人合影、姿势可控。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emCuicERoV3guOMh64VYNrcA6VO1uBfS3aIicTCtKS3eFEBxCVDPwXCyj0Fye0L4toEplkN73YiaibibFw/300?wxtype=jpeg&amp;wxfrom=0"/><p>相信扩散模型（DMs）大家一定都不陌生了，目前已经成为文本生成图像的核心方法，凭借强大的图像生成能力，正重塑艺术创作、广告设计、社交媒体内容生产格局。现在，用一段文字生成个性化头像都不算啥新鲜事儿了。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494074&amp;idx=2&amp;sn=dd17d1ecf5eba14fa77c88ee8bad5b71&amp;chksm=fde17bafd60cf74c1b58238ae475153aecb124c24babc76bfbe1de86e6f282eb3a3fdcd0e864&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 10 Jul 2025 16:06:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[东京大学 | Adobe 提出InstructMove，可通过观察视频中的动作来实现基于指令的图像编辑。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsa4uWnhrMawFt9HHkxP0mNsA8WZRJb5wtxFQzRMjAicAjmryxF8Yliamw/300?wxtype=jpeg&amp;wxfrom=0"/><p>InstructMove是一种基于指令的图像编辑模型，使用多模态 LLM 生成的指令对视频中的帧对进行训练。该模型擅长非刚性编辑，例如调整主体姿势、表情和改变视点，同时保持内容一致性。此外，该方法通过</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494074&amp;idx=3&amp;sn=38203794495b0b81d545166888d06d84&amp;chksm=fd28a56f9498393fe3309b5397157569487156383ec8183decba71bb11027f9e4b8382ce1def&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 10 Jul 2025 16:06:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[图像定制大一统？字节提出DreamO，支持人物生成、 ID保持、虚拟试穿、风格迁移等任务，有效解决多泛化性冲突。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enKvWzJ9QLeWgYQiaKmEWxL1Zrf8JKiaMovMsw9t4ZW4pibcVZlWM8AF7GsajlXAPWl5IJgnfQRpnvoA/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节提出了一个统一的图像定制框架DreamO，支持人物生成、 ID保持、虚拟试穿、风格迁移等多项任务，不仅在广泛的图像定制场景中取得了高质量的结果，而且在适应多条件场景方面也表现出很强的灵活性。现在已</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494074&amp;idx=4&amp;sn=da3f96838bedbd4c24711779e594f19e&amp;chksm=fd548f7d4c09a88ef04dde50786d4416c5e917fc8a53344489f4f0005627bd291711e14aa2f7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 10 Jul 2025 16:06:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里开源 30 亿参数统一模型 Ovis-U1，多模式理解、文生图、图像编辑样样精通，多项学术基准测试领先。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuDmLq7R1rRFldNX6Em3MD3ic6VVyQ7fibbkHSDXdsLJJKPKURibic6bQdSKAoTkLHaP0dHSb0n5P6Zw/640?wxtype=jpeg&amp;wxfrom=0"/><p>Ovis-U1 建立在 Ovis 系列的基础上，是一个拥有 30 亿参数的统一模型，它在一个强大的框架内 无缝集成了多模式理解、文本到图像生成和图像编辑。亮点统一能力：单一模型擅长三大核心任务：理解复</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494061&amp;idx=1&amp;sn=6e63e5a9e52130e0d451caaf7d201703&amp;chksm=fdb9244830660ea611169875bc921b09846518ac4573ac7472ea32484db367308977311154af&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 09 Jul 2025 16:05:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[达摩院提出ReSpace！自回归文本驱动3D室内场景合成与编辑新框架！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5IUS026bgrHtl19k8iaTzNl8icef5pd4T4TvpwHoxzu1cyROia6klKjBkbZgib9aibULqBb20gpjfNRAAw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment论文链接：https:</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494061&amp;idx=2&amp;sn=36d78943e4aa37d67e89084802b839f5&amp;chksm=fdb6f9d980f7a2b4459b2ffa0348e06f782ae9a3f6fed30e4a75d271ffddb6c4d58611b9cf36&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 09 Jul 2025 16:05:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节推出统一多模态模型 BAGEL，GPT-4o 级的图像生成能力直接开源了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elzodISUKsiaVtsAvhTQ7mRre72SQ3NTx8amQXBMt77z295uWjzKl5kweQFLEMa31vXicZ35AvS4Lfw/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节推出的 BAGEL 是一个开源的统一多模态模型，他们直接开源了GPT-4o级别的图像生成能力。（轻松拿捏“万物皆可吉卜力”玩法~）。可以在任何地方对其进行微调、提炼和部署，它以开放的形式提供与 G</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494061&amp;idx=3&amp;sn=0acc483b588836cdca3563b42db99f50&amp;chksm=fdcc48a323133b7d6d279cee7f016b11f9d36c026be8efa1ae14fe51f4f914f5bb6af9aa218c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 09 Jul 2025 16:05:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[TMM 2025 | 超越SOTA！AdaMesh用10秒视频生成个性化语音动画，表情生动性提升40%。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48sD3er0mq7FL0guJnKjkMSPVhPLjFIJ4elWF7POpyFVoOcRqfjLRoPw/300?wxtype=jpeg&amp;wxfrom=0"/><p>在虚拟角色与数字人技术中，如何生成兼具真实感与个性化的语音驱动面部动画仍是关键挑战。现有方法往往依赖海量数据或通用模型，难以捕捉用户独特的说话风格（如微表情、头部动态）。为此，由清华大学深圳国际研究生</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494061&amp;idx=4&amp;sn=99a4f1161c03b733c1c2d4a651862c55&amp;chksm=fdfd74c9064c552a78965f19ea14baf7fe9d06e533249dac463be16b3c394330111a169734ad&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 09 Jul 2025 16:05:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[高质量开源二次元风格图像生成模型Neta Lumina，从Furry到国风，全方位赋能动漫创作新体验！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elCicOusPT4UMbWRggQc9icnnuuc8trMqQZdXicVSnCicAT0JorhOHp7RZ0picPcQW6vmQULK0icb6qeo3A/640?wxtype=jpeg&amp;wxfrom=0"/><p>Neta Lumina是由 Neta.art 实验室开发的高质量动漫风格图像生成模型。基于上海人工智能实验室 Alpha-VLLM 团队发布的 开源Lumina-Image-2.0，利用大量高质量动漫</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494060&amp;idx=1&amp;sn=a20eebec1beb20ed911d356094ecb7a2&amp;chksm=fdde39531d16ffa08c3cd377398230d383667a06fdb8e0a459a44e0032ce817e5b6b0bd008fe&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 08 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[OmniAvatar：让静态照片“活”过来，音频驱动全身动态视频生成新纪元！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elHjoNmnhIZ1WlEINYQPhBVwHtIicGD7DRHo7WEJn1mYDaiaRkEf8ES1uzXk9uBREDlwHfWKdLcqs2w/300?wxtype=jpeg&amp;wxfrom=0"/><p>OmniAvatar：“全能”的数字人视频生成。OmniAvatar 是一个基于LoRA的高效的音频驱动全身人像视频生成系统，支持从音频 + 单张图像 + 提示语生成自然、表达丰富的视频，仅需一条音频</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494060&amp;idx=2&amp;sn=aeb5da8b53d66ac305bfc902eaafa605&amp;chksm=fd719e47eb741df63ed88601e8d70ffececaa704bbb794ce2797fc95f4e20c72891c6c1a0fb3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 08 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[OmniGen2：支持视觉理解、文生图、图像编辑等任务，探索高级多模态生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek7H0AmSXtLibjgFibN8Hs8yrrhZa6JxHCHPbYCDGPOoQiaWTNCX0KMvXDq8E2VibCNrFhOQZicibkpSffw/300?wxtype=jpeg&amp;wxfrom=0"/><p>由北京人工智能研究院提出的 OmniGen2 是一个统一的多模态生成模型，它将强大的视觉理解、文本到图像的合成、基于指令的图像编辑以及主题驱动的上下文生成功能整合在一个框架内。它基于解耦架构，在保留高</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494060&amp;idx=3&amp;sn=c0d6d8df9cdd570fabd8b3a50b45ebec&amp;chksm=fdb45ed38b6035a6f69ebd8e54ea86d965c59e78e40e525e58c664a1bc9a958f213fac466355&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 08 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[DreamActor-H1，让产品与模特“一键生成”高保真交互视频。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48RPbNymvllhYib3H7VZyk223eWwAma3ovH2vTKZCY7Hg7zPurzD9kpqQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>DreamActor-H1 是一个基于扩散变换器 (DiT) 的创新框架，能够根据配对的人与产品图像生成高质量的人与产品演示视频。DreamActor-H1 基于大规模混合数据集进行训练，并结合多类别</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494060&amp;idx=4&amp;sn=d54c16d65743a821708f577633e48ffc&amp;chksm=fd889d72b8c82aed489631733801718df7075762d62a1393cbde5a5bf0ade9a21ba6cd2da13b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 08 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[重磅！黑森林实验室开放 FLUX.1 Kontext [dev]权重，120 亿参数黑科技，重塑图像编辑格局！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elCicOusPT4UMbWRggQc9icnn2bgzfAyBa1WiaESib9rM1Nz4l8rxIE1JdrFUviauFI2D6iaYeqtn00kOdA/640?wxtype=jpeg&amp;wxfrom=0"/><p>迄今为止，所有功能强大的生成式图像编辑模型都只能作为专有工具使用。如今，黑森林实验室发布了 FLUX.1 Kontext [dev]，这是FLUX.1 Kontext [pro]的开发者版本，它在一个</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494059&amp;idx=1&amp;sn=fc21ac36afe7b0218348f3015e41e238&amp;chksm=fdc36173e8147a5a9a5e64906f8abfe7d24a04324397e01c13a0b8e48ac02b6c7c3e5dce989b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 07 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI界的"六边形战士"！港科大×字节提出ComfyMind：生成/编辑/推理三连冠，开源领域再掀狂潮]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elI7B3IZQkA99hvyeKlzPzyeqYm9eaK3j5oUNFlRDs6yaz4YvOHWYMnpeWHk5ic5s7zDkXrP7RYtBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>由香港科技大学、字节跳动提出的一款基于 ComfyUI 平台构建的协作式 AI 系统ComfyMind，旨在实现稳健且可扩展的通用生成功能。在 ComfyBench、GenEval 和 Reason-</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494059&amp;idx=2&amp;sn=8d9f8a6f9061de481329f1e0d96d617f&amp;chksm=fd17c05448ec1253f445f517389231c19c486253683c8744c28853ec65b65d5bd2680b4efcc1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 07 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工力作Sparc3D：开启三维重建可微分优化与高效生成新纪元。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enhd8Da8niase1WltgKePj289UYQ2FkGK7uxrgpyoOIA6cIHk7jU4q6hvNUWTsCz3qI24ic8ibqQ8GjQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>由南洋理工大学推出 Sparc3D 是一个统一的框架，它将稀疏可变形行进立方体表示Sparcubes与新型编码器Sparconv-VAE相结合。Sparcubes 通过将有符号距离和变形场散射到稀疏立</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494059&amp;idx=3&amp;sn=8c6a99ce9411602bf664cb73a62c0ec0&amp;chksm=fd20362e1ed8d53767b53e3882be7f8ce1c3e07e470c4a57fdc50f2a30e277f64f16e8c247e5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 07 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[OmniConsistency: 一种基于扩散模型的风格一致性插件，用于高质量图像风格化。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/B1OJ3jLyfic6Yl0kTibHR6qeAggicnWLicFJTdTmKJibxibXDMjL83Ixvlciaqcwyoro532IvxIK9M3hB7eZ096Yw6jVg/300?wxtype=jpeg&amp;wxfrom=0"/><p>OmniConsistency 提出一种基于扩散模型的风格一致性插件，通过两阶段训练策略和滚动LoRA 银行机制，实现了在多种风格下的风格一致性和内容保真度，性能接近商业级模型 GPT-4o。在图像风</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494059&amp;idx=4&amp;sn=fc6159a5f08a18af583e896a2cf38ef2&amp;chksm=fd4e96f03c4ec6d6591ea8c971fde95953d56bda3cc451c6f6b100781d47d21430cc98acb9fc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 07 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[清华大学&amp;IDEA推出GUAVA：单幅图像生成实时可动画3D上半身，渲染速度突破0.1秒，表情与动作实时同步。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48yKhiadt9NN3KPxkMsuNZZxMYJMZSYDV5RsGLlXBfUxecaFP3xYF5UHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>由清华大学深圳国际研究生院、国际数字经济学院（IDEA）联合提出的 一个用于快速可动画的上半身 3D 高斯形象重建框架GUAVA，对于每张图像，GUAVA 可以在亚秒级时间内通过前馈推理重建 3D 上</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494058&amp;idx=1&amp;sn=0a6f42b794c38bb573c59adfe8cd2ae9&amp;chksm=fd124426eb50aca6d46b2fc5ce2eac358a446692a1a4cb644e9e7529bcceedd67924708a19f6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 06 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICIR2025 | CubeDiff：无需考虑失真，重新利用基于扩散的图像模型来生成360°全景图]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emrL9coT0EQdTjZR7WCoOG6qavvqaKicyhfbe1wrRfKuEmZbfJ8LvrOgQJMgZYG5CztqNUPPASQbtg/300?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中已经给大家介绍过许多关于3D生成的文章，感兴趣的同学可以点击公众号菜单栏查看3D生成专栏，创作不易，欢迎大家点点赞和在看~CubeDiff是一种使用基于扩散的图像模型生成 360° 全景</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494058&amp;idx=2&amp;sn=c93a9d4ab69d09a23e7384d37105f8a9&amp;chksm=fdbff2ea7f542dd2e1f8c5738a2ff62344f9cf6d648c4c03e29c466520c3888ceb0cba733b10&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 06 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[3D 生成新 SOTA！SECERN AI 提出 方法 SVAD，单张图像合成超逼真3D Avatar！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elmzbxIf6OS3v7M1woTicaJczQ6xAAgVU8NYrMphwhLiaiajhcsCMja0TDYcr6RulFp9C6Yt1mtcbiamA/300?wxtype=jpeg&amp;wxfrom=0"/><p>SECERN AI提出的3D生成方法SVAD通过视频扩散生成合成训练数据，利用身份保留和图像恢复模块对其进行增强，并利用这些经过优化的数据来训练3DGS虚拟形象。SVAD在新的姿态和视角下保持身份一致</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494058&amp;idx=3&amp;sn=2562b52332304d65038e31251952fbb4&amp;chksm=fd2d3fa43371e2a3374595cc0325976e353f38dcdb581953cb534868eebbca7f3dae4816a101&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 06 Jul 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[多领域SOTA诞生！Vid2World：打通视频扩散到世界模型的“任督二脉”｜清华、重大]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icoiaeFVcHGjGc24PwYRxSa3SRzxraaxquD1Y4eiapCrHo7GN2pjc3L4XfolskYUicsqxRONc0Q9o3iaR8g/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文链接：https://arxiv.org/pdf/2505.14357 项目链接：https://knightnemo.github.io/vid2world/ 生成效果速览亮点直击首个系统性探索</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247494058&amp;idx=4&amp;sn=b2abc84cbc6080c2bc07b54c10edd3b8&amp;chksm=fdfc65e9780900b97eeaf1eed8bd085da6eb997732c926db0b89d1ca9836d8ebb8b225257c06&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 06 Jul 2025 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>