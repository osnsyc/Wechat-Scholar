<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[统一建模，多人共演！字节提出 MAGREF，引领多主体视频生成新范式！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekCRGvUysqBs1VAaWXnVRNrzRItticzjVhjWUd81tRcCfAejhD1dM2vojtia7GHOFvP4WWK2wzNNN8w/640?wxtype=jpeg&amp;wxfrom=0"/><p>在生成式AI的浪潮中，视频生成正成为继图像与文本后的下一个爆发点。然而，当前主流技术仍困于“单主体、纯人像”的简单场景，面对多主体互动、复杂物体与动态背景交织的现实任务时，往往暴露出三大顽疾：人物身份</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493452&amp;idx=1&amp;sn=c76c1534b7f033ec9185384ee435504b&amp;chksm=fd66fa1232310c278ec571e993ceddd545c7782f21e20a1dc3cdd7f621ddc45a578859dc2416&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 17 Jun 2025 16:21:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI版玩具总动员！Articulate AnyMesh：开放词汇3D可动对象建模，自动给任意物体上关节然后动起来。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emlTwYmibs5btPianRd5BcicyicU9XuKiapM6UREY9FwGXMiclic5aK1IZY2K3p9ywZlqBwIYKbtdPJUg9jw/300?wxtype=jpeg&amp;wxfrom=0"/><p>由马萨诸塞大学阿默斯特分校、上海交通大学、卡内基梅隆大学以及麻省理工学院提出了一个开放词汇的3D可动对象建模框架 Articulate AnyMesh，能够以开放词汇的方式将任何刚性 3D 网格转换为</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493452&amp;idx=2&amp;sn=ad99d5369667f669386479b7259c2f31&amp;chksm=fd8308507c0e52be20bfdfdaaf21ae4c80bbf6cb438c775f5b51ab944ab848fd3e9aacb1dd2c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 17 Jun 2025 16:21:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节、港理工提出超强统一视觉生成模型 Many-for-Many，支持10+任务，8B参数“逆袭”商业视频生成引擎。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emb4MEj35KfTUoB1FWsgTXr1okRdYbDMkiaMBJd2BP7Rly0sBNFZKib2sPFdbSs7MvfFpF6hn5uyKnw/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节、港理工提出超强统一视觉模型 Many-for-Many，如何凭它让 8B 模型“逆袭”商业引擎？字节跳动与香港理工大学提出统一框架 Many-for-Many，它借助众多视觉生成和操作任务的训练</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493452&amp;idx=3&amp;sn=8c425d294c67aea4cb7e1c5b25ddd708&amp;chksm=fd8a9deb181741b838944e7fc8c205a2eff31877ae70bb4219cd71dca2a2ef04146fd8373dd0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 17 Jun 2025 16:21:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI界的"六边形战士"！港科大×字节提出ComfyMind：生成/编辑/推理三连冠，开源领域再掀狂潮]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elI7B3IZQkA99hvyeKlzPzyeqYm9eaK3j5oUNFlRDs6yaz4YvOHWYMnpeWHk5ic5s7zDkXrP7RYtBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>由香港科技大学、字节跳动提出的一款基于 ComfyUI 平台构建的协作式 AI 系统ComfyMind，旨在实现稳健且可扩展的通用生成功能。在 ComfyBench、GenEval 和 Reason-</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493452&amp;idx=4&amp;sn=e7fb28424358da185f65d2810a644d4b&amp;chksm=fd8aeb6e43566e2f0977d4b4c26d984d68fbaba1bf156760bb3d63b778c4a443167fbf6b2695&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 17 Jun 2025 16:21:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 北大提出FreeCloth：面向复杂人体衣物建模的自由生成方法，宽松服装实现高保真细节保留。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekZpjibRlqzZJvpnCtIusibumvetVwHLtQfibNRV5NCjOk9fMZ0qoTnYiblqn55YqgfyU1srg7kyibibCXA/640?wxtype=jpeg&amp;wxfrom=0"/><p>由北京大学王亦洲课题组提出的名为 FreeCloth 的基于点云的混合式人体衣物建模方案，突破了现有方法在宽松衣物建模中的技术瓶颈。针对宽松衣物与骨骼运动关联性较弱的特点，该方法采用无约束自由生成网络</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493408&amp;idx=1&amp;sn=383bbc36ae1dc2202a02fe99bca11d26&amp;chksm=fd2d37b2f93df38948be04d5e11747cc489e5ff29e220b876f7bd5f3805ff3d6c203e085e407&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 16 Jun 2025 22:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[统一图像生成模型OmniGen：可由多模态提示直接生成各种图像。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enjwj4Ry2OH6auaAn9DU954RGLVLiaJQhnSsUOPiaYkiaE5VPAB4AUAtmLI24PhQm9bK4JduBhT9ZjTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个北京市人工智能研究院 提出的统一的图像生成模型OmniGen，可以使用它来执行各种任务，包括但不限于文本到图像生成、主题驱动生成、身份保留生成、图像编辑和图像条件生成。OmniGen</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493408&amp;idx=2&amp;sn=9a628976c12d835f70c8544cd8a69d0e&amp;chksm=fd7ed31d55560641868413a174ba431c37e6d59d2fcfda9526f50f30c42a6fe90a78ce2ba459&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 16 Jun 2025 22:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[东京大学 | Adobe 提出InstructMove，可通过观察视频中的动作来实现基于指令的图像编辑。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsa4uWnhrMawFt9HHkxP0mNsA8WZRJb5wtxFQzRMjAicAjmryxF8Yliamw/300?wxtype=jpeg&amp;wxfrom=0"/><p>InstructMove是一种基于指令的图像编辑模型，使用多模态 LLM 生成的指令对视频中的帧对进行训练。该模型擅长非刚性编辑，例如调整主体姿势、表情和改变视点，同时保持内容一致性。此外，该方法通过</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493408&amp;idx=3&amp;sn=e4dab6f4efe034289ce404bf465ef8f8&amp;chksm=fd192d574255e77b6fec107eb45791414324710f754140cf24d488d6bc87cae0e9fd66810fed&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 16 Jun 2025 22:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[多人视频生成技术新突破！清华提出 DanceTogether：从单张图像到多人互动视频生成，从此告别身份混淆。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekZpjibRlqzZJvpnCtIusibumwGUpiatyy4T61q0VfQegTtcgO14OAic3MrV5T58OOrJYGw4FwLfASNmw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在人工智能与计算机视觉领域，视频生成技术一直是研究的热点与难点。特别是多人互动视频的生成，要求系统能够在复杂多变的场景中，精准地捕捉并再现多个角色的动作、姿态以及他们之间的交互细节。由清华、北大、中科</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493407&amp;idx=1&amp;sn=a2c2472cf92816b2b8ce0a91bb5d20c4&amp;chksm=fdec519b1672387ba3890f4813b85607a9a9ddf7f6baf2f38f436dad9fcec95f38811566352d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 15 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Google DeepMind 发布最强视频生成工具 Veo 3, 可为作品添加音效、环境噪音、对话，文中附体验链接。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcnWs2mR9uePicbSxmgsNGYEOvC44lWnQUfBAMbv2Kgy7vDib4ee4tlF1R091cfagJqdQWc10PdkUA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天跟大家介绍谷歌的视频生成模型 Veo 3，可为作品添加音效、环境噪音甚至对话，所有音频均可原生生成。它还能提供一流的音质，在物理效果、真实感和快速响应方面均表现卓越。相比 Veo2 的改变Veo </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493407&amp;idx=2&amp;sn=bc675a376adc0bdfe84803c5d0b2adad&amp;chksm=fdfa9b614304686f7eb3b3acd152f90ca6e779cc9008ec2d12067b164f873db0643acbf6d64a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 15 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 机器人双臂操控新突破！KStar Diffuser如何解决自碰撞与运动约束世纪难题？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icojvz9clmicqUEHWru0TSQwicibDxwd6pXeiac1QbZwUoibnWeMnE5ib2jBibpdEXVK5T4bFCwMWWK9BcS4dg/300?wxtype=jpeg&amp;wxfrom=0"/><p>文章链接：https://arxiv.org/pdf/2503.10743亮点直击与现有方法仅在笛卡尔空间中优化末端执行器姿态不同，提出了一种新颖的时空机器人图，显式地建模机器人物理配置，以指导生成动</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493407&amp;idx=3&amp;sn=2e1c0744a6c50addbd0eeb81fe49d488&amp;chksm=fd4e2fe848446d9914e4698b44869b3016a7b261fc8c731e15eac0583158ae2998f19816b9e7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 15 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI版玩具总动员！Articulate AnyMesh：开放词汇3D可动对象建模，自动给任意物体上关节然后动起来。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emlTwYmibs5btPianRd5BcicyicU9XuKiapM6UREY9FwGXMiclic5aK1IZY2K3p9ywZlqBwIYKbtdPJUg9jw/640?wxtype=jpeg&amp;wxfrom=0"/><p>由马萨诸塞大学阿默斯特分校、上海交通大学、卡内基梅隆大学以及麻省理工学院提出了一个开放词汇的3D可动对象建模框架 Articulate AnyMesh，能够以开放词汇的方式将任何刚性 3D 网格转换为</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493406&amp;idx=1&amp;sn=baa8fc1cad5ac3601c4e11fa342db56d&amp;chksm=fda9f952fef08d694b80517b1fd50f678c97c64f6071cbc0c4715acc7d1ef84bf110a5570b92&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 14 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里开源语音黑科技！SenseVoice：50+语言识别、听懂你的情绪，速度超Whisper 15倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/CibEZ9gjHpIoYgiabFF6GEeGdauiaiaTxCoCBH01vZHfNlwJYrs133ibV4nls7DmjxUv6LlXeqVgrBZwcgJnicZlFQSA/300?wxtype=jpeg&amp;wxfrom=0"/><p>------语音识别的新高度，情感与事件尽在掌握在人工智能飞速发展的今天，语音识别技术已成为人机交互的核心入口。阿里巴巴通义实验室开源的语音理解模型——SenseVoice，将语音识别技术推向了全新的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493406&amp;idx=2&amp;sn=5742f3d0099c08c56ba2598aad1bf5af&amp;chksm=fd3ca68aad07e1a22442190f6d99baa6f25aca6bf3eeff5152eecca7a3bb24463ae3ba81b759&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 14 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节推出统一多模态模型 BAGEL，GPT-4o 级的图像生成能力直接开源了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elzodISUKsiaVtsAvhTQ7mRre72SQ3NTx8amQXBMt77z295uWjzKl5kweQFLEMa31vXicZ35AvS4Lfw/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节推出的 BAGEL 是一个开源的统一多模态模型，他们直接开源了GPT-4o级别的图像生成能力。（轻松拿捏“万物皆可吉卜力”玩法~）。可以在任何地方对其进行微调、提炼和部署，它以开放的形式提供与 G</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493406&amp;idx=3&amp;sn=47017d61274d9037b431e2a1a4347134&amp;chksm=fde0e141c74deb04d75c77aae9cfc8bf66dc42629b8eb999275b9ee8b9ce6520db8398db5f14&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 14 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[不是P图！用ComfyUI复原老照片，像素级重生太惊艳了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/ACyQFjNqyE62umia43diaibQHp3ufyS7wiaxIEibtXWGdYtMcl71rnkX5MWXjibQMdn9RUbu6m0NMMfLoryHo7Tqzluw/300?wxtype=jpeg&amp;wxfrom=0"/><p>过去的一张张老照片，承载着无数回忆，也记录着一个时代的光影。但随着时间的流逝，那些泛黄、破损、模糊的老照片正一点点被遗忘。幸运的是，AI图像处理的浪潮正悄然改变这一切。而在这股浪潮中，一个名字正在悄然</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493406&amp;idx=4&amp;sn=edeca6b4e2cdaf3f2887d5796ac80616&amp;chksm=fd8a05690a2dc357028325892e4a51eebab916f5b13427b889ae70a003cbdf35b501361ef825&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 14 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节、港理工提出超强统一视觉生成模型 Many-for-Many，支持10+任务，8B参数“逆袭”商业视频生成引擎。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emb4MEj35KfTUoB1FWsgTXr1okRdYbDMkiaMBJd2BP7Rly0sBNFZKib2sPFdbSs7MvfFpF6hn5uyKnw/640?wxtype=jpeg&amp;wxfrom=0"/><p>字节、港理工提出超强统一视觉模型 Many-for-Many，如何凭它让 8B 模型“逆袭”商业引擎？字节跳动与香港理工大学提出统一框架 Many-for-Many，它借助众多视觉生成和操作任务的训练</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493405&amp;idx=1&amp;sn=3e4dbfd14d343d8579f4e0e26b307296&amp;chksm=fdc76cd68e28d8b3bf2db5a5b2cfe602e17d080f17999081e6fd09f699dc5a51e253eb0bfca0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 13 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[集成 R1 后的 GroundingDINO 究竟强在哪？一文带你看清 DINO-R1 的性能变革]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/vgev6PHxuZ3cCzjflZrObrcGTNoJJwzrOXe4jyYx9eDs8QIOJ4W5grQyGf2tTwtS8ooDFDop2w2gcbw8UuNhCw/300?wxtype=jpeg&amp;wxfrom=0"/><p> 导读在开始今天的分享之前，我们不妨先思考一个问题：为什么大语言模型，如 GPT 系列、DeepSeek 等，在数学推理、代码生成等任务中能够展现出强大的泛化能力和对人类意图的良好对齐？除了依赖海量高</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493405&amp;idx=2&amp;sn=1d9c87227f8d4a595b840a46386d63b3&amp;chksm=fdefb42de663cb93dd41f40e5105834443bfee347580c94b6f29dc50332b129dc78b3b64defb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 13 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[FramePack-F1：敏神全新算法重大更新！低显存ComfyUI可体验长视频生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/1BRxta5juGQdbH5tvxzar5TAtTxFX4G9Q9nTTmDOUmW2D7RDQfQTgEAkgeSRpgiaLL0F0qMLAoUcd96ndJ2Esmw/300?wxtype=jpeg&amp;wxfrom=0"/><p> FramePack-F1：全新算法和模型更新FramePack-F1简介在昨天的文章已经介绍过敏神最新基于混元视频的力作FramePack-F1模型，这是仅从前向帧预测未来帧的 FramePack </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493405&amp;idx=3&amp;sn=277d0c1f3b062dadbff33e2bef789f36&amp;chksm=fd4510fef013d2d4ae8640533415e179cbfdd3d760891017c9e00d1cc5cf8791bb800d9881e7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 13 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[英伟达提出最强「描述一切」模型 (DAM)，可生成图像或视频特定区域的详细描述，拿下7个基准SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emXysHeAOso1q4PjdgGCNECib2BYEbUlY3dMInZdicOKQibQAMwDLHA4kgviaROXJC16pncBthoyHBQJQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>英伟达提出「描述一切」模型 (DAM)，这是一个强大的多模态大型语言模型，可以生成图像或视频中特定区域的详细描述。用户可以使用点、框、涂鸦或蒙版来指定区域，DAM 将提供这些区域的丰富且符合上下文的描</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493405&amp;idx=4&amp;sn=bc1636527e4265783664c3d1853f72c5&amp;chksm=fd3c92d5c2f567ff800a410359bb871e3f8c29847d9b37669157da12be8d92e905eebf8da02f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 13 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI界的"六边形战士"！港科大×字节提出ComfyMind：生成/编辑/推理三连冠，开源领域再掀狂潮]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elI7B3IZQkA99hvyeKlzPzyeqYm9eaK3j5oUNFlRDs6yaz4YvOHWYMnpeWHk5ic5s7zDkXrP7RYtBA/640?wxtype=jpeg&amp;wxfrom=0"/><p>由香港科技大学、字节跳动提出的一款基于 ComfyUI 平台构建的协作式 AI 系统ComfyMind，旨在实现稳健且可扩展的通用生成功能。在 ComfyBench、GenEval 和 Reason-</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493383&amp;idx=1&amp;sn=e3cf68d2740fdfd4fbeda2a116a351f0&amp;chksm=fd5de759b3729f260c883d730d074271b5c576c0cc066ac70d0d3603a0605a647f7dc9bc1dbc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 12 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[港科大&amp;快手提出统一上下文视频编辑框架 UNIC，各种视频编辑任务一网打尽，还可进行多项任务组合！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emb4MEj35KfTUoB1FWsgTXrbAeApibeaxibS3pqDl8gmHOmyuzVnspI2tpOfRwXqcA6LGNdsjAjUyWg/300?wxtype=jpeg&amp;wxfrom=0"/><p>由香港科技大学、快手科技提出的UNIC（统一上下文视频编辑）是一个简单而有效的框架，它以上下文的方式统一单个模型中的各种视频编辑任务。从此，视频编辑用着一个工具就够了！ID插入ID交换删除ID相机控制</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493383&amp;idx=2&amp;sn=23b42708652a9dbf4dc058328447d4bf&amp;chksm=fdbbf20cae17c8a76bbb029d9d9ebd1e393e9c1fed49fe65c3233c5e529a8d297dba8838968a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 12 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节开源换脸写真模型InfiniteYou，可实现零样本身份ID一致保持，无缝集成FLUX、ControlNets、LoRAs！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekcpaxd048mMDrAunNibKNFB9QEic6a0icic21hdjU7tWMfgnZWZ32D1adHqJcD4Z8fvzhEvH6KNghsZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个字节刚开源的换脸写真新模型InfiniteYou，这是一种先进的零样本身份ID一致性保持模型，由字节跳动基于文生图领域最强开源模型FLUX模型研发的。InfiniteYou专注于利用</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493383&amp;idx=3&amp;sn=207b3f3d47a18879592dc3a9355ed652&amp;chksm=fd4bbfdc1f330dae8c72471e53b8f5a2b65b27f2c256628f893e138c2e62612b37e0db86cf95&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 12 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[加利福尼亚大学提出TULIP！视觉-语言模型的新王者！AI性能全面碾压CLIP！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5KsLicuyg3oA0dGOnwBictNTE782KtlqwlaVEmKrVyKAO0YzauujiaGWFqaYjHzZqKD5rLk8dQLKZtEg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：TULIP: Towards Unified Language-Image Pretraining论文链接：https://arxiv.org/pdf/2503.15485开源</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493383&amp;idx=4&amp;sn=618e16b5d3990c8378523ee6bda40d1c&amp;chksm=fd52291605ffd1d94d0e7abc651d286e42751b37b7762525ea2dbc5847dce26c6b7a938d7efa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 12 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦联合百度发布Hallo4：让AI肖像“活”起来！新型扩散框架实现高保真音频驱动动画生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em4gibISNFQR95biapR4RJ7Lq56s1kIaYWsxKESfb9riaHUQVlW3JfPib9AP6mL8Hk0Ec5R0f43HYJ8aw/640?wxtype=jpeg&amp;wxfrom=0"/><p>复旦联合百度发布扩散框架Hallo4，实现了准确的唇音同步、自然的面部表情，并能够稳健地处理各种角色身份和环境场景中快速的语音节奏和突然的上身运动。相关链接论文：https://arxiv.org/p</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493382&amp;idx=1&amp;sn=39ffd852e2ccd283cb9c564d565ba08f&amp;chksm=fd339e12fb401aa3f0a1b511e037be5faae1d733d59e62e6dbb2fefda798cdfbd64653043195&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 11 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[大模型再现黑马！英伟达开源Llama-Nemotron系列模型，效果优于DeepSeek-R1。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXFXA8pZKAq59wibWEHiaviafoC1ibJ7eE1fvbrtrICXG1kaXfiaqibBmibzznCUHyiaB4NGTibwK6pmBM0hA/300?wxtype=jpeg&amp;wxfrom=0"/><p>近日，英伟达推出了 Llama-Nemotron 系列模型（基于 Meta AI 的 Llama 模型构建）—— 一个面向高效推理的大模型开放家族，具备卓越的推理能力、推理效率，并采用对企业友好的开放</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493382&amp;idx=2&amp;sn=e165a22ba5dbade025d8e720714e318d&amp;chksm=fd41db51e43660c6a273cbf3ca9bf8a6361499eea144951b7ce0b8a8ced43f2cab4df595a1c6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 11 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦&amp;腾讯优图提出基于扩散的情感说话头像生成方法DICE-Talk，可为说话的肖像生成生动多样的情感。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekBUypVTojw9NicChAveibQcTccRrbh6qA2W0fWIHSYHibiaqHEFxVLBnicZtkEricIgpsqDf5wqctkvqRw/300?wxtype=jpeg&amp;wxfrom=0"/><p>复旦大学和腾讯优图联合提出DICE-Talk，这是一个用于生成具有生动、身份保留的情感表达的谈话头部视频的新框架。可以为会说话的肖像创作出生动多样的情感表达。相关链接论文：https://arxiv.</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493382&amp;idx=3&amp;sn=fa8393a66c634f95b73f0c58e8a5fb7b&amp;chksm=fd769d4f98388a5e69c7bfbbeb0f415a537a271920c9068baabb38ff4756569ce14291045159&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 11 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[告别"纸片人"试衣！阿里&amp;浙大提出3DV-TON，用3D几何骨架+动态纹理场，让虚拟模特"活"出真实衣褶！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emXysHeAOso1q4PjdgGCNECFZTEAl6XrNJIs6kBFtCKh4H4USr1Odbdw4IOg8SSgUfrQQVgR52lmA/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里联合浙大提出3DV-TON，可生成高保真度和时间一致的视频试穿结果，3DV-TON是一种基于几何和纹理 3D 引导的新型扩散框架。 可处理各种类型的服装和身体姿势，同时准确还原服装细节并保持一致的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493382&amp;idx=4&amp;sn=810a8f2ed7e85962c691485b3f4ee9d2&amp;chksm=fdfebe94aa4a5e24ae593709f6e5cba5ccad19f711b823cade08a1892c78a25b46ddb98f6645&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 11 Jun 2025 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>