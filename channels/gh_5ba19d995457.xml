<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    






















    <item>
      <title><![CDATA[Stability AI 联合UIUC提出单视图3D重建方法SPAR3D，0.7秒完成重建并支持交互式用户编辑。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4zxt8tZpVIWNwGIWEeh5iaYqC8uJSSWGn5hv37BppQMhwNVjgbEMmOgw/640?wxtype=jpeg&amp;wxfrom=0"/><p>Stability AI 联合 UIUC 提出一种简单而有效的单视图 3D 重建方法 SPAR3D，这是一款最先进的 3D 重建器，可以从单视图图像重建高质量的 3D 网格。SPAR3D 的重建速度很</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=1&amp;sn=5b72ce7f8c6cf6dfe853a76491eee0f6&amp;chksm=fd22c717e1473e7228e5c739a177c2666bce39b6a4d4da032608ccd5a6c4c8ef5d8bf1f49e72&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[北大提出定制化漫画生成新框架DiffSensei，可生成具有动态多角色控制的漫画图像。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgKtXcy3S4XBxj4sIBiacegBSAicARmN6YDuAjO6tUqgQ6TNNE8CbF3pFw/300?wxtype=jpeg&amp;wxfrom=0"/><p>由北京大学、上海人工智能实验室、南洋理工大学提出了一种新框架DiffSensei可以实现定制化漫画生成，解决现有方法在多角色场景中对角色外观和互动控制不足的问题。DiffSensei结合了基于扩散的图</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=2&amp;sn=91ab2fe971de657581119fbf62674822&amp;chksm=fdacb1e433a0cd2848fe0a808085f5ac9e709fc2854da5931ec18d031054f488c631f6353854&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[DeepSeek-V3 正式发布，已在网页端和 API 全面上线，性能领先，速度飞跃。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsVQ8iaD3eY0RibaYkYGTf6mgibWMibTiaiccjeVMM6rnIOdfQ4sLtwbuJJ1iaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>DeepSeek-V3 在推理速度上相较历史模型有了大幅提升。在目前大模型主流榜单中，DeepSeek-V3 在开源模型中位列榜首，与世界上最先进的闭源模型不分伯仲。unsetunset简介unset</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=3&amp;sn=03adf783346dd287860d17558659a1da&amp;chksm=fdb7b81e6fc16379e42c4f653aee55baa5f32697384b4c16ce412e3f76bb95d4bc23a620c02d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=4&amp;sn=ec1a6b75781ab0ae69edcebe84350dfc&amp;chksm=fd98b3a29816bca11506813a9fd296bd34fb3c13bbc70b066f2b62632f247fe404f9ad23bf8b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[阿里通义实验室提出AnyStory：开启个性化文本到图像生成的新篇章！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2empB05GMsROweibZLQdTRdryE5VRCZQVY9U7yt38iaOH1WTKFoooIOYJGiaeHWGaeL3ZuZ4sUW8zKnjg/640?wxtype=jpeg&amp;wxfrom=0"/><p>在这个数字化时代，生成式AI技术正以前所未有的速度改变着我们的创作方式。近期，阿里通义实验室发表了一篇题为《AnyStory: Towards Unified Single and Multi-Sub</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489981&amp;idx=1&amp;sn=0075a635bd4b70f8897e039ae41f4edd&amp;chksm=fdbca53cb21f66e8c140636186df570a919d004fb7dad68805c216d7df9401cf1b535bb5e0cd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 23 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LineArt：无需训练的高质量设计绘图生成方法，可保留结构准确性并生成高保真外观。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emtHS7t5ic0uQWb1AOhKNDRVgJ9ReODibzN7pbSG8HiaFPKVEMD45h5nQHic2uaYSruibSNfMdcXWxiclIA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一种无需训练的基于扩散模型的高质量设计绘图外观迁移方法LineArt，该方法可以将复杂外观转移到详细设计图上的框架，可促进设计和艺术创作。现有的图像生成技术在细节保留和风格样式一致性方面</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489981&amp;idx=2&amp;sn=96eadb560acd5e28c21979f1e786ec60&amp;chksm=fdba8083198186e9dc773f5cb71a677e6c9bebf19ae4b4d32caea3c4cb225e55aa3950d2e51b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 23 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Snap | 港科大提出端侧文生图模型SnapGen，参数仅SD十分之一，1.4秒内生成1024分辨率图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDuiaVeqQlxjbAyWQVBK9zTWxMkxd3V2yvSqRYageKwuRPwuQj7g2Iog/300?wxtype=jpeg&amp;wxfrom=0"/><p>这项工作提出了一种新颖且高效的 T2I 模型SnapGen，SnapGen 是第一个可以在1.4秒内在移动设备上合成高分辨率图像（1024x1024 ） 的图像生成模型（379M ） ，并在 GenE</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489981&amp;idx=3&amp;sn=9f8f2602584e2a3efdfd94bc889c1009&amp;chksm=fda8ecb605ed337ae24c0a3a01d0b24fd535ed81eb62e4c2b1e0cfb3d9bf8e0f8c5c51eb818d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 23 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NVIDIA提出虚拟试衣新方法EARSB，让时尚与科技完美融合！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2empB05GMsROweibZLQdTRdryhxs0ChYaSb8Q7MXgpODuC675ClEZrLFicPt5eWjzKjxOHWcsP2ic7rBA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在数字化浪潮席卷全球的今天，科技正以前所未有的方式融入我们的生活，包括我们追求时尚的方式。想象一下，无需亲临实体店，只需轻点屏幕，就能轻松试穿心仪的衣物，这不再是遥不可及的梦想。NVIDIA联合波士顿</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489980&amp;idx=1&amp;sn=e5644f7af10968f5b61df062c593f41b&amp;chksm=fdcfcea6689a2282559a810ef16a95cda0124d7b8d32316260107809ec3403ba7823608111c7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 22 Jan 2025 16:06:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[中科大提出新视频流制作动画解决方案RAIN，可实现真人表情移植和动漫实时动画。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekaVfDRjALdOCj5889F1MpAKe7VdTw8TPT3jvjm7A5B4CnAk0BFdqibXzkZls0gnHjLqpeExdtPhpw/300?wxtype=jpeg&amp;wxfrom=0"/><p>中科大提出了一种新的视频流制作动画解决方案RAIN，能够使用单个RTX 4090 GPU 实时低延迟地为无限视频流制作动画。RAIN 的核心思想是有效地计算不同噪声水平和长时间间隔的帧标记注意力，同时</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489980&amp;idx=2&amp;sn=81ee380588888072855c759aeb304672&amp;chksm=fdd8deb7ca6ae1c3a8e752fdb7c425f438867b074eceddaf70aaf8009764afe1499c36077906&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 22 Jan 2025 16:06:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[引领图像编辑领域的新潮流！Edicho：实现跨图像一致编辑的新方法(港科&amp;蚂蚁&amp;斯坦福)]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicEKO56ibffMsiau7qrg2gcpibqTFwwB20Tz8hX6wXGcTC684He5MiazvvzA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在图像处理领域，如何实现跨图像的一致编辑一直是技术挑战。传统方法往往局限于单张图像的编辑，难以保证多张图像间编辑效果的一致性。香港科技大学、蚂蚁集团、斯坦福大学和香港中文大学联合提出Edicho，这一</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489980&amp;idx=3&amp;sn=eb9ea7d88bf97a720a844ec403f8d9a1&amp;chksm=fd4872d652b148f3a01d4c8a01137f892c1849ec2aebeb00ebde9310fcac1e05071713852744&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 22 Jan 2025 16:06:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[解决文生图质量和美学问题，字节跳动提出VMix：多维度美学控制方法，一键提升图像美学。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcic6UBBbFKhnQGPNFnt0uNP0icQj44gWuVMkaK2nsqNia2kicW5icETqKKVMA/640?wxtype=jpeg&amp;wxfrom=0"/><p>为了解决扩散模型在文生图的质量和美学问题，字节跳动&amp;中科大研究团队提出VMix美学条件注入方法，通过将抽象的图像美感拆分成不同维度的美学向量引入扩散模型，从而实现细粒度美学图像生成。论文基于提出的方法</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=1&amp;sn=ae5928d9f36e2133a8fbb5688b2bfc92&amp;chksm=fd98b7372d9f557c10bd9f7b0bbc3b9a88d9e4decba29ba49f776126836c75b897a68574251f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图像超分辨新SOTA！南洋理工提出InvSR,利用大模型图像先验提高SR性能, 登上Huggingface热门项目。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnndGUoqrp8dTsfo3BKVIVGVNf5sWoXGauJCgAEaaCQm9Qb7QfuM34qZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>南洋理工大学的研究者们提出了一种基于扩散反演的新型图像超分辨率 (SR) 技术，可以利用大型预训练扩散模型中蕴含的丰富图像先验来提高 SR 性能。该方法的核心是一个深度噪声预测器，用于估计前向扩散过程</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=2&amp;sn=5bdb432efc836f876133e0e0bae8b7e1&amp;chksm=fd7d483c20654bbcbc1411092a35c0cf3d267b20147fafd7584c2ec4ae3543f1b15307885901&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[小米SU7璀璨洋红限定色360°全景图首次曝光？TRELLIS给你答案，实现可扩展多功能3D生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsrcRtOibx92ZhEt1Z0UdQ7JsQibr17Y0WaD8O08DMM3XIor43XOZVMvXg/300?wxtype=jpeg&amp;wxfrom=0"/><p>清华大学、中国科学技术大学、微软研究院联合提出T RELLIS，这是一个大型 3D 资产生成模型，可根据文本或图像提示（使用 GPT-4o 和 DALL-E3）以各种格式生成高质量的 3D 资产，可在</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=3&amp;sn=e151853f9bd46153b897b4f0e2401ca4&amp;chksm=fdc19fd286937aae1c4f8a3bf173aadfb77794b141bf0d4ea4c3dd8877cded8585316d16e77e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[你要跳舞么？复旦&amp;微软提出StableAnimator：可实现高质量和高保真的ID一致性人类视频生成]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEg0pWxDqMplvkr6CkMHxsZoRegYlaQmYz6ah0rQewI1UFbTMjpYhWh4Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>由复旦、微软、虎牙、CMU的研究团队提出的StableAnimator框架，实现了高质量和高保真的ID一致性人类视频生成。StableAnimator 生成的姿势驱动的人体图像动画展示了其合成高保真和</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489979&amp;idx=4&amp;sn=5824b4904804b29870c71c2ac413eaf1&amp;chksm=fd7aca3517eaf161b05840738e5c62bc3bdfb2c03399c1a5fd9b44c3eed4cd517cf78f17db37&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 21 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[CVPR 2024 Spotlight | 解锁图像编辑新境界, 北大、腾讯提出DiffEditor，让精细编辑更简单！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emtHS7t5ic0uQWb1AOhKNDRVPEH3Xqks3oDpG6kIgEczYPnVPsI98I9LqibzYz8fUXeJrNILLDFicB0A/640?wxtype=jpeg&amp;wxfrom=0"/><p>在图像生成领域，大型文本到图像（T2I）扩散模型近年来取得了革命性的突破。然而，将这些强大的生成能力转化为精细的图像编辑任务，仍面临诸多挑战。CVPR 2024, 来自北京大学深圳研究生院与腾讯PCG</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=1&amp;sn=78e02ad557acbddd73965e30230fe090&amp;chksm=fdbe7557a5166b386b9963284767c09132e19c70d858552f3ead19a6560e9aefc86fe035e88b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adobe与MIT推出自回归实时视频生成技术CausVid。AI可以边生成视频边实时播放！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDwNBzSGR9jz4olCuaibHPBcISVDNbZVjdKgcIl4GiaczxalR4zb0LKJQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>传统的双向扩散模型（顶部）可提供高质量的输出，但存在显著的延迟，需要 219 秒才能生成 128 帧的视频。用户必须等待整个序列完成才能查看任何结果。相比之下CausVid将双向扩散模型提炼为几步自回</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=2&amp;sn=c6502c899b6e3f852dfd2041911215b9&amp;chksm=fd54050113a0b64568ab964cb7a0254d15670d27d222e071ba613a4e6d1dea7fec1a8fb80b56&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[多模态图像生成模型Qwen2vl-Flux，利用Qwen2VL视觉语言能力增强FLUX，可集成ControlNet]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYHx9RLCm1u1zJr61WGBPZZicviaGPyXN8y5ZTaZE9jpPcdNSX1nmUlib5g/300?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen2vl-Flux 是一种先进的多模态图像生成模型，它利用 Qwen2VL 的视觉语言理解能力增强了 FLUX。该模型擅长根据文本提示和视觉参考生成高质量图像，提供卓越的多模态理解和控制。让 F</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=3&amp;sn=27a5753da03f6a835735475d04049920&amp;chksm=fd110660ccaf10ce849fe434a13dc1af783f42a485f7a20baaef50b5258309848cfe0c36dc61&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Google发布新AI工具Whisk：使用图像提示代替文本，快速完成视觉构思。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2end3mWUdomxapVIqKPBfrWChHLuwfMCvRvq1l8Kl6qfOOlq9YUxzPEdzMibDSUo0R5owCSicTJLumBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>Google发布了新的AI工具Whisk，Whisk 是 Google Labs 的一项新实验，可使用图像进行快速而有趣的创作过程。Whisk不会生成带有长篇详细文本提示的图像，而是使用图像进行提示。</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489958&amp;idx=4&amp;sn=a9f392d276409795dbe3f70c902346be&amp;chksm=fd8cb3e29c053652af42eda3f111fb346358a870517a14972adf84936712f347e75edb5b911c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 20 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Snap | 港科大提出端侧文生图模型SnapGen，参数仅SD十分之一，1.4秒内生成1024分辨率图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDuiaVeqQlxjbAyWQVBK9zTWxMkxd3V2yvSqRYageKwuRPwuQj7g2Iog/640?wxtype=jpeg&amp;wxfrom=0"/><p>这项工作提出了一种新颖且高效的 T2I 模型SnapGen，SnapGen 是第一个可以在1.4秒内在移动设备上合成高分辨率图像（1024x1024 ） 的图像生成模型（379M ） ，并在 GenE</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=1&amp;sn=1432436e0f71b9b7e486560668eeed7a&amp;chksm=fd9bc6a0e991e310061f042c8137126e01921ff048fc146bdb79d645fa89ad4db423f53e59fc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OminiControl：一个新的FLUX通用控制模型，单个模型实现图像主题控制和深度控制。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYEn3PFyv0qTxQYEgq8VntmUj91vEEYPJjMADiamfkH94icSBs7fF1Tn1A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中和大家介绍过Flux团队开源了一系列工具套件，感兴趣的小伙伴可以点击下面链接阅读~AI图像编辑重大升级！FLUX.1 Tools发布，为创作者提供了更强大的控制能力。OminiContro</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=2&amp;sn=5f893661ace33b3910b950d9030b7a1c&amp;chksm=fdaad180939f5106e2305580c604ad3fa7e7f1ee76410babf76cc5e46cf65de4c08bb3da09a7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里发布新ID保持项目EcomID, 可从单个ID参考图像生成定制的保ID图像，ComfyUI可使用。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elnGoicbmLL47YzLd4HWhjwazEmicf1F7ZjrxQSj4JNP3x3icluxM84Et2UYGdsdxfDOXnd9OlZYFCwg/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里妈妈发布了一个新的ID保持项目EcomID，旨在从单个ID参考图像生成定制的保ID图像，优势在于很强的语义一致性，同时受人脸关键点控制。EcomID 方法结合了 PuLID 和 InstantID</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=3&amp;sn=2a131da1305f4c755047f499f0b2fb72&amp;chksm=fd9be0eebc204661357981b7050c377b021814104b3d4c883a27a20527b26204b3694c9f62ed&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[港大和字节提出长视频生成模型Loong，可生成一分钟具有一致外观、动态和场景过渡的视频。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eny4Iriba5NSXkHvLxicLITJD5gFTkWLoBqMSNfUicQxXgibIh9n6vokK5ia5EOh7ZDJLVHGEsbaLz86XA/300?wxtype=jpeg&amp;wxfrom=0"/><p>HKU, ByteDance｜⭐️港大和字节联合提出长视频生成模型Loong，该模型可以生成外观一致、运动动态大、场景过渡自然的分钟级长视频。选择以统一的顺序对文本标记和视频标记进行建模，并使用渐进式</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489957&amp;idx=4&amp;sn=a3dc9baea4db7b2ce69c09edf47daebe&amp;chksm=fdce4e89e36260261732b1375680ed14de82d2c716f27c5952d048065d3058b47cb8a2e759f1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 19 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
