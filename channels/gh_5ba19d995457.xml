<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    



























    <item>
      <title><![CDATA[引领图像编辑领域的新潮流！Edicho：实现跨图像一致编辑的新方法(港科&amp;蚂蚁&amp;斯坦福)]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicEKO56ibffMsiau7qrg2gcpibqTFwwB20Tz8hX6wXGcTC684He5MiazvvzA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在图像处理领域，如何实现跨图像的一致编辑一直是技术挑战。传统方法往往局限于单张图像的编辑，难以保证多张图像间编辑效果的一致性。香港科技大学、蚂蚁集团、斯坦福大学和香港中文大学联合提出Edicho，这一</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=1&amp;sn=8f3541833f1b3a1ab71d132b9ae84666&amp;chksm=fd42cdd0435be406b12120fb005b4c4372e9c0c9dc338439136155dc6b190b9bf016091492f3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[图像编辑大一统？多功能图像编辑框架Dedit:可基于图像、文本和掩码进行图像编辑。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekDYMeOJw6PMrPrgUmBfVvIibC8Suae7poAtMSSVAkicNMibK5CyJB4RLSAKFiajeuqXiaiaib0vMibRiaSKCQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个基于图像和文本的编辑的框架D-Edit，它是第一个可以通过掩码编辑实现图像编辑的项目，近期已经在HuggingFace开放使用，并一度冲到了热门项目Top5。使用 D-Edit 的编</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=2&amp;sn=21834e17f83cf54755f4fe2b99836471&amp;chksm=fd51575dfac0bef5eadf6ca456bcf7fbfa3693f52424e4337ed0c4c56ce40fe82a1f5cab5119&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[扩散模型 vs GAN，谁将主宰文生图的未来？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/V4HEViaySCn8Onq8lDttiaFTpUiabu4PiassM06oMOYp7S6fggSCrREIZ3IoecSjrFkOF5jdiaPODhTx0ALBn3icp8Zw/300?wxtype=jpeg&amp;wxfrom=0"/><p>点击蓝字 关注我们导读你知道，就是那种能根据你的文字描述，一键生成图片的神奇技术。想象一下，你只需动动手指，输入一段文字，就能得到一张与之匹配的图片，是不是很酷？从最早的基于GAN（生成对抗网络）的技</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=3&amp;sn=6461fb360bdc70552893d69b1260caf0&amp;chksm=fd41c645caa30cdbe04e69fdb1d738877236baa432454be61ed3b7a23ec777a8f3e93e15345a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[长篇故事可视化方法Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489954&amp;idx=4&amp;sn=25eaecb6ed069f0bc5b3a09873f7d441&amp;chksm=fd735b674728b3e39d42ad141a023a6429bdebef7d1773a6852d410430d7c958b942d115abd5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 16 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[中科大提出新视频流制作动画解决方案RAIN，可实现真人表情移植和动漫实时动画。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekaVfDRjALdOCj5889F1MpAKe7VdTw8TPT3jvjm7A5B4CnAk0BFdqibXzkZls0gnHjLqpeExdtPhpw/640?wxtype=jpeg&amp;wxfrom=0"/><p>中科大提出了一种新的视频流制作动画解决方案RAIN，能够使用单个RTX 4090 GPU 实时低延迟地为无限视频流制作动画。RAIN 的核心思想是有效地计算不同噪声水平和长时间间隔的帧标记注意力，同时</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=1&amp;sn=ad177e495ad3e2052d95dbe49a68ce5b&amp;chksm=fd643ea4d20d09f8ca2c1910bdd5e4a3cfc2a6a287761384b05e51950c1277878f85052cb161&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[理想汽车提出3DRealCar：首个大规模3D真实汽车数据集!]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eldrtUtjxDXrVGVcbe8sY5fn25qjoY7k2Bsz6XJV8GAUvw3FQiaWMcbeiadZBbw9ZE2f1znye2jstGw/300?wxtype=jpeg&amp;wxfrom=0"/><p>理想提出3DRealCar，这是第一个大规模 3D 实车数据集，包含 2500 辆在真实场景中拍摄的汽车。3DRealCar的目标是可以成为促进汽车相关任务的宝贵资源。3DRealcar包含各种颜色、</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=2&amp;sn=2c49fe640f47cd28e9c19753d4fc21e5&amp;chksm=fd179b43565b066cdd786bfcea59e1768886f733e4cb57ba7d5e6b1857ec3c46e7b20b1fe2c6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[UIUC提出InstructG2I：从多模态属性图合成图像​，结合文本和图信息生成内容更丰富有趣！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZqBubdMgx3yBMfDK8JGL4YYX3hw4kJVRCHjFaqvVYYc7nEPXjibpCEug/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的这项工作是伊利诺伊大学厄巴纳-香槟分校的研究者们提出的一个新任务 Graph2Image，其特点是通过调节图信息来合成图像，并引入了一种名为InstructG2I的新型图调节扩散模型来</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=3&amp;sn=04ea94ae2fb61f64de0600a942236967&amp;chksm=fdc660d4cc39437f66c5fa2a96ccd4b26b472eb21bf811ccf8010a8612e5d12eec6af0d81fc3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[SD和Sora们背后的关键技术！一文搞懂所有 VAE 模型（4个AE+12个VAE原理汇总）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icojhGksxPcuzzcs6IGgx9ulYoFvA0ibAbJu9UfI318Eq0Y8x4wk2aoFzldXC9DKiclmSIkGiclenOCY2g/300?wxtype=jpeg&amp;wxfrom=0"/><p> 点击下方卡片，关注“AIGC Studio”随着Stable Diffusion和Sora等技术在生成图像和视频的质量与帧率上取得显著提升，能够在一个低维度的压缩空间进行计算变得越发重要。这种方法不</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489908&amp;idx=4&amp;sn=ef3daf262bc2df101417b200b505e198&amp;chksm=fd2f5a19b6f43147db8be066445396516a176e6a9d7284afcf5956c730473c668c1b3e39bbdd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 15 Jan 2025 16:14:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adobe与MIT推出自回归实时视频生成技术CausVid。AI可以边生成视频边实时播放！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicDwNBzSGR9jz4olCuaibHPBcISVDNbZVjdKgcIl4GiaczxalR4zb0LKJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>传统的双向扩散模型（顶部）可提供高质量的输出，但存在显著的延迟，需要 219 秒才能生成 128 帧的视频。用户必须等待整个序列完成才能查看任何结果。相比之下CausVid将双向扩散模型提炼为几步自回</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489895&amp;idx=1&amp;sn=e23d2324419a090406755f4485d19462&amp;chksm=fdf458d4f29287fde5d74485fbc12cacd9075a7e81b3e97935a12290ff06e2eb2312da594be2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2024 AI TimeLine 回顾（独家视角）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUnCMiaBjZZszIFmMq4b6euJ14V5ibkAcCZdmtRw3lykFcu3iafSO7319RSVydS3scYAgM0oASkKTw/300?wxtype=jpeg&amp;wxfrom=0"/><p>2024 AI TimeLine 回顾（独家视角）2024年，生成式人工智能已远远超越了仅仅作为一个流行词的范畴，它在实际应用和技术创新方面取得了显著进展，成为推动社会进步和产业变革的重要力量。以下是</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489895&amp;idx=2&amp;sn=1dce7fa3a57fc7211c9377baaf0250f6&amp;chksm=fd8587a28797f79d5bdb7df064e27c1ca2580f1014d00e93dadee1de1c98b8189382011c5b16&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ScribbleDiff：使用涂鸦精细引导扩散，实现无需训练的文本到图像生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en4dVnOT75Vve5gBZeAMAcqnHFQnQNTu2jZ3gdtvtEhgfeuBiawdPpo4eRXb4xIj7t0TCyfMVB3Rhg/300?wxtype=jpeg&amp;wxfrom=0"/><p>ScribbleDiff可以通过简单的涂鸦帮助计算机生成图像。比如你在纸上随意画了一些线条，表示你想要的图像的轮廓。ScribbleDiff会利用这些线条来指导图像生成的过程。首先，它会分析这些涂鸦，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489895&amp;idx=3&amp;sn=50249778fb8c43c58af4c0b24772fec9&amp;chksm=fd5d1fbd23d530c55d38ffaacd478644a92c459f9d7c4c24be060d7f1177c52a5533fb21751a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Retinex-Diffusion：让图像照明更加自然、细腻、富有层次感。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enUPMyjgiaKvq6tT4kEsgP5AtNIVia1UOcToyf7rIe0yncY9LBCDwGDAqtMbZTdicZDWLLWnnDLGEPhA/300?wxtype=jpeg&amp;wxfrom=0"/><p>这项研究主要是针对如何智能控制图像中的光照，采用了一种不需要重新训练模型的新方法。简而言之，研究人员利用一种叫作Retinex理论的方法，先识别出图像中的光照元素，然后用这些元素来指导图像生成模型。具</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489895&amp;idx=4&amp;sn=cc61032505cb7c4ea2a2bd594760b149&amp;chksm=fd8e59cf1e33a28ea7571075e4a6538562fd910a3f26a52e6f1f84f5ac4ea218676da97fb29c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 14 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[解锁衣物动画的新境界！EUNet：从单块布料学习衣物动画，让衣物动画更智能。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicgaOxsM6vJ2YozwQGBEXq7npQUx4Kaxib5TJnicQncs1FbFAnkscTs8LQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在动画制作领域，特别是对于衣物动画的制作，传统方法往往依赖于复杂的物理模型和大规模的数据集。然而，这种方法不仅耗时耗力，而且难以保证模型的通用性和鲁棒性。南洋理工、港大和上海人工智能实验室发表了《Le</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489894&amp;idx=1&amp;sn=105176d8e88480b93384b32bcd255198&amp;chksm=fde319f0266fc9f9dfeeec38c04eba9509548378edeba67a04fd2aa3937794a505be3352eb91&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 13 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ECCV 2024 | 3D数字人生成来了！南洋理工提出StructLDM：高质量可控3D生成并支持编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emxeSyCGnibXyZVpG7ibRkqUGd5Ez9HibtJvgxyqTcpLFbVsExsejticUXD4CdK03lP6H6CNDbUhvjWLQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>三维数字人生成和编辑在数字孪生、元宇宙、游戏、全息通讯等领域有广泛应用。传统三维数字人制作往往费时耗力，近年来研究者提出基于三维生成对抗网络（3D GAN）从 2D 图像中学习三维数字人，极大提高了数</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489894&amp;idx=2&amp;sn=2b688be25102437dd598b447ab938c5e&amp;chksm=fd3391c71c34196150632c145113ebafb30b64c0a2cd5eec1aeb3df1d6fa2d71ee06b3b6d1ca&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 13 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[3D虚拟试穿来了，上大、腾讯等提出ClotheDreamer，数字人也能实现穿，脱衣自由！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emPnvPXL7KLqPvkIAqtoJ3wAARpkHstN4O33m9M1haEAL9bqcv7I3brE6IfBs4EUXTjOuq6l2WvZg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天,给大家介绍上大、腾讯等提出的3D服装合成新方法ClotheDreamer,它以其革命性的能力,从简单的文本提示直接生成高保真、可穿戴的3D服装资产,正在重塑电商与空间计算领域的未来。数字人也能实</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489894&amp;idx=3&amp;sn=496bd14cab260f793634097a90dfdddd&amp;chksm=fd779877d09c18e5824400c822995e3fb9165ba77f159696e71df28ed45fdd3323e5fa4cf722&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 13 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[多身份定制化视频创作新突破！Ingredients：可将多个身份照片整合进视频创作实现个性化视频生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elSjibdLXhMBHvRNlreoaGcicdv0XZf04MKlYliaBmemOkIOtBU0wKu89VAd3lkLkDU9GOLfc5OIqIjQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在当今这个数字内容爆炸的时代，视频创作已成为连接人与人、传递信息与情感的重要桥梁。然而，如何高效、高质量地实现多身份定制化视频创作，一直是视频制作领域的一大挑战。近日，北京昆仑研究院的研究团队提出了一</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489877&amp;idx=1&amp;sn=74fa79f7d4ed29043c7d57e2366f4954&amp;chksm=fd3c10cefef16150da79d8f6dc32f48e6fa5237ed24085e2f06d7b9c4ff97759a79b469b9283&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 12 Jan 2025 16:02:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[3D服装生成新SOTA！谷歌和CMU提出FabricDiffusion：可将织物纹理从单个图像迁移到3D服装]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUwdDAlYv2ITgdKDOu7W8riapJOLuXkDL8cFpr3uMCxGiaooqI4z7Zcl4cQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>3D服装生成新工作！谷歌和CMU提出FabricDiffusion：一种将织物纹理从单个服装图像迁移到任意形状的 3D 服装的方法。FabricDiffusion性能表现SOTA！同时可以泛化到uns</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489877&amp;idx=2&amp;sn=30105d19b11d786e9991510cc45ddae1&amp;chksm=fd1e2f1186f3854e4cd34afaa50cc3019bad3f19d681ae4f9d60504bf83306487867da6fd7c2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 12 Jan 2025 16:02:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[超级智能“试衣镜”！GarDiff：高保真保持目标人物特征和服装细节，虚拟试穿技术新SOTA！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUN5oqyRgSButjKACUwRIef94PhQmUMcfJSkj4W9NicELKlw377icJuhpfjx2VUNPWKHMM0Gqib5Eg/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍了很多关于虚拟试穿的文章，本公众号也总结了虚拟试衣专题在公众号菜单栏，感兴趣的小伙伴可以在公众号内搜索“虚拟试衣”阅读～今天给大家介绍一个最新的虚拟试穿技术GarDiff，它可以分析</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489877&amp;idx=3&amp;sn=f90ad43c28663066d80cb069e824741e&amp;chksm=fd8801d336ec98e0124578e2faa6a25f1f0da5980cf022adb702e9cfeeec001195fe94394c9d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 12 Jan 2025 16:02:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一键试衣or一键脱衣？TryOffAnyone：从人像输入中生成高质量平铺服装。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUnCMiaBjZZszIFmMq4b6eoaD7GTE6xt5Kkbz15pCGnYFBHiaXPN7qYcV79yBTU689jcMRI5dJd0g/640?wxtype=jpeg&amp;wxfrom=0"/><p>TryOffAnyone 是一种新颖的单阶段框架，旨在从穿着衣服的人的输入图像和覆盖服装区域的相应服装掩码合成高质量的平铺布料图像。在 VITON-HD 等基准数据集上实现了最先进的性能。该方法在为全</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489847&amp;idx=1&amp;sn=7db0f231ee4ad08a334358ad4ac04e99&amp;chksm=fdbef338250deefff0ec767862840950325fd06792b27e5a99789e611ffa6f00840ba70928f0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 11 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NeurIPS 2024 | SHMT：通过潜在扩散模型进行自监督分层化妆转移（阿里&amp;武汉理工）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsjQl0sGle0TkYDcmMMuGmbtLXibkDVicOAa1tpYmub1EJgQJfZ41lm6WQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>当前的妆容转移技术面临两个主要挑战：缺乏成对数据，导致模型训练依赖于低质量的伪配对数据，从而影响妆容的真实感；不同妆容风格对面部的影响各异，现有方法难以有效处理这种多样性。今天给大家介绍的方法是由阿里</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489847&amp;idx=2&amp;sn=164295c163ad41f30fecbb828173872a&amp;chksm=fd7b4cf596f8808e7e921d7094310ff8bda60fe4ff600ecdb39a7874ed022deba46f6d12bbd2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 11 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里达摩院提出开源AI图片上色模型DDColor:可以为黑白照片、人物、动漫风景等一键上色!]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emSxAhfwGrF5xDyQho3K1KHs5BPR4ic9nBrD4MlgCC5ibUfic09OiajZVFthOcVSdugCDCmu33gKAffhA/300?wxtype=jpeg&amp;wxfrom=0"/><p>DDColor 可以为历史黑白老照片提供生动自然的着色。它甚至可以对动漫游戏中的风景进行着色/重新着色，将您的动画风景转变为逼真的现实生活风格！相关链接项目：github.com/piddnad/DD</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489847&amp;idx=3&amp;sn=52d7b395e98127288238663caa2e670a&amp;chksm=fdb9add807338abab35e833abe48d4e167ec72251d091823166a4d926cd6ba1c72545eebdcbf&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 11 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adobe发布TurboEdit：可以通过文本来编辑图像，编辑时间<0.5秒！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elKcprhHqENugIHSUTwb3EOiaaqictMa8fmmNEDqsoISMhGDZH4oZmh7vtMn5sov6khPdhIypPkhDZQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍Adobe研究院新的研究TurboEdit，可以通过文本来编辑图像，通过一句话就能改变图像中的头发颜色、衣服、帽子、围巾等等。而且编辑飞快，<0.5秒。简直是图像编辑的利器。相关链接项目</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489847&amp;idx=4&amp;sn=be6267976a155ad70190989ed35356e0&amp;chksm=fd7cc3d3e3c38548c43c7110d1c2a8ffd001e2601743e5752de8dfb6c77c5c8e995550e3b2a2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 11 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[2024 AI TimeLine 回顾（独家视角）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUnCMiaBjZZszIFmMq4b6euJ14V5ibkAcCZdmtRw3lykFcu3iafSO7319RSVydS3scYAgM0oASkKTw/640?wxtype=jpeg&amp;wxfrom=0"/><p>2024 AI TimeLine 回顾（独家视角）2024年，生成式人工智能已远远超越了仅仅作为一个流行词的范畴，它在实际应用和技术创新方面取得了显著进展，成为推动社会进步和产业变革的重要力量。以下是</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489813&amp;idx=1&amp;sn=51ebf152c0ae606b68387d40e64f97fb&amp;chksm=fd9d9b818faf50990536c705845570876d1321e1d63846cae890e6b333ec8df682c105046aa8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 10 Jan 2025 16:44:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[东京大学 | Adobe 提出InstructMove，可通过观察视频中的动作来实现基于指令的图像编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsa4uWnhrMawFt9HHkxP0mNsA8WZRJb5wtxFQzRMjAicAjmryxF8Yliamw/300?wxtype=jpeg&amp;wxfrom=0"/><p>InstructMove是一种基于指令的图像编辑模型，使用多模态 LLM 生成的指令对视频中的帧对进行训练。该模型擅长非刚性编辑，例如调整主体姿势、表情和改变视点，同时保持内容一致性。此外，该方法通过</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489813&amp;idx=2&amp;sn=81bd57715905ef2609fb944745535e3f&amp;chksm=fd097f3583d779d5924d78fb290f7d78ac6518b7e55fcdca6724101fc29da25ff3244805dc94&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 10 Jan 2025 16:44:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[提出街景定位大模型AddressCLIP：一张图实现街道级精度定位！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eldKGCwibmhq5RSxC5rV78dDcVpQDWZ2qUibtJW2qRF8ehlmicnuSw3n5MdOVQ0NTovfOnPib1RNDwBibQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>中科院自动化所和阿里云一起推出了街景定位大模型AddressCLIP，只要一张照片就能实现街道级精度的定位。比如给模型看一张北京南锣鼓巷的街景之后，它直接给出了具体的拍摄位置，并列举了附近的多个候选地</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489813&amp;idx=3&amp;sn=d14a4948c82f4267521c7541e0e6c672&amp;chksm=fd9e61e7c96af6e00f0affd2b24d124ab8975f8ee9fe5649ab6eb51e1cb50cbad3ccb1520cfa&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 10 Jan 2025 16:44:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[谷歌DeepMind重磅推出多视角视频扩散模型CAT4D，单视角视频也能转换多视角了。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emYIXZcOoWmiamNNy78gGxDxw4uWDBzPA32XByk1moUtrt0vCzrccsjPryianfay5w8IibgLtxibuRokQ/300?wxtype=jpeg&amp;wxfrom=0"/><p> 单目视觉4D重建再突破！谷歌DeepMind推出多视角视频扩散模型CAT4D，单视角视频也能转换多视角了。单目视觉4D重建再突破！谷歌DeepMind等团队，推出了多视角视频扩散模型CAT4D，它支</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489813&amp;idx=4&amp;sn=d16d497c55111563269493ea006710e9&amp;chksm=fdeea1749619d169d76f7f703c8703cbef63fc1ead7203f12104502e34f29c588633a17da278&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 10 Jan 2025 16:44:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
