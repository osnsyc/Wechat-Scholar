<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[CVPR 2025 | 字节提出个性化多人图像生成新方法ID-Patch，可生成多人合影、姿势可控。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emCuicERoV3guOMh64VYNrcA6VO1uBfS3aIicTCtKS3eFEBxCVDPwXCyj0Fye0L4toEplkN73YiaibibFw/640?wxtype=jpeg&amp;wxfrom=0"/><p>相信扩散模型（DMs）大家一定都不陌生了，目前已经成为文本生成图像的核心方法，凭借强大的图像生成能力，正重塑艺术创作、广告设计、社交媒体内容生产格局。现在，用一段文字生成个性化头像都不算啥新鲜事儿了。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493034&amp;idx=1&amp;sn=9f5924afb753e1095889765b225b0b18&amp;chksm=fd19b0b060dd3755756fbaed7c0b7a77052e1afb720d26c1a7431eb66618698a632c3ecc44b7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 28 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[音视频同步生成的终极突破！浙江大学提出JavisDiT！HiST-Sypo技术实现帧级对齐！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5IllwjzyahfYe3lVzotU3V1MZ9EYSziaPYoRvako9SY8kibHfdibhUKeiaP1lK2fWiafUwY7AOZZqvfibLA/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名：JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchr</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493034&amp;idx=2&amp;sn=dd7bf26e489a129405f608099b6543fa&amp;chksm=fd1f2aca7f1cd02f6502e30201ba29c1c19b63490c051ec9d76f8615c2d666957c6aa5a64793&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 28 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[港大&amp;Adobe联合提出图像生成模型PixelFlow，可直接在原始像素空间中运行，无需VAE即可进行端到端训练。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em57kq23EbSGQ52kUcSia6n8oTIJOicficBicZpibaJQgm7tEpQJ6psVkrLse6pjDUwqiaktvnGSEiaL6xPg/300?wxtype=jpeg&amp;wxfrom=0"/><p>香港大学和Adobe联合提出了一种直接在原始像素空间中运行的图像生成模型PixelFlow，这种方法简化了图像生成过程，无需预先训练的变分自编码器 (VAE)，并使整个模型能够端到端训练。通过高效的级</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493034&amp;idx=3&amp;sn=b5146708b1a48b6b460e759e1bd6c823&amp;chksm=fdca895f7ade8cda1b6efd6628e97b9f145efca6dae57414fc821ecfacdd39179e227dfb17d5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 28 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[开源数字人克隆神器HeyGem：1秒视频生成4K超高清AI形象，用AI重塑数字人创作生态！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elzodISUKsiaVtsAvhTQ7mRrgxstWFTNfP8vOAkR5RI8GOy83ObgNDrZJL0p3TTnAIBViacS7PlySow/640?wxtype=jpeg&amp;wxfrom=0"/><p>在虚拟形象与数字内容需求激增的当下，传统3D数字人制作的高昂成本（动辄数十万美元）与复杂流程，让许多行业望而却步。而今天，一款由Duix.com团队打造的开源AI项目HeyGem，正以颠覆性技术打破这</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493033&amp;idx=1&amp;sn=9226c2e340cbca59548eeff87a8f23eb&amp;chksm=fd465a092a1383186ffc5d22b6a0afe78c54af3da6a888493cec8361036eb85a0f979c380761&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 27 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[利用多模态模型赋能，SONY团队完成音乐到音乐视频描述生成大突破！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5Jn7gNOibialUb7ePwaNgQPKeSIN3Kfa1hwX15JM3vgCh8jl1Fm3ZyyqibhJ0YwwiaTxARyLn4ucxtkUw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：Cross-Modal Learning for Music-to-Music-Video Description Generation论文链接：https://arxiv.o</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493033&amp;idx=2&amp;sn=c8e23e47fa7deb61053b222619a23510&amp;chksm=fdddfb5fccdb16ac37b7c290f74625402e8fb48fc773a71f7c8fa5b19b49270ef7b8d5773d88&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 27 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[太强了！浙大联合上海AI Lab提出视觉统一Diffusion架构DICEPTION！各种视觉任务一网打尽！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcb6XeOfGM7ic3jww1VGas5hyQ5UbdLhbhjcqHwrckdlwdXIvppjK9PlGZVkxMpOMiaT6tDJ32KOqiaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>数源AI 最新论文解读系列论文名：DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks论文链接：https://arx</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493033&amp;idx=3&amp;sn=ee5fb1ef949c7833cd1d17bd1ca23821&amp;chksm=fdc51797ee2ef42656eabe900a70b6c3ef3038a6600223516ea2d9baf712ebffd34135903fc1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 27 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[炸裂！ComfyUI 原生支持 HiDream-I1，全新文本转图神器来了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eloBQe14a8ohz069lCGESt2ibyrQzlh4BO0Xa83u0NI5WzuIBs5KCqMafPkjLwMiapJ0TVwXCaj6ibIQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>ComfyUI 原生支持 HiDream-I1，全新文本转图神器来了！大家好，这不是演习！ComfyUI 终于官宣——原生支持 HiDream-I1 模型啦！对于熟悉图像生成的小伙伴来说，这可是一件值</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493033&amp;idx=4&amp;sn=0a8652929beb0fc72eb6a50a20328cac&amp;chksm=fdc952285125628019fec4114bef0cc112aeacd5d2f3da79e1afcd5d6bca2dd88a6c54f573d8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 27 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节推出统一多模态模型 BAGEL，GPT-4o 级的图像生成能力直接开源了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elzodISUKsiaVtsAvhTQ7mRre72SQ3NTx8amQXBMt77z295uWjzKl5kweQFLEMa31vXicZ35AvS4Lfw/640?wxtype=jpeg&amp;wxfrom=0"/><p>字节推出的 BAGEL 是一个开源的统一多模态模型，他们直接开源了GPT-4o级别的图像生成能力。（轻松拿捏“万物皆可吉卜力”玩法~）。可以在任何地方对其进行微调、提炼和部署，它以开放的形式提供与 G</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493007&amp;idx=1&amp;sn=2450124ac892965707e662a3dc99bc9c&amp;chksm=fd7dc800f40b88b86aacdf8601e0ca9c3f439daaa452f60ea93ada0fffeebebee7c885927c98&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 26 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[图像编辑革命，万物皆可插入！浙大/哈佛/南洋理工提出Insert Anything，告别PS抠图，AI让世界无缝生长。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enmjqTKh2qwkPiauc2Ejsn7Ficnb2ehPShfDudYtibS1fkY0Su3IFmdP3MkS9KDH1gsquQnXh8Ku6TPQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>浙江大学、哈佛大学、南洋理工大学联合提出了统一的图像插入框架Insert Anything，支持多种实际场景，包括艺术创作、逼真的脸部交换、电影场景构图、虚拟服装试穿、配饰定制和数字道具更换，下图展示</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493007&amp;idx=2&amp;sn=a993359dfcd1ddbe8fb2d41a16338b03&amp;chksm=fddb59fee978f4956ebe2d33aedb21e6b9844ecb3a8f0d51768ea60d29d709637e0d620b9992&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 26 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[6秒音频即可克隆AI语音！FLOAT数字人生成语音/口型/表情，情感同步超惊艳，文中附工作流。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elmzbxIf6OS3v7M1woTicaJcmBGicWjwiauMpFknBOofINibzHjBSIibjwDHKYvhnzulS1E2KIPicobCywA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的 FLOAT 是一种基于流匹配的音频驱动的说话肖像视频生成方法，可以增强语音驱动的情感运动。该方法唇形同步质量高，生成速度还很快。6秒音频完美生成语音/口型/表情。情绪转移由于 FLO</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493007&amp;idx=3&amp;sn=d2d8f0a69bd8915725228c6d49ceed96&amp;chksm=fd52c14d6ed8d1a95ec30520f43231c925fa8d2faf080b5bc3c5b92d662f12dad398ac18a8e8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 26 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦&amp;腾讯优图提出基于扩散的情感说话头像生成方法DICE-Talk，可为说话的肖像生成生动多样的情感。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekBUypVTojw9NicChAveibQcTccRrbh6qA2W0fWIHSYHibiaqHEFxVLBnicZtkEricIgpsqDf5wqctkvqRw/300?wxtype=jpeg&amp;wxfrom=0"/><p>复旦大学和腾讯优图联合提出DICE-Talk，这是一个用于生成具有生动、身份保留的情感表达的谈话头部视频的新框架。可以为会说话的肖像创作出生动多样的情感表达。相关链接论文：https://arxiv.</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493007&amp;idx=4&amp;sn=3c06934c2b776520241c62a87bb0dc89&amp;chksm=fd1f7254e73a0c323fdb694250b84023bd68bf21e1735681fe97dc797df6b7ed20cccb88544a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 26 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AIGC Studio 联合机械工业出版社给读者免费送新书啦，开启 AIGC 智能教学新时代！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elzodISUKsiaVtsAvhTQ7mRrRLGsBHPcPFYjEQGy2R1696AH45yrdicaI7TibaZZoxAiauDh2ic1IMLl1w/640?wxtype=jpeg&amp;wxfrom=0"/><p>亲爱的读者们，我们正身处人工智能（AI）技术飞速发展的浪潮中，AI正以前所未有的速度重塑教育领域。你是否渴望了解AI如何赋能教学，提升学习效率？是否想掌握最新的AI教育工具，成为未来教育的引领者？现在</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493006&amp;idx=1&amp;sn=ef99a6d477ab0bfb69aaa25eed245895&amp;chksm=fd6ff6adb9bc8e77c3664fa305ee40e9abcb379a943670e2ab16dfe33ed3c136e6427333ac34&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 25 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[视觉生成领域新突破！无需引入任何外部表征组件：SRA助力Diffusion Transformer实现自我表征指导。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcnWs2mR9uePicbSxmgsNGY5eoKECJw2VJZHk986ZktQp6KeSnEV1SrSa2JVMKibe6QbJ5dpasib3Kw/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇文章来自公众号粉丝投稿，对于Diffusion transformer在视觉生成领域获取高质量表征不容易的问题，文章提出了一种SRA(Self-Representation Alignment)方</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492810&amp;idx=1&amp;sn=f9e0c5c2aad85a2d7583a90415a29409&amp;chksm=fd0ef80dadbe86088af659924fa73e14ef4cd85a9a05517fdcb28cdaacaac9000390d1697871&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 24 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Apple提出UniGen！多模态理解生成统一xii新架构！CoT - V提升图像生成质量！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5KqpbKjwyf8GDnoGZ1ANRZVHSofem5JIanFIxSibozXUibNxHviaUIPE6FTh1nw9lCf16QMqWDaqf7cg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：UniGen: Enhanced Training&amp;Test-Time Strategies for Unified Multimodal Understanding and </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492810&amp;idx=2&amp;sn=78a042467cb774887c54251d98689a96&amp;chksm=fd946423c2521902cc57950d1287e8d034ef55b11a3e2f1e0f9d8b7eeee9b548cbf4f4a32cc3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 24 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI视频生成新突破！字节提出一致性视频生成方法Phantom：通过跨模态对齐生成主题一致的视频，超多应用场景。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enmjqTKh2qwkPiauc2Ejsn7FjUDMtLDDzOxeRDTsjoBO7nWymp4ibfUg4ngicJhNSbFdrgXOp81mHMKg/300?wxtype=jpeg&amp;wxfrom=0"/><p>Phantom 是一个统一的视频生成框架，适用于单主题和多主题参考，基于现有的文本转视频和图像转视频架构构建。它通过重新设计联合文本-图像注入模型，利用文本-图像-视频三元组数据实现跨模态对齐。此外，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492810&amp;idx=3&amp;sn=8631834744a0367b70cbbaa3318d8d31&amp;chksm=fdc8db091d82c60cec2d7eea45943088786edb2aae642d9bd11d9212aec08b217189201e53f0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 24 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里联合上海 AI Lab 提出DMM！多个模型压缩成一个通用 T2I 模型！实现可控任意风格生成!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enKvWzJ9QLeWgYQiaKmEWxL1P5PicFV1tgibVmCBLQ8OGUjLpAzElnOKcfkVy2nedvyOpMtQl3n9DKicw/640?wxtype=jpeg&amp;wxfrom=0"/><p>阿里联合上海 AI Lab 提出了一种基于分数蒸馏的模型合并范式DMM，将多个模型压缩为一个多功能的 T2I 模型。DMM 能够在风格提示的控制下生成各种专业风格的图像（写实风格、亚洲肖像、动漫风格等</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492720&amp;idx=1&amp;sn=226943ef8dcad184d80aea9af3e66c76&amp;chksm=fdc6db95f92d04890c4c8ff8a3de9561aae0b2f6790b9f3e9141f5b06f4a3780c04a675396a4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里&amp;北邮提出基于Wan2.1的音频驱动数字人FantasyTalking，只需输入肖像、语音和文字即可生成动画。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en6YOGtn3XXJMye1oxLXOtQU407yMKvXQv0r7RibwF9tY6RoaWiaXTsGib66ALF1tYibibzZZ51ibfTjibCA/300?wxtype=jpeg&amp;wxfrom=0"/><p>由高德地图、阿里巴巴、北邮联合提出首个基于Wan2.1的音频驱动数字人FantasyTalking，只需输入肖像图像、语音和文字，即可生成表情丰富、肢体动作自然且具有身份特征的动画肖像。此外，Fant</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492720&amp;idx=2&amp;sn=3db9d9c49853f654b8c2e932bdf6a00c&amp;chksm=fd9cd3c30ef9959d1019ded712b0e3c8c4bcd48e24ca905d1b669a64585ca97b10124e654992&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大联合上海AI Lab提出视觉统一Diffusion架构DICEPTION！各种视觉任务一网打尽！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcb6XeOfGM7ic3jww1VGas5hyQ5UbdLhbhjcqHwrckdlwdXIvppjK9PlGZVkxMpOMiaT6tDJ32KOqiaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>数源AI 最新论文解读系列论文名：DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks论文链接：https://arx</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492720&amp;idx=3&amp;sn=eabe660b6826efd4bcd5177e7649126e&amp;chksm=fd623da5c814d23f87bfcabdc32e88c6af9567b359cf6169f58d4aad0baf2e71deddd81cd47f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[利用多模态模型赋能，SONY团队完成音乐到音乐视频描述生成大突破！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5Jn7gNOibialUb7ePwaNgQPKeSIN3Kfa1hwX15JM3vgCh8jl1Fm3ZyyqibhJ0YwwiaTxARyLn4ucxtkUw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：Cross-Modal Learning for Music-to-Music-Video Description Generation论文链接：https://arxiv.o</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492720&amp;idx=4&amp;sn=fafdd17b3cd673e2d4df703c9a9fde7c&amp;chksm=fdfec1ddee646414065722bf9c79272528050c348aa0f35d63a395fe3063e446845947914ef1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[谷歌黑科技炸场！LightLab：只需一张图+AI，光影编辑像呼吸一样简单，废片秒变电影级大片！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek5oLyjfCjICEWyMhWNvFXD4gjUliaiaZ2wibPXVQsv1Vm81gqpJibC7scwg1DqQJbMMmvr7P8D5cPMCA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中以及和大家介绍过需要关于图像&amp;视频重打光的方法，在今天的推送文章中，已经帮大家重新整理好了，欢迎大家点击阅读~今天给大家介绍谷歌提出的一种基于扩散模型的方法LightLab，可以实现对单</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=1&amp;sn=99354cc4985dc55c109838046f75fbb4&amp;chksm=fdacbfb75e0042cba268425f308b31bcb71945fd3ae2c0f3f770d26fc237ebabe2f3c9f892fa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IC-Light升级，支持视频重打光! RelightVid可在多视频场景中重照明，支持文本提示、背景视频和HDR输入！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek6QSiaic7OicOck7L6SeBvmG8KxGGhaK7IiaIoGtBJsFyM7LffJExAYwxgr09hKHicONPnN40NOq3Cib7A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中已经和大家介绍过ControlNet作者关于图像重打光的工作IC-light，这篇论文也是获得了ICLR2025的满分评分，感兴趣的小伙伴可以点击下面链接阅读！ICLR 2025满分论文，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=2&amp;sn=2950a56f26e8587c95186e0c9117dcdb&amp;chksm=fdff6c2ef47e1f54bb031474e3e4498163df8529daca24886b2d05817c3423e03251dc4a09e8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[耶鲁大学联合Adobe提出SynthLight：智能重塑人像照明，打造完美光影！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elPyLquFq9rYTicjFkPwyh9fFVDfMwbeuJFlesWohTUXxZRSXxUpCJVwUUib0mdhjaia5sa6Ciaibic8AQg/300?wxtype=jpeg&amp;wxfrom=0"/><p>耶鲁大学和Adobe提出一种用于人像重新照明的扩散模型SynthLight，该方法将图像重新照明视为重新渲染问题，其中像素会根据环境照明条件的变化而变化。在真实肖像照片上可以产生逼真的照明效果，包括颈</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=3&amp;sn=5e06c57c62bdf8160727f2911a8a7b18&amp;chksm=fd9f9180528417d6eb5a90537c53aa755d2edbc52acb91625e764569e4287f78b99d00f03150&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR 2025满分论文，ControlNet作者新作IC-light，控制生成图像照明，代码模型已开源。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enrJ6ibDPoiaQXzhMdZU8spAicfF8vgRl2qenVGz9ZkJkJgBYXd26ys3WTPnNDJtK81bRSE1Xia412nbQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>罕见！ICLR 2025 惊现了一篇满分论文，4个审稿人同时打出了[10,10,10, 10]，这是什么炸裂的存在？！这就是ControlNet的作者张吕敏，继ControlNet 后提出的IC-li</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=4&amp;sn=9b617f922e3e7a1681a2343d5385cc9a&amp;chksm=fda03aa51aca322b363f9343404c40c107837a266926ac84f76fc3ec2849b48616e1e8bdbe2b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[​IC-light V2：基于FLUX训练，支持风格化图像，细节远高于SD1.5。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXSHQPic5rJ8IBKiaNosJKl4zjuJBWaX5OT2Lf7ZUfribJIKIVPQd63HVFcGOR4owDUYVuia3JgZoueg/300?wxtype=jpeg&amp;wxfrom=0"/><p>IC-light V2：支持处理风格化图像“IC-Light”全称是“Imposing Consistent Light”，IC-Light 是一个操纵图像照明的项目。目前已经发布了两种类型的模型，两</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247492719&amp;idx=5&amp;sn=ef2c28606afe86bf50fd73c138202ff6&amp;chksm=fd68bd28182b21beb89ddf7f52ae25c260dc958743a1b826470b8b1ecb395c9954533849f572&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 May 2025 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>