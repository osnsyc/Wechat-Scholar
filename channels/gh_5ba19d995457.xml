<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    


























    <item>
      <title><![CDATA[DeepSeek开源多模态模型Janus-Pro的ComfyUI使用教程，文中附模型和工作流下载。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enb5zKF6gvurVAmOLrPRAhFLvb45xtbwKzvDHEfDoJNyqQeAWoiaN8o3QOQZeARnWnmvTZdNh0ABsQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍DeepSeek发布的Janus-Pro模型的ComfyUI实践教程，包含ComfyUI安装，模型下载，工作流下载等，欢迎大家一起交流学习，也欢迎添加公众号小助手加入读者交流群，一起探索</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490090&amp;idx=1&amp;sn=15975a24f6a497ab289dc082535605ff&amp;chksm=fd11c062b25aec5d580af3595184cc77a90f769852d2db7529e26abc088780680e424da7391a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 30 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[DeepSeek开源Janus-Pro-7B：多模态AI模型性能超越DALL-E 3 和 Stable Diffusion 3!]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enb5zKF6gvurVAmOLrPRAhFcaHiaiclO2A4p8xHMlrzicfJTT2YRJ96xcpxAialNYX1udoHkp8paKetdA/640?wxtype=jpeg&amp;wxfrom=0"/><p>中国人工智能公司 DeepSeek 的 R1“推理”人工智能已经引起了广泛关注，位居应用商店排行榜首位并改变了股市。随后DeepSeek又宣布开源新一代多模态模型Janus-Pro-7B，该模型在图像</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490089&amp;idx=1&amp;sn=5d6b970e167bbefefe3d5a9069994f2e&amp;chksm=fd046eaacb64ee13f8358309220e9666091b2d41f7707a4d5c8bfac669224dd975f842b1864c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 29 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[DeepSeek-V3 正式发布，已在网页端和 API 全面上线，性能领先，速度飞跃。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsVQ8iaD3eY0RibaYkYGTf6mgibWMibTiaiccjeVMM6rnIOdfQ4sLtwbuJJ1iaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>DeepSeek-V3 在推理速度上相较历史模型有了大幅提升。在目前大模型主流榜单中，DeepSeek-V3 在开源模型中位列榜首，与世界上最先进的闭源模型不分伯仲。unsetunset简介unset</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490089&amp;idx=2&amp;sn=1450aba31793a33547691c2fb3f64eff&amp;chksm=fdb35206a7a8fd4a10fed566080d52842362005e2c1f9dda4ae31838691f6426e4e343434fdb&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 29 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[单张照片生成3D头部模型！Adobe提出FaceLift，从单一人脸图像重建360度头部模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4qXFbiaEqoEPejUQNwuqLOrpIE3WmoKJBxjrMnCoHDn3huArYyaCa7Ew/300?wxtype=jpeg&amp;wxfrom=0"/><p>FaceLift是Adobe和加州大学默塞德分校推出的单图像到3D头部模型的转换技术,能从单一的人脸图像中重建出360度的头部模型。FaceLift基于两阶段的流程实现:基于扩散的多视图生成模型从单张</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490089&amp;idx=3&amp;sn=cc228ca85a55b5726e11e4760de7ccbd&amp;chksm=fd086da1e7d52f9441ecd1408151bb33bf023955d87e9cc58168dc7a04c6d5acd19c51e7aab8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 29 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一个LoRA同时处理内容和风格？UIUC提出UnZipLoRA，可同时训练两个LoRA，与原有LoRA兼容。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekK8oWGMLQzxWWfB4pkH8CCntib22WMYOGLwrnJpjeM7SCyxLnkZxmUEMEJADyvx3g4ZicOs12gOU1Q/300?wxtype=jpeg&amp;wxfrom=0"/><p> 一个LoRA可以同时处理内容和风格了？UIUC提出UnZipLoRA， 可将元素从单个图像中分离出来同时训练两个LoRA，与原有LoRA兼容。伊利诺伊大学厄巴纳-香槟分校的研究者们提出了一种将图像分</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490089&amp;idx=4&amp;sn=de4955462f5361ad77dd3016fc2b6956&amp;chksm=fd6b9fa7ae324a93f197b9f9b9f084f9c2a3dd9bd38485d87b8be89cd619767d8456c66803b1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 29 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[「感谢读者一路同行，2025一起解锁AIGC的更多惊喜！」来自AIGC Studio的新年祝福～]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enb5zKF6gvurVAmOLrPRAhFtpjLqq0QJVtln8ZXSDeWic26Br1Fh6k6LHfu2sgOLxdibBRetJFSsoBg/640?wxtype=jpeg&amp;wxfrom=0"/><p>2024年已经画上句号，感谢各位读者一年来的陪伴与支持！这一年，我们共同探索了AIGC的无限可能，分享了无数有趣、前沿的内容。每一篇文章的背后，都是对技术的热爱和对未来的期待，而你们的每一次阅读、点赞</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490088&amp;idx=1&amp;sn=be4ee50cc4cece6c840cb89d6e6b59ac&amp;chksm=fda21e8d46b1b3bae575f3b3dd554103a92fc7bccafa1c82bce3bf70c3a82953dd1005fbcd72&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 28 Jan 2025 16:11:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[英伟达开源4K图像生成模型Sana，可在16G显存电脑部署，支持ComfyUI和LoRA训练。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek6Zafxy9AicSRodyIcwlSHNT9mr6NOzfTpJPhveE41Xmh1RVMhhibAgXAt3qSb6eFx0HfpEYX74THA/640?wxtype=jpeg&amp;wxfrom=0"/><p>英伟达开源了一个可以直接生成 4K 图片的模型 Sana。 Sana-0.6B 可以在 16GB 的笔记本电脑 GPU 上部署。生成 1024 × 1024 分辨率的图像只需不到 1 秒钟。官方已经支</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490058&amp;idx=1&amp;sn=f671f319d7fd3f82c07be81aa55e5d2e&amp;chksm=fd2fa9352628b4ff1d6bf2f915de5fba7754aa2146ab0844f895cf296f9e8054e23e35342a70&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 27 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[复旦&amp;字节提出layout-to-image新范式，支持基于布局的MM-DiT架构下可控图像生成！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekXX8zYF4UxzjmCibmVsNeNfHdzvia0ykHy5vQljhxHZhBKib0DHCIdedbOAMsic8KZ423vtGia19o4Wow/300?wxtype=jpeg&amp;wxfrom=0"/><p>本篇分享论文CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490058&amp;idx=2&amp;sn=781b6dc047987911723ab4d92e23378b&amp;chksm=fd8118d00f71769b5a87ce76c4599f7c3d0195c96a246327b08ec706f264bdfc71f7019ce276&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 27 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[SHMT：通过潜在扩散模型进行自监督分层化妆转移（阿里&amp;武汉理工）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsjQl0sGle0TkYDcmMMuGmbtLXibkDVicOAa1tpYmub1EJgQJfZ41lm6WQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>当前的妆容转移技术面临两个主要挑战：缺乏成对数据，导致模型训练依赖于低质量的伪配对数据，从而影响妆容的真实感；不同妆容风格对面部的影响各异，现有方法难以有效处理这种多样性。今天给大家介绍的方法是由阿里</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490058&amp;idx=3&amp;sn=04b9f8f32aa3b7145c1821b47d4ea26b&amp;chksm=fd8a91cc96f2a2e1f3d41311d6c21bd95ff9b7cd6d1a34f76e7e1317e3ab7127dc8ba3c8fafa&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 27 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[提出街景定位大模型AddressCLIP：一张图实现街道级精度定位！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eldKGCwibmhq5RSxC5rV78dDcVpQDWZ2qUibtJW2qRF8ehlmicnuSw3n5MdOVQ0NTovfOnPib1RNDwBibQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>中科院自动化所和阿里云一起推出了街景定位大模型AddressCLIP，只要一张照片就能实现街道级精度的定位。比如给模型看一张北京南锣鼓巷的街景之后，它直接给出了具体的拍摄位置，并列举了附近的多个候选地</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490058&amp;idx=4&amp;sn=187ced6c7d7f40ec09a755763c6b9670&amp;chksm=fdbb265fbde436ab4fc4e9a040a1e60d68e1f8fe5cb9ed097502d404f8fbc9ee9218b3981df6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 27 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[单张照片生成3D头部模型！Adobe提出FaceLift，从单一人脸图像重建360度头部模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4qXFbiaEqoEPejUQNwuqLOrpIE3WmoKJBxjrMnCoHDn3huArYyaCa7Ew/640?wxtype=jpeg&amp;wxfrom=0"/><p>FaceLift是Adobe和加州大学默塞德分校推出的单图像到3D头部模型的转换技术,能从单一的人脸图像中重建出360度的头部模型。FaceLift基于两阶段的流程实现:基于扩散的多视图生成模型从单张</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490057&amp;idx=1&amp;sn=abf129eb3d79e47e5868f82fb2c27131&amp;chksm=fd9941c812f75472c02b9919c97a24ac51e9f744f06c2d95ff899d806b3168f8ee11988e2aea&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一键试衣or一键脱衣？TryOffAnyone：从人像输入中生成高质量平铺服装。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUnCMiaBjZZszIFmMq4b6eoaD7GTE6xt5Kkbz15pCGnYFBHiaXPN7qYcV79yBTU689jcMRI5dJd0g/300?wxtype=jpeg&amp;wxfrom=0"/><p>TryOffAnyone 是一种新颖的单阶段框架，旨在从穿着衣服的人的输入图像和覆盖服装区域的相应服装掩码合成高质量的平铺布料图像。在 VITON-HD 等基准数据集上实现了最先进的性能。该方法在为全</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490057&amp;idx=2&amp;sn=bc8fef13bc2549981e44048cb0ce7664&amp;chksm=fdc3bef4062c2ca60abc1de2e59d7633505ea3632ac5e653b4afaee22b45b08a5c37c1a8f94b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adobe发布TurboEdit：可以通过文本来编辑图像，编辑时间<0.5秒！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elKcprhHqENugIHSUTwb3EOiaaqictMa8fmmNEDqsoISMhGDZH4oZmh7vtMn5sov6khPdhIypPkhDZQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍Adobe研究院新的研究TurboEdit，可以通过文本来编辑图像，通过一句话就能改变图像中的头发颜色、衣服、帽子、围巾等等。而且编辑飞快，<0.5秒。简直是图像编辑的利器。相关链接项目</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490057&amp;idx=3&amp;sn=8c33be1915dfe03902e10b066e2fc7c6&amp;chksm=fdbf1488eda57bc8c82735506307c1fbdb6ce67b2ad1e96e7c7c2ee7ce0e7550abae3df32d22&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[浙大 | 腾讯 | 华为提出视频生成框架VideoMaker，可由参考图实现Zero-shot定制化视频生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekaVfDRjALdOCj5889F1MpALuqg2wbFklkt9TIVHLSyQfTQ65do3Pe4Szhc0sWs0dMVTLfiavGbvRQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>浙大联合腾讯和华为提出了一种新的定制化视频生成框架——VideoMaker，利用VDM的内在能力，实现高质量的zero-shot定制化视频生成。该方法通过直接输入参考图像到VDM中，利用其固有的特征提</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490056&amp;idx=1&amp;sn=515a850881d7ef3822914ea6560d3fe1&amp;chksm=fdca9bd4cd455e689d4221795ed3a48e51a7a8bd31df2e309570e49c5780f65d6bf29cb81bdd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 00:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一键脱衣？TryOffAnyone：从人像输入中生成高质量平铺服装。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUnCMiaBjZZszIFmMq4b6eoaD7GTE6xt5Kkbz15pCGnYFBHiaXPN7qYcV79yBTU689jcMRI5dJd0g/300?wxtype=jpeg&amp;wxfrom=0"/><p>TryOffAnyone 是一种新颖的单阶段框架，旨在从穿着衣服的人的输入图像和覆盖服装区域的相应服装掩码合成高质量的平铺布料图像。在 VITON-HD 等基准数据集上实现了最先进的性能。该方法在为全</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490056&amp;idx=2&amp;sn=c2ea44a0f6e1f07f65487d61e6412a7d&amp;chksm=fd112f0f1371c0eaf18562e06b386ac72123601926151ffbf5be041b5380504cf2629aaaf778&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 00:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekB7CXUYR45xqh1P2Q9zWuxgmicJiaO6JPkkhoaibkSARt6qftWXI9ofZjt9NK9vuibg0UrfhA2kTPRaQ/300?wxtype=jpeg&amp;wxfrom=0"/><p> 腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT今天介绍的文章来自公众号粉丝投稿，腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT，给定一个人像图像和一个衣物图像，就可以生成一个</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490056&amp;idx=3&amp;sn=c251f6741179b8376358db838c84bd6b&amp;chksm=fd4e02ee03cd7d095d3a8e55f9b6f2b344a8493200479a2c7d298e8bf5cab6b1c928d230c38b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 26 Jan 2025 00:08:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Stability AI 联合UIUC提出单视图3D重建方法SPAR3D，0.7秒完成重建并支持交互式用户编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4zxt8tZpVIWNwGIWEeh5iaYqC8uJSSWGn5hv37BppQMhwNVjgbEMmOgw/640?wxtype=jpeg&amp;wxfrom=0"/><p>Stability AI 联合 UIUC 提出一种简单而有效的单视图 3D 重建方法 SPAR3D，这是一款最先进的 3D 重建器，可以从单视图图像重建高质量的 3D 网格。SPAR3D 的重建速度很</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=1&amp;sn=5b72ce7f8c6cf6dfe853a76491eee0f6&amp;chksm=fd22c717e1473e7228e5c739a177c2666bce39b6a4d4da032608ccd5a6c4c8ef5d8bf1f49e72&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[北大提出定制化漫画生成新框架DiffSensei，可生成具有动态多角色控制的漫画图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgKtXcy3S4XBxj4sIBiacegBSAicARmN6YDuAjO6tUqgQ6TNNE8CbF3pFw/300?wxtype=jpeg&amp;wxfrom=0"/><p>由北京大学、上海人工智能实验室、南洋理工大学提出了一种新框架DiffSensei可以实现定制化漫画生成，解决现有方法在多角色场景中对角色外观和互动控制不足的问题。DiffSensei结合了基于扩散的图</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=2&amp;sn=91ab2fe971de657581119fbf62674822&amp;chksm=fdacb1e433a0cd2848fe0a808085f5ac9e709fc2854da5931ec18d031054f488c631f6353854&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[DeepSeek-V3 正式发布，已在网页端和 API 全面上线，性能领先，速度飞跃。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsVQ8iaD3eY0RibaYkYGTf6mgibWMibTiaiccjeVMM6rnIOdfQ4sLtwbuJJ1iaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>DeepSeek-V3 在推理速度上相较历史模型有了大幅提升。在目前大模型主流榜单中，DeepSeek-V3 在开源模型中位列榜首，与世界上最先进的闭源模型不分伯仲。unsetunset简介unset</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=3&amp;sn=03adf783346dd287860d17558659a1da&amp;chksm=fdb7b81e6fc16379e42c4f653aee55baa5f32697384b4c16ce412e3f76bb95d4bc23a620c02d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490027&amp;idx=4&amp;sn=ec1a6b75781ab0ae69edcebe84350dfc&amp;chksm=fd98b3a29816bca11506813a9fd296bd34fb3c13bbc70b066f2b62632f247fe404f9ad23bf8b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 24 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
