<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    



























    <item>
      <title><![CDATA[实时高保真人脸编辑方法PersonaMagic，可根据肖像无缝生成新角色、风格或场景图像。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsExUD85bPCqAicxMSnpMibzPacGZ2JJmvUibAHrezkU5Now3FS5ib9Ibg1A/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的是一个高保真实时人脸编辑方法PersonaMagic，通过分阶段的文本条件调节和动态嵌入学习来优化人脸定制。该技术利用时序动态的交叉注意力机制，能够在不同阶段有效捕捉人脸特征，从而在生</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489722&amp;idx=1&amp;sn=b2f92abfd72f30a2c97c71956ed34636&amp;chksm=fd6800c47f0c2991ded408839135c22c38b0877dfe88876c4720a8184860e8bc69b411bc71c4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sat, 04 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[厦门大学联合网易提出StoryWeaver，可根据统一模型内给定的角色实现高质量的故事可视化]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgocfw6Age4Y0U1AOuZS8cHib80ewP5RdXmZjaprD8L6TqR9iasUM3VUVQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>厦门大学联合网易提出StoryWeaver，可以根据统一模型内给定的角色实现高质量的故事可视化。可根据故事文本生成与之匹配的图像，并且确保每个角色在不同的场景中保持一致。本文的方法主要包括以下几个步骤</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489722&amp;idx=2&amp;sn=969054662185f2223e25b429951ded30&amp;chksm=fd452d58f81ba824a385b3572cbed3099c51fad658e2cadeb3ff378178e1d6072d2d09d2e009&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sat, 04 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[北航 | 第一个多功能即插即用适配器MV-Adapter：轻松实现多视图一致图像生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en3n1j1LLVnKmKxjJUkVMkfSL2lH1ru1uCJuUuA21YKHU5ia5SLlWu0BztQtHU3YSeZIYv3K9nGSHQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>北航提出了第一个多功能的即插即用适配器MV-Adapter。可以在不改变原有网络结构或特征空间的情况下增强T2I模型及其衍生模型。MV-Adapter 在 SDXL 上实现了高达768分辨率的多视图图</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489722&amp;idx=3&amp;sn=94f038194ddc92abe4f1a2f30679938e&amp;chksm=fd98c87a4016f45531a7ad0e9dbba449558ccfcfe1870393fb6e3324dfeb487548746a9b39af&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Sat, 04 Jan 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[NeurIPS 2024 | SHMT：通过潜在扩散模型进行自监督分层化妆转移（阿里&amp;武汉理工）]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsjQl0sGle0TkYDcmMMuGmbtLXibkDVicOAa1tpYmub1EJgQJfZ41lm6WQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>当前的妆容转移技术面临两个主要挑战：缺乏成对数据，导致模型训练依赖于低质量的伪配对数据，从而影响妆容的真实感；不同妆容风格对面部的影响各异，现有方法难以有效处理这种多样性。今天给大家介绍的方法是由阿里</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489721&amp;idx=1&amp;sn=2be8e8a5d80508b4133580ed51703f1a&amp;chksm=fd2089f9c33f235bd1530ac701fa2edf457dfa7f5996741fa6bde7a5c5e0a1be2da98d1d2f90&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 03 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[字节 &amp; 清华大学提出 AnyDressing ：通过潜在扩散模型实现可定制的多服装虚拟试穿。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el4eIaML40rNcaURA3WTSibqnpQ2g1NFlaI6Rqk2xKeeM63oDt2Siblvd3Z2JVicAdFicCgWdSPicLJVzw/300?wxtype=jpeg&amp;wxfrom=0"/><p> 字节&amp;清华大学提出AnyDressing:可利用参考服饰和文本定制化人物，解决多服饰组合搭配、文本响应以及服饰细节的问题。今天的文章来自公众号粉丝投稿，清华大学联合字节提出了一项虚拟试穿新方法Any</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489721&amp;idx=2&amp;sn=186a0cd6ac36bc766945ce6f3cedc137&amp;chksm=fde20e8ea21d5673292365bfcd769af84b430c7f96743357b0b2f673d8440ecc9d2b0b3e2c7c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 03 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NeurIPS2024 | OCR-Omni来了！字节&amp;华师提出统一的多模态生成模型TextHarmony。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enzia0AxvG3w5jnu5Q1nyOUw9icM2HlibUyI4RtyIsiaB9FOY9taoKCFlibTeImBZT585GC3ias7FialR6UA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在人工智能领域，赋予机器类人的图像文字感知、理解、编辑和生成能力一直是研究热点。目前，视觉文字领域的大模型研究主要聚焦于单模态生成任务。尽管这些模型在某些任务上实现了统一，但在 OCR 领域的多数任务</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489721&amp;idx=3&amp;sn=575774cf8c38a69a9736873d7de5bf2b&amp;chksm=fd29798bdf09671a5235a5555128d3e87cb629cbc56f9c9300427ea4b76098de93f2c2145dee&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 03 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Adobe发布TurboEdit：可以通过文本来编辑图像，编辑时间<0.5秒！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elKcprhHqENugIHSUTwb3EOiaaqictMa8fmmNEDqsoISMhGDZH4oZmh7vtMn5sov6khPdhIypPkhDZQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍Adobe研究院新的研究TurboEdit，可以通过文本来编辑图像，通过一句话就能改变图像中的头发颜色、衣服、帽子、围巾等等。而且编辑飞快，<0.5秒。简直是图像编辑的利器。相关链接项目</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489721&amp;idx=4&amp;sn=af85645b39b8f853a5e7a6a05dcc35b2&amp;chksm=fdd53542f4f69d819ccbddbc87fbe2abb3fe32410924817c4f74767bc6faa4258af0b17be5ed&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 03 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[DeepSeek-V3 正式发布，已在网页端和 API 全面上线，性能领先，速度飞跃。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsVQ8iaD3eY0RibaYkYGTf6mgibWMibTiaiccjeVMM6rnIOdfQ4sLtwbuJJ1iaA/640?wxtype=jpeg&amp;wxfrom=0"/><p>DeepSeek-V3 在推理速度上相较历史模型有了大幅提升。在目前大模型主流榜单中，DeepSeek-V3 在开源模型中位列榜首，与世界上最先进的闭源模型不分伯仲。unsetunset简介unset</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489720&amp;idx=1&amp;sn=37d99adfeccf039c163e40aa4e4a8d0f&amp;chksm=fd7eebeff56de5574f87630440193c3b6960cad6aaaa0ca52b3ae500f74e11a0cf370b54a6b5&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 02 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[港大&amp;Adobe提出通用生成框架UniReal：通过学习真实世界动态实现通用图像生成和编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en3n1j1LLVnKmKxjJUkVMkfcOvKg4alghJicfViaQXPN3cGVy3SYtSRiaWE0jyTlhQNs1mRy2lUoe0Pw/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的文章来自公众号粉丝投稿，由香港大学，Adobe提出的统一图像生产与编辑方法UniReal，将多种图像任务统一成视频生成的范式，并且在大规模视频中学习真实的动态与变化，在指令编辑、图像定</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489720&amp;idx=2&amp;sn=2f84d86995b73b27fb3b92f0c6b3d085&amp;chksm=fd555218281a9319d9859f19559ae0f696728154d08e5b33ed1ad959416fe244c5a001f0eb0b&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 02 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekB7CXUYR45xqh1P2Q9zWuxgmicJiaO6JPkkhoaibkSARt6qftWXI9ofZjt9NK9vuibg0UrfhA2kTPRaQ/300?wxtype=jpeg&amp;wxfrom=0"/><p> 腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT今天介绍的文章来自公众号粉丝投稿，腾讯优图提出首个基于DiT的高保真虚拟试衣算法FitDiT，给定一个人像图像和一个衣物图像，就可以生成一个</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489720&amp;idx=3&amp;sn=dc405dd99bd4f38e90ceaa21d96f66b8&amp;chksm=fd238322df37c5b711af36f45e8031c314dbf63e440249193b31245fe9025e524c2d7c90c8c3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 02 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[一个LoRA同时处理内容和风格？UIUC提出UnZipLoRA，可同时训练两个LoRA，与原有LoRA兼容。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekK8oWGMLQzxWWfB4pkH8CCntib22WMYOGLwrnJpjeM7SCyxLnkZxmUEMEJADyvx3g4ZicOs12gOU1Q/300?wxtype=jpeg&amp;wxfrom=0"/><p> 一个LoRA可以同时处理内容和风格了？UIUC提出UnZipLoRA， 可将元素从单个图像中分离出来同时训练两个LoRA，与原有LoRA兼容。伊利诺伊大学厄巴纳-香槟分校的研究者们提出了一种将图像分</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489720&amp;idx=4&amp;sn=b857982d0fee32d95048e165ebb0eae1&amp;chksm=fd111ce86c8c6a2f2826a52808e3b75c335b0db95c8ce3e42c78edd5c5476f135ce4fdb40a3c&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 02 Jan 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[小米SU7璀璨洋红限定色360°全景图首次曝光？TRELLIS给你答案，实现可扩展多功能3D生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsrcRtOibx92ZhEt1Z0UdQ7JsQibr17Y0WaD8O08DMM3XIor43XOZVMvXg/640?wxtype=jpeg&amp;wxfrom=0"/><p>清华大学、中国科学技术大学、微软研究院联合提出T RELLIS，这是一个大型 3D 资产生成模型，可根据文本或图像提示（使用 GPT-4o 和 DALL-E3）以各种格式生成高质量的 3D 资产，可在</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489669&amp;idx=1&amp;sn=36d0afe739324bc3842d4cd6008e8e25&amp;chksm=fda7a07e7f15d76ce93d38d0dfd6994d48899b4038219f37ada3fcac7e62a49127481979d5f4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 01 Jan 2025 16:12:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图像超分辨新SOTA！南洋理工提出InvSR,利用大模型图像先验提高SR性能, 登上Huggingface热门项目。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnndGUoqrp8dTsfo3BKVIVGVNf5sWoXGauJCgAEaaCQm9Qb7QfuM34qZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>南洋理工大学的研究者们提出了一种基于扩散反演的新型图像超分辨率 (SR) 技术，可以利用大型预训练扩散模型中蕴含的丰富图像先验来提高 SR 性能。该方法的核心是一个深度噪声预测器，用于估计前向扩散过程</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489669&amp;idx=2&amp;sn=0971c3716bde51b2bcf476d22295d564&amp;chksm=fd84fa5231e1f0c05fcf2364c1a5cbb27e29ba9b33215b78ffa172f856139d7edd374031df65&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 01 Jan 2025 16:12:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[复旦&amp;微软提出StableAnimator：可实现高质量和高保真的ID一致性人类视频生成]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEg0pWxDqMplvkr6CkMHxsZoRegYlaQmYz6ah0rQewI1UFbTMjpYhWh4Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>由复旦、微软、虎牙、CMU的研究团队提出的StableAnimator框架，实现了高质量和高保真的ID一致性人类视频生成。StableAnimator 生成的姿势驱动的人体图像动画展示了其合成高保真和</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489669&amp;idx=3&amp;sn=0b44d736033f14559b1a09932bffa443&amp;chksm=fd4edaa1da45948ebe5ab8734d3637a75adf646dd410ebd0c3d2987852254d4870b1a5fbab5e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 01 Jan 2025 16:12:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Qwen团队重磅上线视觉推理大模型QVQ-72B-preview，一键解答作业难题。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekN68oxVrNWcCyHfGTVIhcl50IXxo2v08icDp0bq5YEWrnzFKc8v7VMQojd7H1RZvADQEVaD0qswLw/640?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen团队推出了新成员QVQ-72B-preview，这是一个专注于提升视觉推理能力的实验性研究模型。提升了视觉表示的效率和准确性。它在多模态评测集如MMMU、MathVista和MathVisio</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489639&amp;idx=1&amp;sn=e05ef8f4853643ef7c9d4461b3727555&amp;chksm=fd40efe620a312739eaeaa1b79a8c1d11170c92df19337ba1ea78a9a7c6111b00e34934f745d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 31 Dec 2024 16:15:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图像超分辨新SOTA！南洋理工提出InvSR,利用大模型图像先验提高SR性能, 登上Huggingface热门项目。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnndGUoqrp8dTsfo3BKVIVGVNf5sWoXGauJCgAEaaCQm9Qb7QfuM34qZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>南洋理工大学的研究者们提出了一种基于扩散反演的新型图像超分辨率 (SR) 技术，可以利用大型预训练扩散模型中蕴含的丰富图像先验来提高 SR 性能。该方法的核心是一个深度噪声预测器，用于估计前向扩散过程</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489639&amp;idx=2&amp;sn=858a37051645868622e8b834e57c40f0&amp;chksm=fde65936fc2bed24136e43a9aadd8584f241d2dde89b1856e8b20344f60b6b194b07496dee69&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 31 Dec 2024 16:15:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[可控人物图像生成统一框架Leffa，可精确控制虚拟试穿和姿势转换！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnnw4qVp8X9ONMxpUQDBiaYSIRDzOCoVkXLaTPVaE68iceCFZ392Kf5RIrA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个Huggingface上虚拟试穿的热门项目Leffa，Leffa是一个可控人物图像生成的统一框架，可以精确操纵外观（即虚拟试穿）和姿势（即姿势转换）。从效果看生成效果很不错！unse</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489639&amp;idx=3&amp;sn=e3ef98419cabe86df58c3484de00526b&amp;chksm=fdbe827ff3829fe60fc41c07a1440fde34f343ff102cf5454d15b83f645398318c6faa7e48b3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 31 Dec 2024 16:15:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[MinT: 第一个能够生成顺序事件并控制其时间戳的文本转视频模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el8quKicUEibqsQFrF8ttU8UZh4icQduMVEEU78HQlZOU3Jzp7NeFwOc2OPJY7cjEn7Ed2h55fjhbhkw/640?wxtype=jpeg&amp;wxfrom=0"/><p>MinT 是第一个能够生成顺序事件并控制其时间戳的文本转视频模型。使用 MinT 生成时间控制的多事件视频。给定一系列事件文本提示及其所需的开始和结束时间戳，MinT 可以合成具有一致主题和背景的平滑</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=1&amp;sn=4cc1e9782198d97e023be49c570c5249&amp;chksm=fdc4d674c1b6c4b53c80a92b2542cf2cf7fbb46df95468b8923fd11088223614db2b56ca7c3e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[文生图像编辑来了！英伟达提出Add-it，无需训练，可根据文本提示向图像添加对象。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emmkDiagtskaHJodPFibMTUYJZY50N6JpzSdpSqpDMMdhm1JHxUv7E3vPPDa6XmXuygFoa0eiaBct3Bg/300?wxtype=jpeg&amp;wxfrom=0"/><p>Nvidia提出了Add-it，这是一种无需训练的方法，可根据文本提示向图像添加对象。Add-it 适用于真实图像和生成的图像。该方法利用现有的文本转图像模型 (FLUX.1-dev)，无需额外训练。</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=2&amp;sn=566fe4350cf67ef6d2d7a9a09060120d&amp;chksm=fda21a8a7cf22fed31266e524093c2ebc981786a431baef3e210a879af0a6a1e427ecb3df9da&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[组件可控个性化生成方法MagicTailor：生成过程可自由地定制ID。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en5zm71fQSgV6aaqPJln47UM68LBoxEpKSBewPN29AuBHv1SMicLD8losQPDWSsMKunkqyr9HuAUvA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天的文章来自公众号粉丝投稿，文章提出了一种组件可控的个性化生成方法MagicTailor，旨在个性化生成过程中可以自由地定制ID的特定组件。相关链接论文阅读：https://arxiv.org/pd</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=3&amp;sn=d8b0190b565e5a322023c87139a1d533&amp;chksm=fdd78adc07e69558a57beb4b78af0dfb5e21a7de235a8328e15c7b7c96d361f7ccfe23100038&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[InstructG2I：从多模态属性图合成图像​，结合文本和图信息生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZqBubdMgx3yBMfDK8JGL4YYX3hw4kJVRCHjFaqvVYYc7nEPXjibpCEug/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的这项工作是伊利诺伊大学厄巴纳-香槟分校的研究者们提出的一个新任务 Graph2Image，其特点是通过调节图信息来合成图像，并引入了一种名为InstructG2I的新型图调节扩散模型来</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=4&amp;sn=363221eefdc92b37e01cd8a4f6db2128&amp;chksm=fde05d77fd7f26f7275acac5b61adef6a882e141fb6d2de0f5b88968818d9ae465e44d9c5fc3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[厦门大学联合网易提出StoryWeaver，可根据统一模型内给定的角色实现高质量的故事可视化]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgocfw6Age4Y0U1AOuZS8cHib80ewP5RdXmZjaprD8L6TqR9iasUM3VUVQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>厦门大学联合网易提出StoryWeaver，可以根据统一模型内给定的角色实现高质量的故事可视化。可根据故事文本生成与之匹配的图像，并且确保每个角色在不同的场景中保持一致。本文的方法主要包括以下几个步骤</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=1&amp;sn=c782363bce96016f3e8c2f715b56e861&amp;chksm=fdf93cef46b317faa353e7283368c73339337bed7bf895b825b526dbfc7479f1e41d0da20bac&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[创作智能助手，能够根据剧本文字和对话自动检索电影并可视化！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enm3u2iayFicevODtDHfHCic2dial6Uws5O2vFicJicvmCc8DZwf1HYialKbmHHLAiarHic2SO2oS7ibXVTWHGw/300?wxtype=jpeg&amp;wxfrom=0"/><p>斯坦福大学的研究者们开发了一个电影剧本可视化工具ScriptViz工具，ScriptViz的工作原理可以简单地理解为一个智能助手，它帮助剧作家将文字变成生动的画面。比如，如果剧作家写了一个在沙漠中的对</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=2&amp;sn=332bce6460f70f0830809cde032388a3&amp;chksm=fd12098a1ab2846eb3359c0551c325d3a79bbbfbdfd82c78614c5824276e5dc56d2a8f197277&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[统一的图像生成模型OmniGen：可以根据多模态提示直接生成各种图像，无需额外插件。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enjwj4Ry2OH6auaAn9DU954RGLVLiaJQhnSsUOPiaYkiaE5VPAB4AUAtmLI24PhQm9bK4JduBhT9ZjTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个北京市人工智能研究院 提出的统一的图像生成模型OmniGen，可以使用它来执行各种任务，包括但不限于文本到图像生成、主题驱动生成、身份保留生成、图像编辑和图像条件生成。OmniGen</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=3&amp;sn=08b149dfe26bf066222cc73877b78242&amp;chksm=fd9e35388274b31250e6985f7e61a16afccd2e91f4885aaf16ebeec0bb792e620119da0511b1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GroundingBooth：一个用于文本到图像的定制框架，支持多主题和文本联合接地定制！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUN5oqyRgSButjKACUwRIxoR4VWqymzeNXHxsW4rxM6qoeicJM6XkODXXx3zP4H0duuNP0vk91Sg/300?wxtype=jpeg&amp;wxfrom=0"/><p>GroundingBooth是一个用于文本到图像的接地定制框架。首先提取文本描述和图像的特征，然后通过一种特殊的注意力机制来控制这些特征的结合。这个机制就像是一个精密的筛子，确保每个对象和背景之间的信</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=4&amp;sn=0b03970a4500375157a6e5584b4fd40e&amp;chksm=fda3dcb602f0fff45703b90e695966844e38c997b24166341ce49d4f703c41308f31ae2be3dc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[北大提出定制化漫画生成新框架DiffSensei，可生成具有动态多角色控制的漫画图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgKtXcy3S4XBxj4sIBiacegBSAicARmN6YDuAjO6tUqgQ6TNNE8CbF3pFw/640?wxtype=jpeg&amp;wxfrom=0"/><p>由北京大学、上海人工智能实验室、南洋理工大学提出了一种新框架DiffSensei可以实现定制化漫画生成，解决现有方法在多角色场景中对角色外观和互动控制不足的问题。DiffSensei结合了基于扩散的图</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=1&amp;sn=b242cd79e0d6fdde2bb684f494ff8b0a&amp;chksm=fdaa8a0bd628c63d1224ba504439f4063b0e235f7a8fcc8a38d5e52ac249747ea98e0a6d5904&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=2&amp;sn=0e5c05fa7cc769debfa2acf65dd3bcf3&amp;chksm=fd2b650114031db3e983bfa81f5e412080910de8e51cd3643b6a993cfa78ab2ee4201be135b3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里推出升级版AI翻译工具：Marco MT 性能超越Google、DeepL和ChatGPT]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekDYMeOJw6PMrPrgUmBfVvICGVGwvK1ZowHkm5otQN1GWBq1oKgOpXCvFcU6T8e0WgLCSBUqvcfmg/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里巴巴的国际业务部门于推出了一款升级版的AI翻译工具，名为Marco MT。这款工具在翻译性能上超越了Google、DeepL和ChatGPT的同类产品。该工具的目标是帮助商户更好地在全球市场销售，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=3&amp;sn=2d4b568d6b3d35777de19d5d9a4768e3&amp;chksm=fd049aa9fbcf7990ab62709c0d42b033866822b910f678b1376468cf52e23ebcef61e0a3463e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ScribbleDiff：使用涂鸦精细引导扩散，实现无需训练的文本到图像生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en4dVnOT75Vve5gBZeAMAcqnHFQnQNTu2jZ3gdtvtEhgfeuBiawdPpo4eRXb4xIj7t0TCyfMVB3Rhg/300?wxtype=jpeg&amp;wxfrom=0"/><p>ScribbleDiff可以通过简单的涂鸦帮助计算机生成图像。比如你在纸上随意画了一些线条，表示你想要的图像的轮廓。ScribbleDiff会利用这些线条来指导图像生成的过程。首先，它会分析这些涂鸦，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=4&amp;sn=4a76a1ca979610f344374ca94cb2b306&amp;chksm=fdc57379b4c76dcf849242f881eb3d3365a1d7fc30643f9a61254a1cacd48365f527801f17f4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
