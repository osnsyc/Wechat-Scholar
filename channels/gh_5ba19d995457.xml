<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    
































    <item>
      <title><![CDATA[MinT: 第一个能够生成顺序事件并控制其时间戳的文本转视频模型。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el8quKicUEibqsQFrF8ttU8UZh4icQduMVEEU78HQlZOU3Jzp7NeFwOc2OPJY7cjEn7Ed2h55fjhbhkw/640?wxtype=jpeg&amp;wxfrom=0"/><p>MinT 是第一个能够生成顺序事件并控制其时间戳的文本转视频模型。使用 MinT 生成时间控制的多事件视频。给定一系列事件文本提示及其所需的开始和结束时间戳，MinT 可以合成具有一致主题和背景的平滑</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=1&amp;sn=4cc1e9782198d97e023be49c570c5249&amp;chksm=fdc4d674c1b6c4b53c80a92b2542cf2cf7fbb46df95468b8923fd11088223614db2b56ca7c3e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[文生图像编辑来了！英伟达提出Add-it，无需训练，可根据文本提示向图像添加对象。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emmkDiagtskaHJodPFibMTUYJZY50N6JpzSdpSqpDMMdhm1JHxUv7E3vPPDa6XmXuygFoa0eiaBct3Bg/300?wxtype=jpeg&amp;wxfrom=0"/><p>Nvidia提出了Add-it，这是一种无需训练的方法，可根据文本提示向图像添加对象。Add-it 适用于真实图像和生成的图像。该方法利用现有的文本转图像模型 (FLUX.1-dev)，无需额外训练。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=2&amp;sn=566fe4350cf67ef6d2d7a9a09060120d&amp;chksm=fda21a8a7cf22fed31266e524093c2ebc981786a431baef3e210a879af0a6a1e427ecb3df9da&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[组件可控个性化生成方法MagicTailor：生成过程可自由地定制ID。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en5zm71fQSgV6aaqPJln47UM68LBoxEpKSBewPN29AuBHv1SMicLD8losQPDWSsMKunkqyr9HuAUvA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天的文章来自公众号粉丝投稿，文章提出了一种组件可控的个性化生成方法MagicTailor，旨在个性化生成过程中可以自由地定制ID的特定组件。相关链接论文阅读：https://arxiv.org/pd</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=3&amp;sn=d8b0190b565e5a322023c87139a1d533&amp;chksm=fdd78adc07e69558a57beb4b78af0dfb5e21a7de235a8328e15c7b7c96d361f7ccfe23100038&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[InstructG2I：从多模态属性图合成图像​，结合文本和图信息生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZqBubdMgx3yBMfDK8JGL4YYX3hw4kJVRCHjFaqvVYYc7nEPXjibpCEug/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的这项工作是伊利诺伊大学厄巴纳-香槟分校的研究者们提出的一个新任务 Graph2Image，其特点是通过调节图信息来合成图像，并引入了一种名为InstructG2I的新型图调节扩散模型来</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=4&amp;sn=363221eefdc92b37e01cd8a4f6db2128&amp;chksm=fde05d77fd7f26f7275acac5b61adef6a882e141fb6d2de0f5b88968818d9ae465e44d9c5fc3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[厦门大学联合网易提出StoryWeaver，可根据统一模型内给定的角色实现高质量的故事可视化]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgocfw6Age4Y0U1AOuZS8cHib80ewP5RdXmZjaprD8L6TqR9iasUM3VUVQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>厦门大学联合网易提出StoryWeaver，可以根据统一模型内给定的角色实现高质量的故事可视化。可根据故事文本生成与之匹配的图像，并且确保每个角色在不同的场景中保持一致。本文的方法主要包括以下几个步骤</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=1&amp;sn=c782363bce96016f3e8c2f715b56e861&amp;chksm=fdf93cef46b317faa353e7283368c73339337bed7bf895b825b526dbfc7479f1e41d0da20bac&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[创作智能助手，能够根据剧本文字和对话自动检索电影并可视化！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enm3u2iayFicevODtDHfHCic2dial6Uws5O2vFicJicvmCc8DZwf1HYialKbmHHLAiarHic2SO2oS7ibXVTWHGw/300?wxtype=jpeg&amp;wxfrom=0"/><p>斯坦福大学的研究者们开发了一个电影剧本可视化工具ScriptViz工具，ScriptViz的工作原理可以简单地理解为一个智能助手，它帮助剧作家将文字变成生动的画面。比如，如果剧作家写了一个在沙漠中的对</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=2&amp;sn=332bce6460f70f0830809cde032388a3&amp;chksm=fd12098a1ab2846eb3359c0551c325d3a79bbbfbdfd82c78614c5824276e5dc56d2a8f197277&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[统一的图像生成模型OmniGen：可以根据多模态提示直接生成各种图像，无需额外插件。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enjwj4Ry2OH6auaAn9DU954RGLVLiaJQhnSsUOPiaYkiaE5VPAB4AUAtmLI24PhQm9bK4JduBhT9ZjTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个北京市人工智能研究院 提出的统一的图像生成模型OmniGen，可以使用它来执行各种任务，包括但不限于文本到图像生成、主题驱动生成、身份保留生成、图像编辑和图像条件生成。OmniGen</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=3&amp;sn=08b149dfe26bf066222cc73877b78242&amp;chksm=fd9e35388274b31250e6985f7e61a16afccd2e91f4885aaf16ebeec0bb792e620119da0511b1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GroundingBooth：一个用于文本到图像的定制框架，支持多主题和文本联合接地定制！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUN5oqyRgSButjKACUwRIxoR4VWqymzeNXHxsW4rxM6qoeicJM6XkODXXx3zP4H0duuNP0vk91Sg/300?wxtype=jpeg&amp;wxfrom=0"/><p>GroundingBooth是一个用于文本到图像的接地定制框架。首先提取文本描述和图像的特征，然后通过一种特殊的注意力机制来控制这些特征的结合。这个机制就像是一个精密的筛子，确保每个对象和背景之间的信</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=4&amp;sn=0b03970a4500375157a6e5584b4fd40e&amp;chksm=fda3dcb602f0fff45703b90e695966844e38c997b24166341ce49d4f703c41308f31ae2be3dc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[北大提出定制化漫画生成新框架DiffSensei，可生成具有动态多角色控制的漫画图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgKtXcy3S4XBxj4sIBiacegBSAicARmN6YDuAjO6tUqgQ6TNNE8CbF3pFw/640?wxtype=jpeg&amp;wxfrom=0"/><p>由北京大学、上海人工智能实验室、南洋理工大学提出了一种新框架DiffSensei可以实现定制化漫画生成，解决现有方法在多角色场景中对角色外观和互动控制不足的问题。DiffSensei结合了基于扩散的图</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=1&amp;sn=b242cd79e0d6fdde2bb684f494ff8b0a&amp;chksm=fdaa8a0bd628c63d1224ba504439f4063b0e235f7a8fcc8a38d5e52ac249747ea98e0a6d5904&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=2&amp;sn=0e5c05fa7cc769debfa2acf65dd3bcf3&amp;chksm=fd2b650114031db3e983bfa81f5e412080910de8e51cd3643b6a993cfa78ab2ee4201be135b3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里推出升级版AI翻译工具：Marco MT 性能超越Google、DeepL和ChatGPT]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekDYMeOJw6PMrPrgUmBfVvICGVGwvK1ZowHkm5otQN1GWBq1oKgOpXCvFcU6T8e0WgLCSBUqvcfmg/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里巴巴的国际业务部门于推出了一款升级版的AI翻译工具，名为Marco MT。这款工具在翻译性能上超越了Google、DeepL和ChatGPT的同类产品。该工具的目标是帮助商户更好地在全球市场销售，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=3&amp;sn=2d4b568d6b3d35777de19d5d9a4768e3&amp;chksm=fd049aa9fbcf7990ab62709c0d42b033866822b910f678b1376468cf52e23ebcef61e0a3463e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ScribbleDiff：使用涂鸦精细引导扩散，实现无需训练的文本到图像生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en4dVnOT75Vve5gBZeAMAcqnHFQnQNTu2jZ3gdtvtEhgfeuBiawdPpo4eRXb4xIj7t0TCyfMVB3Rhg/300?wxtype=jpeg&amp;wxfrom=0"/><p>ScribbleDiff可以通过简单的涂鸦帮助计算机生成图像。比如你在纸上随意画了一些线条，表示你想要的图像的轮廓。ScribbleDiff会利用这些线条来指导图像生成的过程。首先，它会分析这些涂鸦，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=4&amp;sn=4a76a1ca979610f344374ca94cb2b306&amp;chksm=fdc57379b4c76dcf849242f881eb3d3365a1d7fc30643f9a61254a1cacd48365f527801f17f4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[单幅图像合成 360° 3D场景的新方法：PanoDreamer，可同时生成全景图像和相应的深度信息。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el8quKicUEibqsQFrF8ttU8UZJIaZuVsUww2Z2IDLp7MYqLaIsWuo5dAG2Y1iaHf8AibCjXj4KFPbTv3A/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文介绍了一种从单幅图像合成 360° 3D 场景的新方法。该方法以连贯的方式生成全景图及其相应的深度，解决了现有最先进方法（如 LucidDreamer 和 WonderJourney 的局限性。这</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489622&amp;idx=1&amp;sn=597d6e7ed97d103ec2143d22e33cdd2e&amp;chksm=fd20afec93b756bb1cb7b5e891da7c2129a289d868bf8624afe0ed08ed812becdaa2493f5953&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 27 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[超越DragDiffusion!哈工程联合南大提出FastDrag：可以几秒内完成基于拖动的图像编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el1M9OhKbp7aVFqDIicZqBo4KKdolgwrn8m3yECIn5VrcqNnCuNxG4ud1KNvKlkWI1InpekwGDSGUQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中给大家介绍过许多关于通过拖拽实现图像和视频编辑的方法，感兴趣的小伙伴可以点击👇链接阅读和收藏，整理不易，欢迎大家给文章点点赞和在看！StableDrag：一种基于Diffusion模型的图</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489622&amp;idx=2&amp;sn=7ac8d89777915b50aa83d078b3b850f1&amp;chksm=fd20013a5fa5a1ad363e4af3b8eff7ee73f42e920c75169872ccd305a1cac44943da0363f540&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 27 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[提出街景定位大模型AddressCLIP：一张图实现街道级精度定位！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eldKGCwibmhq5RSxC5rV78dDcVpQDWZ2qUibtJW2qRF8ehlmicnuSw3n5MdOVQ0NTovfOnPib1RNDwBibQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>中科院自动化所和阿里云一起推出了街景定位大模型AddressCLIP，只要一张照片就能实现街道级精度的定位。比如给模型看一张北京南锣鼓巷的街景之后，它直接给出了具体的拍摄位置，并列举了附近的多个候选地</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489622&amp;idx=3&amp;sn=a0f6aa7fc9640f3eaa60ad0d5e203e69&amp;chksm=fd043413ce14fd55963fb599d5b9aba61a3d415acb61c6b5cf57eff4add43ef149244676222f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 27 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[马斯克X-AI发布文生图模型Aurora，已集成到聊天机器人Grok中, 将面向所有用户开放。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enEtibxlukPrYKiah0Ke78WycRneTHUqfva8MTdTmSgCvDdsSgDibeDCo0O9j7sQLFlf1NJg4xTOdYlA/640?wxtype=jpeg&amp;wxfrom=0"/><p>千呼万唤，马斯克X-AI发布了文生图模型Aurora，并将其整合进了聊天机器人Grok中。Aurora不仅支持文本输入，还可从用户提供的图像中获取灵感，或直接编辑用户上传的图像。Aurora 是一个自</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489621&amp;idx=1&amp;sn=e8f57b95b7a63b69b24c705aa62b1d25&amp;chksm=fdbeaac08638906cdc9f8ecc535f8540f69aca8e161f96b09fda5dd895921793fd068012a3bd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 26 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[马斯克开源自家大模型Grok-1：具有314B参数，由 xAI从头开始训练！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enWzcG7CYZD52ibvUMepv0Iwdt3jHibqYyWbHkPFIjN2NntK7V7gHu8xamHsvQHCYWziazNFwTMtpZJA/300?wxtype=jpeg&amp;wxfrom=0"/><p>就在刚刚，马斯克在最后一刻如约开源了Grok，模型有314B大小，这是第一个如此规模的开源模型。如此体量直接斩获目前最大开源模型的头衔。据了解，Grok-1于2023年10月完成预训练阶段，该版本针对</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489621&amp;idx=2&amp;sn=ab2cbe1da3ae5496c60fe3b196ff4d34&amp;chksm=fdedf8b7fcc9725eeacdb74c3d47f5fd0f797441ea8e780811140b00c0f480ebd760cfcd648f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 26 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Poetry2Image：专为中文古诗词图像生成，忠于原诗意境和语义。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emh13jOMSY9oYmD0NHOx8BcYfwYJj74Cog1EPA8EQnekKhKwrxDasX2PxLvN7VqWDL8nRUrassVIw/300?wxtype=jpeg&amp;wxfrom=0"/><p>直接基于诗句中的文本进行图像生成通常会导致丢失图像中的关键元素。为了解决此问题，哈工大提出Poetry2Image，通过实施有针对性的图像校正解决这个问题，有效地捕捉这首诗所传达的语义和艺术精髓。Po</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489621&amp;idx=3&amp;sn=754ba5fc2cff8ff8631be61e8bf36d0b&amp;chksm=fd531951e5441790b6291d6199603cd4f375badd322774cd643397a695b5a6efe4dd5f6ebb70&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 26 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[「视觉AI任意门」AnyDoor，只需点两下鼠标就可以实现任意场景物体交换]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ektxTkDn4pgjVvOnwLzwyficpf10rGdcVX0iate9R5qWNUJpdHGSBNRYqI6ZaAPPyjIAwiaUiapjzicWqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>        香港大学、阿里集团、蚂蚁集团联合开源了基于扩散模型的，图像生成、控制模型——AnyDoor。AnyDoor实现了零样本的图像嵌入，主要功能是“图像传送”，点两下鼠标，就能把物体无缝「传</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489621&amp;idx=4&amp;sn=f351a616ed482da184cfbd6806d6e2db&amp;chksm=fda893596db4cb16c6fdb658db3d45a34ccb04b4c4ff05a5fa9e533c095335ff47314c9aa056&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 26 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图像超分辨新SOTA！南洋理工提出InvSR,利用大模型图像先验提高SR性能, 登上Huggingface热门项目。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnndGUoqrp8dTsfo3BKVIVGVNf5sWoXGauJCgAEaaCQm9Qb7QfuM34qZw/640?wxtype=jpeg&amp;wxfrom=0"/><p>Zongsheng Yue南洋理工大学的研究者们提出了一种基于扩散反演的新型图像超分辨率 (SR) 技术，可以利用大型预训练扩散模型中蕴含的丰富图像先验来提高 SR 性能。该方法的核心是一个深度噪声预</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489619&amp;idx=1&amp;sn=f182985655b722a6b28af1786f859571&amp;chksm=fdbc675665c75a8ea94a8f7b610b9b037cad7eb123e6362921c03687fe1e7665adf82e833a11&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 25 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯提出ViewCrafter：一张图像就可以制作影视特效和游戏画面！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elPOajsP01qNvmwKPWKzicOsEEbzBxMRtYWNicS7fSTCWsMB1YH3tWIYmKrNLzGfsI0qOR2rIwTbN4g/300?wxtype=jpeg&amp;wxfrom=0"/><p>北大和港中文联合腾讯人工智能实验室提出了 ViewCrafter，这是一种利用视频扩散模型的先验从单个或稀疏图像合成一般场景的高保真新视图的新方法。可以简单理解为将复杂的图像转换成新角度的图像版本。首</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489619&amp;idx=2&amp;sn=dc67cf591a46e2150d13b00de98b73ff&amp;chksm=fdae0c285f441ba5991e81d8edbf83059683056fc5ea8043688b9b8ede786ba3d67e7ad8e943&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 25 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[腾讯 | 中科大提出Make-It-Animatable：一秒内可将任何3D人形模型变成动画角色]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elS0nh744Xc7tB6W08RA4SgeG73PxwM4k72wVClKXaAf0yPYtOtKUjLgb02frh2Xh9vAawfNd5XlA/300?wxtype=jpeg&amp;wxfrom=0"/><p>腾讯联合中科大提出了一种用于动画 3D 角色制作的新型框架Make-It-Animatable，可以在不到一秒的时间内使任何 3D 人形模型准备好进行角色动画制作，支持各种 3D 表示且生成质量和速度</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489619&amp;idx=3&amp;sn=f110c42876046cc736ed5007fbf41cf1&amp;chksm=fd93a6797762f9f6b4034cc8c563c5ac1f1708cc1b8629eb6908a8f1ff1c2e2bc797b69ab156&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 25 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OminiControl：一个新的FLUX通用控制模型，单个模型实现图像主题控制和深度控制。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enuCwIlu7cc4lHd3hwJicoyYEn3PFyv0qTxQYEgq8VntmUj91vEEYPJjMADiamfkH94icSBs7fF1Tn1A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中和大家介绍过Flux团队开源了一系列工具套件，感兴趣的小伙伴可以点击下面链接阅读~AI图像编辑重大升级！FLUX.1 Tools发布，为创作者提供了更强大的控制能力。OminiContro</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489619&amp;idx=4&amp;sn=6e5b4127d53c33675b51b3f9f032089f&amp;chksm=fddd382a70616c2204bec85e723206b35c8f3361803c638b6088921c0828c765b039773d190e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 25 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Huggingface Trending！可控人物图像生成统一框架Leffa，可精确控制虚拟试穿和姿势转换！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnnw4qVp8X9ONMxpUQDBiaYSIRDzOCoVkXLaTPVaE68iceCFZ392Kf5RIrA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个Huggingface上虚拟试穿的热门项目Leffa，Leffa是一个可控人物图像生成的统一框架，可以精确操纵外观（即虚拟试穿）和姿势（即姿势转换）。从效果看生成效果很不错！unse</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489618&amp;idx=1&amp;sn=5650938e5dfb4b7cae5f6332248d43a8&amp;chksm=fd56b7afacd586da1bca5e2ba6ecd149324ad285ce83a3b2f13b6f115720221ed66909414938&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 24 Dec 2024 16:22:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[快手可图上线一键换衣Kolors Virtual Try-On，直冲开源项目Top 1！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enWSibOOIL5olEVr4QGiapQjV2hBCzbZIzmXNjBtLkqG3ndIHVUZLFsPYXXXTBsmL3ojSmdUZztnn5w/300?wxtype=jpeg&amp;wxfrom=0"/><p>前几天，快手可图团队在HuggingFace上面搭建了一个虚拟试衣Demo,本周该项目的火热程度已经冲到了HuggingFace的Top 1。那么Kolors Virtual Try-On 到底有什么</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489618&amp;idx=2&amp;sn=899bdd8828512e8c20fb7a8ca1be8518&amp;chksm=fd485cd11a954ad216359cf5192e06bb1a9be4127e6ed8cef9f8d1f3929131f70201ddc4c0c2&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 24 Dec 2024 16:22:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[文生图像编辑来了！英伟达提出Add-it，无需训练，可根据文本提示向图像添加对象。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emmkDiagtskaHJodPFibMTUYJZY50N6JpzSdpSqpDMMdhm1JHxUv7E3vPPDa6XmXuygFoa0eiaBct3Bg/300?wxtype=jpeg&amp;wxfrom=0"/><p>Nvidia提出了Add-it，这是一种无需训练的方法，可根据文本提示向图像添加对象。Add-it 适用于真实图像和生成的图像。该方法利用现有的文本转图像模型 (FLUX.1-dev)，无需额外训练。</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489618&amp;idx=3&amp;sn=7c9705d9e6e007ca6b69fcff90dcaad4&amp;chksm=fdae9ffee1f075d90261eb89d50d3b0a87af92faecebb0267931044a8a7810fb6c114cd26ce6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 24 Dec 2024 16:22:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Haper SD Lora: 8步就可以用 Flux-dev生成图片!]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eksy31uial7qPUM1bKGJAsI4BP97gdj3Ez3f9MI6ahElq7FZkQBowmnJAu1mBM0rOdWlon0wCb0tibw/300?wxtype=jpeg&amp;wxfrom=0"/><p>2024 年 8 月 26 日,字节开源了 FLUX Dev 的 Haper SD Lora。只需要 8 步或者 16 步就可以用 FLUX 生成图片，大幅减少 FLUX 的生成时间。建议 LoRA </p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489618&amp;idx=4&amp;sn=dfd78622d906312f68c2b0ed5955a264&amp;chksm=fd27903edaf8a39334e7be2706fbedf385c1ab86eb40a7737181d9d9ddcc4bfecb9858b24067&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 24 Dec 2024 16:22:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
