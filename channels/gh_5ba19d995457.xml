<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[机械工业出版社《AIGC驱动工业智能设备》推荐~]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekXhZDAiay6OPIoc0eYXlRohIGkuUlnCzvaKicjaicliadzAF5hrUcY83Z2RwOBKD2YSonMqVLYNdIT4A/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的书籍来自机械工业出版社的《AIGC驱动工业智能设备》，该书从基础入手，深入讲解AI技术的基本概念和原理。通过通俗易懂的讲解和示例，帮助读者建立坚实的理论基础，为后续章节的深入学习打下良</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493805&amp;idx=1&amp;sn=e011f7e6f7f12d05e3ea952941daee6c&amp;chksm=fd0488bfa724989be4c8dd714890efab7a6b5bec80a689309138ae3761d2c4f577452d8a16b0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 28 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[美团LLIA登场，让照片“开口说话”不卡顿：低延迟、高帧率，音频驱动肖像视频进入实时交互时代！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48pVRHuhqxC0rZetfMaQO9vH1icmibhHy1HIl7coV7ubJl4gO4TZRJKv4w/640?wxtype=jpeg&amp;wxfrom=0"/><p>美团提出了一种基于扩散模型的音频驱动人像视频生成框架LLIA。该方法实现了低延迟、流畅且真实的双向通信。在NVIDIA RTX 4090D显卡上，该模型在384×384分辨率下最高帧率可达78 FPS</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493790&amp;idx=1&amp;sn=366e009b9ffc8d4c7a5dc430de28732c&amp;chksm=fd4c9fc1cd1af69cd1b2a67d2907000f4012e05e139b0d617c25fc1b361cefd5d466a4883f71&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 27 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[腾讯开源 HunyuanVideo-Avatar，一张图+一段音频实现图中人物、动物甚至虚拟角色开口说话！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em4gibISNFQR95biapR4RJ7Lq5BIttmnJoy6onMGT6hEJiblmfujJkZFpZjpO6usAYRtw7aj1tZbJZYw/300?wxtype=jpeg&amp;wxfrom=0"/><p>腾讯混元团队提出的 HunyuanVideo-Avatar 是一个基于多模态扩散变换器（MM-DiT）的模型，能够生成动态、情绪可控和多角色对话视频。支持仅 10GB VRAM 的单 GPU运行，支持</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493790&amp;idx=2&amp;sn=2fef17851b32fe0303906936a0fc78da&amp;chksm=fd149a9632a0d794e229375a46821c35d7228e88629ed23f3e877af7b2c8ae29d390927296d8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 27 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI版玩具总动员！Articulate AnyMesh：开放词汇3D可动对象建模，自动给任意物体上关节然后动起来。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emlTwYmibs5btPianRd5BcicyicU9XuKiapM6UREY9FwGXMiclic5aK1IZY2K3p9ywZlqBwIYKbtdPJUg9jw/300?wxtype=jpeg&amp;wxfrom=0"/><p>由马萨诸塞大学阿默斯特分校、上海交通大学、卡内基梅隆大学以及麻省理工学院提出了一个开放词汇的3D可动对象建模框架 Articulate AnyMesh，能够以开放词汇的方式将任何刚性 3D 网格转换为</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493790&amp;idx=3&amp;sn=976a7c78a2ab0772279b873c013c4fcc&amp;chksm=fd45ed03a803c648d1ce8e2515fbc33aec0040475d261347728382814a73c1979bec1eb5c841&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 27 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI界的"六边形战士"！港科大×字节提出ComfyMind：生成/编辑/推理三连冠，开源领域再掀狂潮]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elI7B3IZQkA99hvyeKlzPzyeqYm9eaK3j5oUNFlRDs6yaz4YvOHWYMnpeWHk5ic5s7zDkXrP7RYtBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>由香港科技大学、字节跳动提出的一款基于 ComfyUI 平台构建的协作式 AI 系统ComfyMind，旨在实现稳健且可扩展的通用生成功能。在 ComfyBench、GenEval 和 Reason-</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493790&amp;idx=4&amp;sn=49423cce96d58d15d7dd9efb1d09001d&amp;chksm=fd8c20e8762120cb6c33a1899352b2b8dec74971d3388d360788ec72e059e36772d2cdb4f713&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 27 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[开源多模态生成模型新标杆！OmniGen2：支持视觉理解、文生图、图像编辑等任务，探索高级多模态生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek7H0AmSXtLibjgFibN8Hs8yrrhZa6JxHCHPbYCDGPOoQiaWTNCX0KMvXDq8E2VibCNrFhOQZicibkpSffw/640?wxtype=jpeg&amp;wxfrom=0"/><p>由北京人工智能研究院提出的 OmniGen2 是一个统一的多模态生成模型，它将强大的视觉理解、文本到图像的合成、基于指令的图像编辑以及主题驱动的上下文生成功能整合在一个框架内。它基于解耦架构，在保留高</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493789&amp;idx=1&amp;sn=ac58f5ea6bea434235c7ca2e7bb0b2a4&amp;chksm=fdd67fe2f6e5474d04f19cd431698a293ab1bcd64e56efa93c8f18bfc8b09cf4511b3cf1431f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 26 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[UniRelight：用AI重新定义光影，一张图片也能“玩转”重光照！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/B1OJ3jLyfic7KwJk2LgWQGVllkaSM8Yden54sxzolLeKOFFxwK9icp1NVTJKWB0YicQloE0ZSIvv1TppUDEibwHmGg/300?wxtype=jpeg&amp;wxfrom=0"/><p>UniRelight 是一种基于视频扩散模型的新型重光照技术，能够在单次推理中联合估计场景的反照率并合成重光照输出，显著提升了跨场景的泛化能力和视觉效果。在视频处理领域，我们是不是经常因为缺乏高质量的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493789&amp;idx=2&amp;sn=e65fa9371048f301bc4c1cea8eaa0ccc&amp;chksm=fd9562965274c906bc22cf3f5986624cc9e3ee4c15b88b0b77ffd3d401f7b26c3d4c386856de&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 26 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[文生图新架构！清华提出MADFormer！混合自回归与扩散的Transformer模型！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5Ir9WHCQBicWXDuF5MpvmWMQh4QPfVT8nXE9Tnw27035bkagHHFhhyzApmdO2oxAbKbOs56pmZG7JQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generati</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493789&amp;idx=3&amp;sn=cc8ef879ad4f49695204a467fb4019a0&amp;chksm=fdeac8a98186185d77d7d170cb7098200ccd314e56acbb73e5c74fc64ac44abbd41fb24bcd23&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 26 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[多模态任务大一统！蚂蚁推出Ming-Omni：图像、文本、语音三模态无缝融合，一网打尽复杂任务！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48J6PaQ6vvBzg7xSUic6gUlNictqMXib837dJ3ia9U0lib3hc3BMuQHGGd8fA/300?wxtype=jpeg&amp;wxfrom=0"/><p>Ming-lite-omni 是 Ming-omni 的轻量版本，源自 Ling-lite，具有28亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493789&amp;idx=4&amp;sn=4641e20e9ef06f598cd597c861a21932&amp;chksm=fde54945fe6826bb23e9f84b659454e4417f4cb6acaeb6606adad91bc5c50addc5b91e22ac62&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 26 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[超越SOTA！浙大&amp;斯坦福提出 DiffLocks，单图头发 3D 重建精度提升30%，首次支持非洲式卷发生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48OPjpkTnf2mQjjRuiawRZ5BrVMBtgVJ4QGZRnabuCSKficVh97iaqr4QzQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>由浙江大学、斯坦福大学等联合提出的DiffLocks，给定一张 RGB 图像，DiffLocks 使用扩散模型生成精确的 3D 发束。该模型基于一个包含 RGB 图像和相应 3D 发束的新型合成头发数</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493763&amp;idx=1&amp;sn=0bf1f3f786dc31b81e0847855e288227&amp;chksm=fd51247ca97c2bbfe87bb4030530f09d84032465bba763ccf1aace713c6d488d7bbb65ed9e1b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 25 Jun 2025 16:02:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[3D人脸黑科技！Pixel3DMM：单张RGB图像秒变3D人脸，姿势表情精准还原，几何精度碾压竞品15%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elXFXA8pZKAq59wibWEHiaviafiabtefYD9pHZ4MPj0OpAkqBJmnicoxT1Oib952Bqw8Vt7paicb51B2WQfw/300?wxtype=jpeg&amp;wxfrom=0"/><p>慕尼黑工业大学和伦敦大学学院提出了一款经过微调的 DINO ViT模型 Pixel3DMM，用于逐像素表面法线和 UV 坐标预测。从上到下，下图展示了 FFHQ 输入图像、估计的表面法线、根据预测的 </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493763&amp;idx=2&amp;sn=374f9b5b2b28daf20d4c0e5f8da51fcd&amp;chksm=fd6a5064474934cbf9708d754546571b75685a26cf7a2e09702d3b403017228c644eb9066a8e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 25 Jun 2025 16:02:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[谢赛宁团队提出BLIP3-o：融合自回归与扩散模型的统一多模态架构，开创CLIP特征驱动的图像理解与生成新范式!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ek5oLyjfCjICEWyMhWNvFXDN37WVtXa4JeBibibTSdNGmBP0wSFhuUAJkiaz9qNwiccNW4SuNJ7FvduuQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>BLIP3-o 是一个统一的多模态模型，它将自回归模型的推理和指令遵循优势与扩散模型的生成能力相结合。与之前扩散 VAE 特征或原始像素的研究不同，BLIP3-o 扩散了语义丰富的CLIP 图像特征，</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493763&amp;idx=3&amp;sn=d062a5824d4a2c5e5ac49b61d8aae7e2&amp;chksm=fd8bae8b28d224cb36bc812e6595032b1895d5b1ede8ed58f97e8f8c76847c7c3eac75ea017e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 25 Jun 2025 16:02:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大联合上海AI Lab提出视觉统一Diffusion架构DICEPTION！各种视觉任务一网打尽！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcb6XeOfGM7ic3jww1VGas5hyQ5UbdLhbhjcqHwrckdlwdXIvppjK9PlGZVkxMpOMiaT6tDJ32KOqiaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>数源AI 最新论文解读系列论文名：DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks论文链接：https://arx</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493763&amp;idx=4&amp;sn=c5ff46e216b20fbf10814dfed6641a19&amp;chksm=fd16369ffb227699b070236f25f240b1643557b49629436c74f5a5c2b49a123f00cd48122c56&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 25 Jun 2025 16:02:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[多模态任务大一统！蚂蚁推出Ming-Omni：图像、文本、语音三模态无缝融合，一网打尽复杂任务！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48J6PaQ6vvBzg7xSUic6gUlNictqMXib837dJ3ia9U0lib3hc3BMuQHGGd8fA/640?wxtype=jpeg&amp;wxfrom=0"/><p>Ming-lite-omni 是 Ming-omni 的轻量版本，源自 Ling-lite，具有28亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493761&amp;idx=1&amp;sn=327e2892d147905881517d651605f33a&amp;chksm=fd58910f7804665cc7e1427c6a98181ed23263280d170e407a53cb7528e63a7fa7dcc373c5da&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 24 Jun 2025 15:33:18 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[StepFun提出Step-Video-T2V！300亿参数视频生成大模型！可生成204帧视频！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcJicm2I75ZP1rkl1ZMqicoKfreYnRFLqFBbibqBpPJl9LzNL6OUXy1tmllZuicN8KGIYIbPRjfSZnnOw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名：Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model论</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493761&amp;idx=2&amp;sn=172ad152f29a13d0cc614df4f6ff5282&amp;chksm=fdea51744a0d30335d6423e7e277c9f2d770ba23696a24d760204ee894d0f20d827250e308e8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 24 Jun 2025 15:33:18 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大联合上海AI Lab提出视觉统一Diffusion架构DICEPTION！各种视觉任务一网打尽！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAcb6XeOfGM7ic3jww1VGas5hyQ5UbdLhbhjcqHwrckdlwdXIvppjK9PlGZVkxMpOMiaT6tDJ32KOqiaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>数源AI 最新论文解读系列论文名：DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks论文链接：https://arx</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493761&amp;idx=3&amp;sn=32a7e26cdaa841f1b6dc17ae83dc7e21&amp;chksm=fd00b75aa70a08f01d71e61c9ff79aa966358bb841b5e52823725da164c0b068345a3f28e2d8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 24 Jun 2025 15:33:18 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[LBM：用于图像到图像直接快速转换，支持可控照明、图像恢复、物体移除等功能！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eniaAibjBDYoftj8VvjntaLlazzrjyAuCaxtUgTmwTpbpXdlUbj1mP1pmA9QicicVlSzvQAT83J2fYzAA/300?wxtype=jpeg&amp;wxfrom=0"/><p>LBM是一种新型、多功能且可扩展的方法，它依赖于潜在空间中的桥匹配来实现快速的图像到图像转换。该方法仅使用一个推理步骤即可在各种图像到图像任务中达到最佳效果。除了效率之外，该方法在不同图像转换任务（例</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493761&amp;idx=4&amp;sn=c459c00d57cc052c08c86f1d594c6ef1&amp;chksm=fdf0720a8936003c010c7d03db8c0b59013587bda38aa67f0d23de32e341b8bb4565d0124e2e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 24 Jun 2025 15:33:18 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[TMM 2025 | 超越SOTA！AdaMesh用10秒视频生成个性化语音动画，表情生动性提升40%。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48sD3er0mq7FL0guJnKjkMSPVhPLjFIJ4elWF7POpyFVoOcRqfjLRoPw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在虚拟角色与数字人技术中，如何生成兼具真实感与个性化的语音驱动面部动画仍是关键挑战。现有方法往往依赖海量数据或通用模型，难以捕捉用户独特的说话风格（如微表情、头部动态）。为此，由清华大学深圳国际研究生</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493722&amp;idx=1&amp;sn=50c3f9227adeff4dfca60ed06c734781&amp;chksm=fd944c47d51fa331253f7dc46543c4da2886b67017deee3483730b56c6d8858a3444cdd7d45b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 23 Jun 2025 16:04:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里EMO2重磅升级！手部动作生成+超逼真表情，音频驱动人像视频生成再进化！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en9libmJyfFzq4ma8I0IqAGYiaHtTElCkzOGD9sY0N1Qp8FDJqnDN5BkTWSW0TSu1sYeAgQzRiaicMcRw/300?wxtype=jpeg&amp;wxfrom=0"/><p>在之前的文章中已经和大家介绍过阿里提出的音频驱动的人像视频生成方法EMO，感兴趣的小伙伴可以点击下面链接阅读~阿里最新EMO：只需要提供一张照片和一段音频，即可生成会说话唱歌的AI视频此外公众号的底部</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493722&amp;idx=2&amp;sn=1b35700fce90619c337670929cd351e5&amp;chksm=fd10aa0f0914f8214324b9116402e3e998924161ef50973c232c364f4e14e04925d19dc67d93&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 23 Jun 2025 16:04:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[PlayerOne横空出世：港大×达摩院重塑虚拟世界交互范式，动作捕捉驱动AAA级场景自由探索。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enfUCFX9WW23BajIFJBpRq3xvD6IHNj8gocPOicHAPyQsE13dEpzsl31yyrObIKhz86FlHOmK6LtVg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天介绍的文章来自公众号读者投稿，由香港大学与阿里达摩院联合研发的PlayerOne模型正式亮相。该技术突破传统虚拟场景构建范式，通过单张图像输入即可生成高保真动态虚拟世界，并支持用户以实时动作捕捉实</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493722&amp;idx=3&amp;sn=91262785aa5549cd654c402eed5ff9a9&amp;chksm=fd5e5b3d05d289f8aa1e0ddabb096c327fee2eaf1304d876d2cf28a2e1e1eca4244ec06289a1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 23 Jun 2025 16:04:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[3D 生成新 SOTA！SECERN AI 提出 方法 SVAD，单张图像合成超逼真3D Avatar！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elmzbxIf6OS3v7M1woTicaJczQ6xAAgVU8NYrMphwhLiaiajhcsCMja0TDYcr6RulFp9C6Yt1mtcbiamA/300?wxtype=jpeg&amp;wxfrom=0"/><p>SECERN AI提出的3D生成方法SVAD通过视频扩散生成合成训练数据，利用身份保留和图像恢复模块对其进行增强，并利用这些经过优化的数据来训练3DGS虚拟形象。SVAD在新的姿态和视角下保持身份一致</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493722&amp;idx=4&amp;sn=6a94dc88ad1a0ca7ae70e6360408d4c5&amp;chksm=fd1e9c79bcf1556e2ed52396d0e7c17952ae6698b8c4250808d28659c4550353b53c997c70c2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 23 Jun 2025 16:04:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[电商广告新利器！字节提出 DreamActor-H1，让产品与模特“一键生成”高保真交互视频。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en2EDDB4tU8uPEQUN9G5w48RPbNymvllhYib3H7VZyk223eWwAma3ovH2vTKZCY7Hg7zPurzD9kpqQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>DreamActor-H1 是一个基于扩散变换器 (DiT) 的创新框架，能够根据配对的人与产品图像生成高质量的人与产品演示视频。DreamActor-H1 基于大规模混合数据集进行训练，并结合多类别</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493721&amp;idx=1&amp;sn=fc232eceb4cf740b972e6ace540bf0c2&amp;chksm=fde8e6f3d61c5b1cc9f3b8e6ebf746e43b9fda435c77337f89c5dcc3ba4e8f790da42988fc3e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 22 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[如何使用DeepSeek进行科研图表绘制？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/vI9nYe94fsFkhhJibgYhskdb4vjUEaTlFuY2pp216d97E3UsjQZuBJkB8oBHK2OrmMP1t3zaSDLBxT6GhVGv5rQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>有时候我们写论文或者看 blog，看到别人画的很好看的结构图，觉得自己肯定画不了这么好。但是现在可以让大模型来帮我们结构图。一共需要用到两个工具：大模型、Draw.io。下面的示例会使用 Claude</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493721&amp;idx=2&amp;sn=1b6dc32b2de2032c5cb92223cb590e3d&amp;chksm=fd8dfab3a425aafcbe539086f2a332d1de3600e88d319c089caf8e839ef3d4ed51b7d35402e5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 22 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[港大&amp;Adobe联合提出图像生成模型PixelFlow，可直接在原始像素空间中运行，无需VAE即可进行端到端训练。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em57kq23EbSGQ52kUcSia6n8oTIJOicficBicZpibaJQgm7tEpQJ6psVkrLse6pjDUwqiaktvnGSEiaL6xPg/300?wxtype=jpeg&amp;wxfrom=0"/><p>香港大学和Adobe联合提出了一种直接在原始像素空间中运行的图像生成模型PixelFlow，这种方法简化了图像生成过程，无需预先训练的变分自编码器 (VAE)，并使整个模型能够端到端训练。通过高效的级</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493721&amp;idx=3&amp;sn=2a38c255968f417d1569c66c9e709743&amp;chksm=fda677b972ed81ac21350aab9ae8034898721cca28db4fc2718fd7f96e3bddb592fbf03d4f87&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 22 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR2025 | 同济提出无需训练的肖像动画框架FaceShot，让表情包、动漫人物、玩具等“开口说话”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emXysHeAOso1q4PjdgGCNECN5vlsQZr9AOKKvriaYqbhSHH5y8IBJg25HQaMqclHrVZ7Dp9ObVuiaww/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天和大家分享同济大学的最新研究FaceShot: 一举打破肖像动画模型“驱动真人”的局限，FaceShot 的动画效果可应用于各个领域的角色，包括 3D 动漫、表情符号、2D 动漫、玩具、动物等等。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493721&amp;idx=4&amp;sn=8a8aa68fc4b333e67931da9c5c047fb6&amp;chksm=fd24a7bad3dcddc39de7bda470320384ea8de354b198803efac15ee21230d477aff7b07bfa32&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 22 Jun 2025 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>