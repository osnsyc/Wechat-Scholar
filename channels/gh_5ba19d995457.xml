<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    































    <item>
      <title><![CDATA[小米SU7璀璨洋红限定色360°全景图首次曝光？TRELLIS给你答案，实现可扩展多功能3D生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsrcRtOibx92ZhEt1Z0UdQ7JsQibr17Y0WaD8O08DMM3XIor43XOZVMvXg/640?wxtype=jpeg&amp;wxfrom=0"/><p>清华大学、中国科学技术大学、微软研究院联合提出T RELLIS，这是一个大型 3D 资产生成模型，可根据文本或图像提示（使用 GPT-4o 和 DALL-E3）以各种格式生成高质量的 3D 资产，可在</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489669&amp;idx=1&amp;sn=36d0afe739324bc3842d4cd6008e8e25&amp;chksm=fda7a07e7f15d76ce93d38d0dfd6994d48899b4038219f37ada3fcac7e62a49127481979d5f4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 01 Jan 2025 16:12:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[图像超分辨新SOTA！南洋理工提出InvSR,利用大模型图像先验提高SR性能, 登上Huggingface热门项目。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnndGUoqrp8dTsfo3BKVIVGVNf5sWoXGauJCgAEaaCQm9Qb7QfuM34qZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>南洋理工大学的研究者们提出了一种基于扩散反演的新型图像超分辨率 (SR) 技术，可以利用大型预训练扩散模型中蕴含的丰富图像先验来提高 SR 性能。该方法的核心是一个深度噪声预测器，用于估计前向扩散过程</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489669&amp;idx=2&amp;sn=0971c3716bde51b2bcf476d22295d564&amp;chksm=fd84fa5231e1f0c05fcf2364c1a5cbb27e29ba9b33215b78ffa172f856139d7edd374031df65&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 01 Jan 2025 16:12:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[复旦&amp;微软提出StableAnimator：可实现高质量和高保真的ID一致性人类视频生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEg0pWxDqMplvkr6CkMHxsZoRegYlaQmYz6ah0rQewI1UFbTMjpYhWh4Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>由复旦、微软、虎牙、CMU的研究团队提出的StableAnimator框架，实现了高质量和高保真的ID一致性人类视频生成。StableAnimator 生成的姿势驱动的人体图像动画展示了其合成高保真和</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489669&amp;idx=3&amp;sn=0b44d736033f14559b1a09932bffa443&amp;chksm=fd4edaa1da45948ebe5ab8734d3637a75adf646dd410ebd0c3d2987852254d4870b1a5fbab5e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 01 Jan 2025 16:12:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[Qwen团队重磅上线视觉推理大模型QVQ-72B-preview，一键解答作业难题。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekN68oxVrNWcCyHfGTVIhcl50IXxo2v08icDp0bq5YEWrnzFKc8v7VMQojd7H1RZvADQEVaD0qswLw/640?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen团队推出了新成员QVQ-72B-preview，这是一个专注于提升视觉推理能力的实验性研究模型。提升了视觉表示的效率和准确性。它在多模态评测集如MMMU、MathVista和MathVisio</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489639&amp;idx=1&amp;sn=e05ef8f4853643ef7c9d4461b3727555&amp;chksm=fd40efe620a312739eaeaa1b79a8c1d11170c92df19337ba1ea78a9a7c6111b00e34934f745d&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 31 Dec 2024 16:15:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图像超分辨新SOTA！南洋理工提出InvSR,利用大模型图像先验提高SR性能, 登上Huggingface热门项目。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnndGUoqrp8dTsfo3BKVIVGVNf5sWoXGauJCgAEaaCQm9Qb7QfuM34qZw/300?wxtype=jpeg&amp;wxfrom=0"/><p>南洋理工大学的研究者们提出了一种基于扩散反演的新型图像超分辨率 (SR) 技术，可以利用大型预训练扩散模型中蕴含的丰富图像先验来提高 SR 性能。该方法的核心是一个深度噪声预测器，用于估计前向扩散过程</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489639&amp;idx=2&amp;sn=858a37051645868622e8b834e57c40f0&amp;chksm=fde65936fc2bed24136e43a9aadd8584f241d2dde89b1856e8b20344f60b6b194b07496dee69&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 31 Dec 2024 16:15:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[可控人物图像生成统一框架Leffa，可精确控制虚拟试穿和姿势转换！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emvRmmSX73ApBN83mPSIUnnw4qVp8X9ONMxpUQDBiaYSIRDzOCoVkXLaTPVaE68iceCFZ392Kf5RIrA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个Huggingface上虚拟试穿的热门项目Leffa，Leffa是一个可控人物图像生成的统一框架，可以精确操纵外观（即虚拟试穿）和姿势（即姿势转换）。从效果看生成效果很不错！unse</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489639&amp;idx=3&amp;sn=e3ef98419cabe86df58c3484de00526b&amp;chksm=fdbe827ff3829fe60fc41c07a1440fde34f343ff102cf5454d15b83f645398318c6faa7e48b3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 31 Dec 2024 16:15:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[MinT: 第一个能够生成顺序事件并控制其时间戳的文本转视频模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el8quKicUEibqsQFrF8ttU8UZh4icQduMVEEU78HQlZOU3Jzp7NeFwOc2OPJY7cjEn7Ed2h55fjhbhkw/640?wxtype=jpeg&amp;wxfrom=0"/><p>MinT 是第一个能够生成顺序事件并控制其时间戳的文本转视频模型。使用 MinT 生成时间控制的多事件视频。给定一系列事件文本提示及其所需的开始和结束时间戳，MinT 可以合成具有一致主题和背景的平滑</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=1&amp;sn=4cc1e9782198d97e023be49c570c5249&amp;chksm=fdc4d674c1b6c4b53c80a92b2542cf2cf7fbb46df95468b8923fd11088223614db2b56ca7c3e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[文生图像编辑来了！英伟达提出Add-it，无需训练，可根据文本提示向图像添加对象。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emmkDiagtskaHJodPFibMTUYJZY50N6JpzSdpSqpDMMdhm1JHxUv7E3vPPDa6XmXuygFoa0eiaBct3Bg/300?wxtype=jpeg&amp;wxfrom=0"/><p>Nvidia提出了Add-it，这是一种无需训练的方法，可根据文本提示向图像添加对象。Add-it 适用于真实图像和生成的图像。该方法利用现有的文本转图像模型 (FLUX.1-dev)，无需额外训练。</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=2&amp;sn=566fe4350cf67ef6d2d7a9a09060120d&amp;chksm=fda21a8a7cf22fed31266e524093c2ebc981786a431baef3e210a879af0a6a1e427ecb3df9da&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[组件可控个性化生成方法MagicTailor：生成过程可自由地定制ID。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en5zm71fQSgV6aaqPJln47UM68LBoxEpKSBewPN29AuBHv1SMicLD8losQPDWSsMKunkqyr9HuAUvA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天的文章来自公众号粉丝投稿，文章提出了一种组件可控的个性化生成方法MagicTailor，旨在个性化生成过程中可以自由地定制ID的特定组件。相关链接论文阅读：https://arxiv.org/pd</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=3&amp;sn=d8b0190b565e5a322023c87139a1d533&amp;chksm=fdd78adc07e69558a57beb4b78af0dfb5e21a7de235a8328e15c7b7c96d361f7ccfe23100038&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[InstructG2I：从多模态属性图合成图像​，结合文本和图信息生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZqBubdMgx3yBMfDK8JGL4YYX3hw4kJVRCHjFaqvVYYc7nEPXjibpCEug/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的这项工作是伊利诺伊大学厄巴纳-香槟分校的研究者们提出的一个新任务 Graph2Image，其特点是通过调节图信息来合成图像，并引入了一种名为InstructG2I的新型图调节扩散模型来</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489625&amp;idx=4&amp;sn=363221eefdc92b37e01cd8a4f6db2128&amp;chksm=fde05d77fd7f26f7275acac5b61adef6a882e141fb6d2de0f5b88968818d9ae465e44d9c5fc3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 30 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[厦门大学联合网易提出StoryWeaver，可根据统一模型内给定的角色实现高质量的故事可视化]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgocfw6Age4Y0U1AOuZS8cHib80ewP5RdXmZjaprD8L6TqR9iasUM3VUVQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>厦门大学联合网易提出StoryWeaver，可以根据统一模型内给定的角色实现高质量的故事可视化。可根据故事文本生成与之匹配的图像，并且确保每个角色在不同的场景中保持一致。本文的方法主要包括以下几个步骤</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=1&amp;sn=c782363bce96016f3e8c2f715b56e861&amp;chksm=fdf93cef46b317faa353e7283368c73339337bed7bf895b825b526dbfc7479f1e41d0da20bac&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[创作智能助手，能够根据剧本文字和对话自动检索电影并可视化！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enm3u2iayFicevODtDHfHCic2dial6Uws5O2vFicJicvmCc8DZwf1HYialKbmHHLAiarHic2SO2oS7ibXVTWHGw/300?wxtype=jpeg&amp;wxfrom=0"/><p>斯坦福大学的研究者们开发了一个电影剧本可视化工具ScriptViz工具，ScriptViz的工作原理可以简单地理解为一个智能助手，它帮助剧作家将文字变成生动的画面。比如，如果剧作家写了一个在沙漠中的对</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=2&amp;sn=332bce6460f70f0830809cde032388a3&amp;chksm=fd12098a1ab2846eb3359c0551c325d3a79bbbfbdfd82c78614c5824276e5dc56d2a8f197277&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[统一的图像生成模型OmniGen：可以根据多模态提示直接生成各种图像，无需额外插件。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enjwj4Ry2OH6auaAn9DU954RGLVLiaJQhnSsUOPiaYkiaE5VPAB4AUAtmLI24PhQm9bK4JduBhT9ZjTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个北京市人工智能研究院 提出的统一的图像生成模型OmniGen，可以使用它来执行各种任务，包括但不限于文本到图像生成、主题驱动生成、身份保留生成、图像编辑和图像条件生成。OmniGen</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=3&amp;sn=08b149dfe26bf066222cc73877b78242&amp;chksm=fd9e35388274b31250e6985f7e61a16afccd2e91f4885aaf16ebeec0bb792e620119da0511b1&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[GroundingBooth：一个用于文本到图像的定制框架，支持多主题和文本联合接地定制！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekibUN5oqyRgSButjKACUwRIxoR4VWqymzeNXHxsW4rxM6qoeicJM6XkODXXx3zP4H0duuNP0vk91Sg/300?wxtype=jpeg&amp;wxfrom=0"/><p>GroundingBooth是一个用于文本到图像的接地定制框架。首先提取文本描述和图像的特征，然后通过一种特殊的注意力机制来控制这些特征的结合。这个机制就像是一个精密的筛子，确保每个对象和背景之间的信</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489623&amp;idx=4&amp;sn=0b03970a4500375157a6e5584b4fd40e&amp;chksm=fda3dcb602f0fff45703b90e695966844e38c997b24166341ce49d4f703c41308f31ae2be3dc&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 29 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[北大提出定制化漫画生成新框架DiffSensei，可生成具有动态多角色控制的漫画图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcSnOoT1icicSWQibicicqfkyEgKtXcy3S4XBxj4sIBiacegBSAicARmN6YDuAjO6tUqgQ6TNNE8CbF3pFw/640?wxtype=jpeg&amp;wxfrom=0"/><p>由北京大学、上海人工智能实验室、南洋理工大学提出了一种新框架DiffSensei可以实现定制化漫画生成，解决现有方法在多角色场景中对角色外观和互动控制不足的问题。DiffSensei结合了基于扩散的图</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=1&amp;sn=b242cd79e0d6fdde2bb684f494ff8b0a&amp;chksm=fdaa8a0bd628c63d1224ba504439f4063b0e235f7a8fcc8a38d5e52ac249747ea98e0a6d5904&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Story-Adapter：能够生成更高质量、更具细腻交互的故事图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekx1e8oxA3YKibkhot7h9UJZSKKULxCTzezvw8wSOvf1jqib40MePuLWQamEVrmH3RC3HsKvOkJ9S3A/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前已经给大家介绍过关于故事文本生成图像的相关内容，感兴趣的小伙伴可以点击以下链接阅读~字节&amp;南开提出StoryDiffusion：生成一致的图像和视频来讲述复杂故事，图灵奖得主Yann LeCun亲</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=2&amp;sn=0e5c05fa7cc769debfa2acf65dd3bcf3&amp;chksm=fd2b650114031db3e983bfa81f5e412080910de8e51cd3643b6a993cfa78ab2ee4201be135b3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里推出升级版AI翻译工具：Marco MT 性能超越Google、DeepL和ChatGPT]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekDYMeOJw6PMrPrgUmBfVvICGVGwvK1ZowHkm5otQN1GWBq1oKgOpXCvFcU6T8e0WgLCSBUqvcfmg/300?wxtype=jpeg&amp;wxfrom=0"/><p>阿里巴巴的国际业务部门于推出了一款升级版的AI翻译工具，名为Marco MT。这款工具在翻译性能上超越了Google、DeepL和ChatGPT的同类产品。该工具的目标是帮助商户更好地在全球市场销售，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=3&amp;sn=2d4b568d6b3d35777de19d5d9a4768e3&amp;chksm=fd049aa9fbcf7990ab62709c0d42b033866822b910f678b1376468cf52e23ebcef61e0a3463e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ScribbleDiff：使用涂鸦精细引导扩散，实现无需训练的文本到图像生成。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en4dVnOT75Vve5gBZeAMAcqnHFQnQNTu2jZ3gdtvtEhgfeuBiawdPpo4eRXb4xIj7t0TCyfMVB3Rhg/300?wxtype=jpeg&amp;wxfrom=0"/><p>ScribbleDiff可以通过简单的涂鸦帮助计算机生成图像。比如你在纸上随意画了一些线条，表示你想要的图像的轮廓。ScribbleDiff会利用这些线条来指导图像生成的过程。首先，它会分析这些涂鸦，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489624&amp;idx=4&amp;sn=4a76a1ca979610f344374ca94cb2b306&amp;chksm=fdc57379b4c76dcf849242f881eb3d3365a1d7fc30643f9a61254a1cacd48365f527801f17f4&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 28 Dec 2024 16:19:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[单幅图像合成 360° 3D场景的新方法：PanoDreamer，可同时生成全景图像和相应的深度信息。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el8quKicUEibqsQFrF8ttU8UZJIaZuVsUww2Z2IDLp7MYqLaIsWuo5dAG2Y1iaHf8AibCjXj4KFPbTv3A/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文介绍了一种从单幅图像合成 360° 3D 场景的新方法。该方法以连贯的方式生成全景图及其相应的深度，解决了现有最先进方法（如 LucidDreamer 和 WonderJourney 的局限性。这</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489622&amp;idx=1&amp;sn=597d6e7ed97d103ec2143d22e33cdd2e&amp;chksm=fd20afec93b756bb1cb7b5e891da7c2129a289d868bf8624afe0ed08ed812becdaa2493f5953&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 27 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[超越DragDiffusion!哈工程联合南大提出FastDrag：可以几秒内完成基于拖动的图像编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2el1M9OhKbp7aVFqDIicZqBo4KKdolgwrn8m3yECIn5VrcqNnCuNxG4ud1KNvKlkWI1InpekwGDSGUQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中给大家介绍过许多关于通过拖拽实现图像和视频编辑的方法，感兴趣的小伙伴可以点击👇链接阅读和收藏，整理不易，欢迎大家给文章点点赞和在看！StableDrag：一种基于Diffusion模型的图</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489622&amp;idx=2&amp;sn=7ac8d89777915b50aa83d078b3b850f1&amp;chksm=fd20013a5fa5a1ad363e4af3b8eff7ee73f42e920c75169872ccd305a1cac44943da0363f540&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 27 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[提出街景定位大模型AddressCLIP：一张图实现街道级精度定位！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eldKGCwibmhq5RSxC5rV78dDcVpQDWZ2qUibtJW2qRF8ehlmicnuSw3n5MdOVQ0NTovfOnPib1RNDwBibQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>中科院自动化所和阿里云一起推出了街景定位大模型AddressCLIP，只要一张照片就能实现街道级精度的定位。比如给模型看一张北京南锣鼓巷的街景之后，它直接给出了具体的拍摄位置，并列举了附近的多个候选地</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489622&amp;idx=3&amp;sn=a0f6aa7fc9640f3eaa60ad0d5e203e69&amp;chksm=fd043413ce14fd55963fb599d5b9aba61a3d415acb61c6b5cf57eff4add43ef149244676222f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Fri, 27 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[马斯克X-AI发布文生图模型Aurora，已集成到聊天机器人Grok中, 将面向所有用户开放。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enEtibxlukPrYKiah0Ke78WycRneTHUqfva8MTdTmSgCvDdsSgDibeDCo0O9j7sQLFlf1NJg4xTOdYlA/640?wxtype=jpeg&amp;wxfrom=0"/><p>千呼万唤，马斯克X-AI发布了文生图模型Aurora，并将其整合进了聊天机器人Grok中。Aurora不仅支持文本输入，还可从用户提供的图像中获取灵感，或直接编辑用户上传的图像。Aurora 是一个自</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489621&amp;idx=1&amp;sn=e8f57b95b7a63b69b24c705aa62b1d25&amp;chksm=fdbeaac08638906cdc9f8ecc535f8540f69aca8e161f96b09fda5dd895921793fd068012a3bd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 26 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[马斯克开源自家大模型Grok-1：具有314B参数，由 xAI从头开始训练！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enWzcG7CYZD52ibvUMepv0Iwdt3jHibqYyWbHkPFIjN2NntK7V7gHu8xamHsvQHCYWziazNFwTMtpZJA/300?wxtype=jpeg&amp;wxfrom=0"/><p>就在刚刚，马斯克在最后一刻如约开源了Grok，模型有314B大小，这是第一个如此规模的开源模型。如此体量直接斩获目前最大开源模型的头衔。据了解，Grok-1于2023年10月完成预训练阶段，该版本针对</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489621&amp;idx=2&amp;sn=ab2cbe1da3ae5496c60fe3b196ff4d34&amp;chksm=fdedf8b7fcc9725eeacdb74c3d47f5fd0f797441ea8e780811140b00c0f480ebd760cfcd648f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 26 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Poetry2Image：专为中文古诗词图像生成，忠于原诗意境和语义。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emh13jOMSY9oYmD0NHOx8BcYfwYJj74Cog1EPA8EQnekKhKwrxDasX2PxLvN7VqWDL8nRUrassVIw/300?wxtype=jpeg&amp;wxfrom=0"/><p>直接基于诗句中的文本进行图像生成通常会导致丢失图像中的关键元素。为了解决此问题，哈工大提出Poetry2Image，通过实施有针对性的图像校正解决这个问题，有效地捕捉这首诗所传达的语义和艺术精髓。Po</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489621&amp;idx=3&amp;sn=754ba5fc2cff8ff8631be61e8bf36d0b&amp;chksm=fd531951e5441790b6291d6199603cd4f375badd322774cd643397a695b5a6efe4dd5f6ebb70&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 26 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[「视觉AI任意门」AnyDoor，只需点两下鼠标就可以实现任意场景物体交换]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ektxTkDn4pgjVvOnwLzwyficpf10rGdcVX0iate9R5qWNUJpdHGSBNRYqI6ZaAPPyjIAwiaUiapjzicWqA/300?wxtype=jpeg&amp;wxfrom=0"/><p>        香港大学、阿里集团、蚂蚁集团联合开源了基于扩散模型的，图像生成、控制模型——AnyDoor。AnyDoor实现了零样本的图像嵌入，主要功能是“图像传送”，点两下鼠标，就能把物体无缝「传</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247489621&amp;idx=4&amp;sn=f351a616ed482da184cfbd6806d6e2db&amp;chksm=fda893596db4cb16c6fdb658db3d45a34ccb04b4c4ff05a5fa9e533c095335ff47314c9aa056&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Thu, 26 Dec 2024 16:00:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
