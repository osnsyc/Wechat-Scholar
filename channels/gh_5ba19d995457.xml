<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[AIGC Studio]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[AIGC Studio公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      <title>gh_5ba19d995457</title>
    </image>
    <item>
      <title><![CDATA[腾讯开源 HunyuanVideo-Avatar，一张图+一段音频实现图中人物、动物甚至虚拟角色开口说话！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em4gibISNFQR95biapR4RJ7Lq5BIttmnJoy6onMGT6hEJiblmfujJkZFpZjpO6usAYRtw7aj1tZbJZYw/640?wxtype=jpeg&amp;wxfrom=0"/><p>腾讯混元团队提出的 HunyuanVideo-Avatar 是一个基于多模态扩散变换器（MM-DiT）的模型，能够生成动态、情绪可控和多角色对话视频。支持仅 10GB VRAM 的单 GPU运行，支持</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493505&amp;idx=1&amp;sn=192f1cb676680ac1a83d15098a6df491&amp;chksm=fd19a8a2b3c783792481eca39abcdb73e4c9b8f44776312af769afac0480e8a12e9ef9950796&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 20 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[6秒音频即可克隆AI语音！FLOAT数字人生成语音/口型/表情，情感同步超惊艳，文中附工作流。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elmzbxIf6OS3v7M1woTicaJcmBGicWjwiauMpFknBOofINibzHjBSIibjwDHKYvhnzulS1E2KIPicobCywA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的 FLOAT 是一种基于流匹配的音频驱动的说话肖像视频生成方法，可以增强语音驱动的情感运动。该方法唇形同步质量高，生成速度还很快。6秒音频完美生成语音/口型/表情。情绪转移由于 FLO</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493505&amp;idx=2&amp;sn=dd438da6df94216717c29c1e27d81417&amp;chksm=fd635dffacf389be8dfa57e48a72233ac69ee978378b2879554c240d89cbdfd170ea450c1dec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 20 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[开源数字人克隆神器HeyGem：1秒视频生成4K超高清AI形象，用AI重塑数字人创作生态！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elzodISUKsiaVtsAvhTQ7mRrgxstWFTNfP8vOAkR5RI8GOy83ObgNDrZJL0p3TTnAIBViacS7PlySow/300?wxtype=jpeg&amp;wxfrom=0"/><p>在虚拟形象与数字内容需求激增的当下，传统3D数字人制作的高昂成本（动辄数十万美元）与复杂流程，让许多行业望而却步。而今天，一款由Duix.com团队打造的开源AI项目HeyGem，正以颠覆性技术打破这</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493505&amp;idx=3&amp;sn=cd549827e46ad001a6cf9b863bd62dfd&amp;chksm=fd2421513f0013e32a1c21c12ccb1130de7072158c9577117a863ecb11c542b9192f6304bd75&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 20 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[加利福尼亚大学提出TULIP！视觉-语言模型的新王者！AI性能全面碾压CLIP！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5KsLicuyg3oA0dGOnwBictNTE782KtlqwlaVEmKrVyKAO0YzauujiaGWFqaYjHzZqKD5rLk8dQLKZtEg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：TULIP: Towards Unified Language-Image Pretraining论文链接：https://arxiv.org/pdf/2503.15485开源</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493505&amp;idx=4&amp;sn=5fcb5d89678ab6a2588f377a71b17a86&amp;chksm=fd632de2cd7242c0e4de70e0848d100a02cd66f6f44634129fd70ae974e23a25d68b1150a75c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 20 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[PlayerOne横空出世：港大×达摩院重塑虚拟世界交互范式，动作捕捉驱动AAA级场景自由探索。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enfUCFX9WW23BajIFJBpRq3xvD6IHNj8gocPOicHAPyQsE13dEpzsl31yyrObIKhz86FlHOmK6LtVg/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天介绍的文章来自公众号读者投稿，由香港大学与阿里达摩院联合研发的PlayerOne模型正式亮相。该技术突破传统虚拟场景构建范式，通过单张图像输入即可生成高保真动态虚拟世界，并支持用户以实时动作捕捉实</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493504&amp;idx=1&amp;sn=2a1317de9928f83ef6684a66995d2d8d&amp;chksm=fd2923ee7c7c853d47c4e8ac9a8f96fd2cf54bbd8a12efcbebdd606c81b3cfa579ca4ab4b09d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 19 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦联合百度发布Hallo4：让AI肖像“活”起来！新型扩散框架实现高保真音频驱动动画生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2em4gibISNFQR95biapR4RJ7Lq56s1kIaYWsxKESfb9riaHUQVlW3JfPib9AP6mL8Hk0Ec5R0f43HYJ8aw/300?wxtype=jpeg&amp;wxfrom=0"/><p>复旦联合百度发布扩散框架Hallo4，实现了准确的唇音同步、自然的面部表情，并能够稳健地处理各种角色身份和环境场景中快速的语音节奏和突然的上身运动。相关链接论文：https://arxiv.org/p</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493504&amp;idx=2&amp;sn=9be5793da41f19e0b244c36280adeee7&amp;chksm=fd31034ec8502b4043e11ed3f27a27dcaf1ca16c58bd92e848e733e65a6bfbeff53baa8c7dd2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 19 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 字节提出个性化多人图像生成新方法ID-Patch，可生成多人合影、姿势可控。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emCuicERoV3guOMh64VYNrcA6VO1uBfS3aIicTCtKS3eFEBxCVDPwXCyj0Fye0L4toEplkN73YiaibibFw/300?wxtype=jpeg&amp;wxfrom=0"/><p>相信扩散模型（DMs）大家一定都不陌生了，目前已经成为文本生成图像的核心方法，凭借强大的图像生成能力，正重塑艺术创作、广告设计、社交媒体内容生产格局。现在，用一段文字生成个性化头像都不算啥新鲜事儿了。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493504&amp;idx=3&amp;sn=54634b736df796435a94901240aa3a11&amp;chksm=fd27535cb515d93d0d3fefd2e6e600ec0295643e3195f67aa42370e6e92e0cf62aa358775fd9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 19 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[腾讯混元&amp;InstantX开源InstantCharacter，跨角色外观、姿势和风格个性化生成。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2eloBQe14a8ohz069lCGESt2mVMulTo5LC5G2oFcJtOgsuJWSCokK4anUcgT9xP5mIuHTqbM9wOvIw/300?wxtype=jpeg&amp;wxfrom=0"/><p>腾讯混元联合InstantX团队提出全新角色定制生图框架 InstantCharacter，与当前的SoTA方法GPT4o取得了相当的结果，然而，GPT4o并未开源。相比之下，InstantChara</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493504&amp;idx=4&amp;sn=e1afe7cc00faf1a30ca9437dc0646003&amp;chksm=fdfbe9ba9a684cfbd005c4fff4124310510f18c22527121fa7ebdced9fb656e2d8fa5158efe2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 19 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[从单口相声到群口辩论：中山大学开源MultiTalk：多角色对话生成SOTA模型，语音-视觉对齐精度达98.7%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enfUCFX9WW23BajIFJBpRq3lz9nCs5icOy90Hv0zVbmIjdyTsfJWWDS7Fo3ugfyXkMKIEyJEtsAoHg/640?wxtype=jpeg&amp;wxfrom=0"/><p>由中山大学、美团、港科大开源的 MultiTalk 可实现多虚拟人对话视频生成。在语音与嘴形同步方面达到了SOTA性能，并支持通过prompt实现人物、物体与场景的交互。相关链接主页：https://</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493501&amp;idx=1&amp;sn=5506451de844d40841eb5372602369ef&amp;chksm=fd61a013ba2e967a16bc8e560b4453e0b2ad004134a67bc3d36681874dbf9e01ca1739d59811&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 18 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[6秒音频即可克隆AI语音！FLOAT数字人生成语音/口型/表情，情感同步超惊艳，文中附工作流。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elmzbxIf6OS3v7M1woTicaJcmBGicWjwiauMpFknBOofINibzHjBSIibjwDHKYvhnzulS1E2KIPicobCywA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的 FLOAT 是一种基于流匹配的音频驱动的说话肖像视频生成方法，可以增强语音驱动的情感运动。该方法唇形同步质量高，生成速度还很快。6秒音频完美生成语音/口型/表情。情绪转移由于 FLO</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493501&amp;idx=2&amp;sn=bac846f6e2578e64257273e7d74dca29&amp;chksm=fd81ea54433dc1b92c49436a20933d66de4e3ad5547b2cf24f7de9ca8ff8bd4a8affad931595&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 18 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里&amp;北邮提出基于Wan2.1的音频驱动数字人FantasyTalking，只需输入肖像、语音和文字即可生成动画。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en6YOGtn3XXJMye1oxLXOtQU407yMKvXQv0r7RibwF9tY6RoaWiaXTsGib66ALF1tYibibzZZ51ibfTjibCA/300?wxtype=jpeg&amp;wxfrom=0"/><p>由高德地图、阿里巴巴、北邮联合提出首个基于Wan2.1的音频驱动数字人FantasyTalking，只需输入肖像图像、语音和文字，即可生成表情丰富、肢体动作自然且具有身份特征的动画肖像。此外，Fant</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493501&amp;idx=3&amp;sn=afe2a09d2797b059b1ed1e056f556f16&amp;chksm=fd30b484b88569dd163ec1d7ce9646b538b1b1c3df5465c6a5be8699cf08550cc4c971131111&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 18 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[谷歌研究院联手牛津大学推出Bolt3D！7秒内单GPU生成高保真3D，推理成本直降300倍！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5LR8w1T4XSJwAUg3UkzLpMRYxbTOuSXUEpxZVs5u18QTNFMFHe41E6SY6vfhMbJicRDetQWdibB3Nicg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最新论文解读系列论文名：Bolt3D: Generating 3D Scenes in Seconds论文链接：https://arxiv.org/pdf/2503.14445开源代码：https:/</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493501&amp;idx=4&amp;sn=afc434ddc560712c7fea75eaff9a3b13&amp;chksm=fd705d1ab4817d8addb3504d012894bfbfe096c632a8b324d2cca852aab7edc113350d9c820a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 18 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[统一建模，多人共演！字节提出 MAGREF，引领多主体视频生成新范式！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekCRGvUysqBs1VAaWXnVRNrzRItticzjVhjWUd81tRcCfAejhD1dM2vojtia7GHOFvP4WWK2wzNNN8w/640?wxtype=jpeg&amp;wxfrom=0"/><p>在生成式AI的浪潮中，视频生成正成为继图像与文本后的下一个爆发点。然而，当前主流技术仍困于“单主体、纯人像”的简单场景，面对多主体互动、复杂物体与动态背景交织的现实任务时，往往暴露出三大顽疾：人物身份</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493452&amp;idx=1&amp;sn=c76c1534b7f033ec9185384ee435504b&amp;chksm=fd66fa1232310c278ec571e993ceddd545c7782f21e20a1dc3cdd7f621ddc45a578859dc2416&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 17 Jun 2025 16:21:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI版玩具总动员！Articulate AnyMesh：开放词汇3D可动对象建模，自动给任意物体上关节然后动起来。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emlTwYmibs5btPianRd5BcicyicU9XuKiapM6UREY9FwGXMiclic5aK1IZY2K3p9ywZlqBwIYKbtdPJUg9jw/300?wxtype=jpeg&amp;wxfrom=0"/><p>由马萨诸塞大学阿默斯特分校、上海交通大学、卡内基梅隆大学以及麻省理工学院提出了一个开放词汇的3D可动对象建模框架 Articulate AnyMesh，能够以开放词汇的方式将任何刚性 3D 网格转换为</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493452&amp;idx=2&amp;sn=ad99d5369667f669386479b7259c2f31&amp;chksm=fd8308507c0e52be20bfdfdaaf21ae4c80bbf6cb438c775f5b51ab944ab848fd3e9aacb1dd2c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 17 Jun 2025 16:21:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节、港理工提出超强统一视觉生成模型 Many-for-Many，支持10+任务，8B参数“逆袭”商业视频生成引擎。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emb4MEj35KfTUoB1FWsgTXr1okRdYbDMkiaMBJd2BP7Rly0sBNFZKib2sPFdbSs7MvfFpF6hn5uyKnw/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节、港理工提出超强统一视觉模型 Many-for-Many，如何凭它让 8B 模型“逆袭”商业引擎？字节跳动与香港理工大学提出统一框架 Many-for-Many，它借助众多视觉生成和操作任务的训练</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493452&amp;idx=3&amp;sn=8c425d294c67aea4cb7e1c5b25ddd708&amp;chksm=fd8a9deb181741b838944e7fc8c205a2eff31877ae70bb4219cd71dca2a2ef04146fd8373dd0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 17 Jun 2025 16:21:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI界的"六边形战士"！港科大×字节提出ComfyMind：生成/编辑/推理三连冠，开源领域再掀狂潮]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elI7B3IZQkA99hvyeKlzPzyeqYm9eaK3j5oUNFlRDs6yaz4YvOHWYMnpeWHk5ic5s7zDkXrP7RYtBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>由香港科技大学、字节跳动提出的一款基于 ComfyUI 平台构建的协作式 AI 系统ComfyMind，旨在实现稳健且可扩展的通用生成功能。在 ComfyBench、GenEval 和 Reason-</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493452&amp;idx=4&amp;sn=e7fb28424358da185f65d2810a644d4b&amp;chksm=fd8aeb6e43566e2f0977d4b4c26d984d68fbaba1bf156760bb3d63b778c4a443167fbf6b2695&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 17 Jun 2025 16:21:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 北大提出FreeCloth：面向复杂人体衣物建模的自由生成方法，宽松服装实现高保真细节保留。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekZpjibRlqzZJvpnCtIusibumvetVwHLtQfibNRV5NCjOk9fMZ0qoTnYiblqn55YqgfyU1srg7kyibibCXA/640?wxtype=jpeg&amp;wxfrom=0"/><p>由北京大学王亦洲课题组提出的名为 FreeCloth 的基于点云的混合式人体衣物建模方案，突破了现有方法在宽松衣物建模中的技术瓶颈。针对宽松衣物与骨骼运动关联性较弱的特点，该方法采用无约束自由生成网络</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493408&amp;idx=1&amp;sn=383bbc36ae1dc2202a02fe99bca11d26&amp;chksm=fd2d37b2f93df38948be04d5e11747cc489e5ff29e220b876f7bd5f3805ff3d6c203e085e407&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 16 Jun 2025 22:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[统一图像生成模型OmniGen：可由多模态提示直接生成各种图像。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enjwj4Ry2OH6auaAn9DU954RGLVLiaJQhnSsUOPiaYkiaE5VPAB4AUAtmLI24PhQm9bK4JduBhT9ZjTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一个北京市人工智能研究院 提出的统一的图像生成模型OmniGen，可以使用它来执行各种任务，包括但不限于文本到图像生成、主题驱动生成、身份保留生成、图像编辑和图像条件生成。OmniGen</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493408&amp;idx=2&amp;sn=9a628976c12d835f70c8544cd8a69d0e&amp;chksm=fd7ed31d55560641868413a174ba431c37e6d59d2fcfda9526f50f30c42a6fe90a78ce2ba459&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 16 Jun 2025 22:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[东京大学 | Adobe 提出InstructMove，可通过观察视频中的动作来实现基于指令的图像编辑。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsa4uWnhrMawFt9HHkxP0mNsA8WZRJb5wtxFQzRMjAicAjmryxF8Yliamw/300?wxtype=jpeg&amp;wxfrom=0"/><p>InstructMove是一种基于指令的图像编辑模型，使用多模态 LLM 生成的指令对视频中的帧对进行训练。该模型擅长非刚性编辑，例如调整主体姿势、表情和改变视点，同时保持内容一致性。此外，该方法通过</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493408&amp;idx=3&amp;sn=e4dab6f4efe034289ce404bf465ef8f8&amp;chksm=fd192d574255e77b6fec107eb45791414324710f754140cf24d488d6bc87cae0e9fd66810fed&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 16 Jun 2025 22:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[多人视频生成技术新突破！清华提出 DanceTogether：从单张图像到多人互动视频生成，从此告别身份混淆。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekZpjibRlqzZJvpnCtIusibumwGUpiatyy4T61q0VfQegTtcgO14OAic3MrV5T58OOrJYGw4FwLfASNmw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在人工智能与计算机视觉领域，视频生成技术一直是研究的热点与难点。特别是多人互动视频的生成，要求系统能够在复杂多变的场景中，精准地捕捉并再现多个角色的动作、姿态以及他们之间的交互细节。由清华、北大、中科</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493407&amp;idx=1&amp;sn=a2c2472cf92816b2b8ce0a91bb5d20c4&amp;chksm=fdec519b1672387ba3890f4813b85607a9a9ddf7f6baf2f38f436dad9fcec95f38811566352d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 15 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Google DeepMind 发布最强视频生成工具 Veo 3, 可为作品添加音效、环境噪音、对话，文中附体验链接。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elcnWs2mR9uePicbSxmgsNGYEOvC44lWnQUfBAMbv2Kgy7vDib4ee4tlF1R091cfagJqdQWc10PdkUA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天跟大家介绍谷歌的视频生成模型 Veo 3，可为作品添加音效、环境噪音甚至对话，所有音频均可原生生成。它还能提供一流的音质，在物理效果、真实感和快速响应方面均表现卓越。相比 Veo2 的改变Veo </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493407&amp;idx=2&amp;sn=bc675a376adc0bdfe84803c5d0b2adad&amp;chksm=fdfa9b614304686f7eb3b3acd152f90ca6e779cc9008ec2d12067b164f873db0643acbf6d64a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 15 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 机器人双臂操控新突破！KStar Diffuser如何解决自碰撞与运动约束世纪难题？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icojvz9clmicqUEHWru0TSQwicibDxwd6pXeiac1QbZwUoibnWeMnE5ib2jBibpdEXVK5T4bFCwMWWK9BcS4dg/300?wxtype=jpeg&amp;wxfrom=0"/><p>文章链接：https://arxiv.org/pdf/2503.10743亮点直击与现有方法仅在笛卡尔空间中优化末端执行器姿态不同，提出了一种新颖的时空机器人图，显式地建模机器人物理配置，以指导生成动</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493407&amp;idx=3&amp;sn=2e1c0744a6c50addbd0eeb81fe49d488&amp;chksm=fd4e2fe848446d9914e4698b44869b3016a7b261fc8c731e15eac0583158ae2998f19816b9e7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 15 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[AI版玩具总动员！Articulate AnyMesh：开放词汇3D可动对象建模，自动给任意物体上关节然后动起来。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emlTwYmibs5btPianRd5BcicyicU9XuKiapM6UREY9FwGXMiclic5aK1IZY2K3p9ywZlqBwIYKbtdPJUg9jw/640?wxtype=jpeg&amp;wxfrom=0"/><p>由马萨诸塞大学阿默斯特分校、上海交通大学、卡内基梅隆大学以及麻省理工学院提出了一个开放词汇的3D可动对象建模框架 Articulate AnyMesh，能够以开放词汇的方式将任何刚性 3D 网格转换为</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493406&amp;idx=1&amp;sn=baa8fc1cad5ac3601c4e11fa342db56d&amp;chksm=fda9f952fef08d694b80517b1fd50f678c97c64f6071cbc0c4715acc7d1ef84bf110a5570b92&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 14 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里开源语音黑科技！SenseVoice：50+语言识别、听懂你的情绪，速度超Whisper 15倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/CibEZ9gjHpIoYgiabFF6GEeGdauiaiaTxCoCBH01vZHfNlwJYrs133ibV4nls7DmjxUv6LlXeqVgrBZwcgJnicZlFQSA/300?wxtype=jpeg&amp;wxfrom=0"/><p>------语音识别的新高度，情感与事件尽在掌握在人工智能飞速发展的今天，语音识别技术已成为人机交互的核心入口。阿里巴巴通义实验室开源的语音理解模型——SenseVoice，将语音识别技术推向了全新的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493406&amp;idx=2&amp;sn=5742f3d0099c08c56ba2598aad1bf5af&amp;chksm=fd3ca68aad07e1a22442190f6d99baa6f25aca6bf3eeff5152eecca7a3bb24463ae3ba81b759&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 14 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[字节推出统一多模态模型 BAGEL，GPT-4o 级的图像生成能力直接开源了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elzodISUKsiaVtsAvhTQ7mRre72SQ3NTx8amQXBMt77z295uWjzKl5kweQFLEMa31vXicZ35AvS4Lfw/300?wxtype=jpeg&amp;wxfrom=0"/><p>字节推出的 BAGEL 是一个开源的统一多模态模型，他们直接开源了GPT-4o级别的图像生成能力。（轻松拿捏“万物皆可吉卜力”玩法~）。可以在任何地方对其进行微调、提炼和部署，它以开放的形式提供与 G</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493406&amp;idx=3&amp;sn=47017d61274d9037b431e2a1a4347134&amp;chksm=fde0e141c74deb04d75c77aae9cfc8bf66dc42629b8eb999275b9ee8b9ce6520db8398db5f14&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 14 Jun 2025 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[不是P图！用ComfyUI复原老照片，像素级重生太惊艳了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/ACyQFjNqyE62umia43diaibQHp3ufyS7wiaxIEibtXWGdYtMcl71rnkX5MWXjibQMdn9RUbu6m0NMMfLoryHo7Tqzluw/300?wxtype=jpeg&amp;wxfrom=0"/><p>过去的一张张老照片，承载着无数回忆，也记录着一个时代的光影。但随着时间的流逝，那些泛黄、破损、模糊的老照片正一点点被遗忘。幸运的是，AI图像处理的浪潮正悄然改变这一切。而在这股浪潮中，一个名字正在悄然</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247493406&amp;idx=4&amp;sn=edeca6b4e2cdaf3f2887d5796ac80616&amp;chksm=fd8a05690a2dc357028325892e4a51eebab916f5b13427b889ae70a003cbdf35b501361ef825&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 14 Jun 2025 16:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>