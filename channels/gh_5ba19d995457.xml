<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AIGC Studio]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AIGC Studio公众号]]></description>
    

    <language>zh-cn</language>
    

    <image>
      

      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_5ba19d995457.jpg</url>
      

      <title>gh_5ba19d995457</title>
      

    </image>
    






























    <item>
      <title><![CDATA[DeepSeek将开启大模型免费潮？ChatGPT和文心一言相继宣布全面免费开放！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2ekAuUNNvuhqhCqoRTbKeLmgGsbjAiciaibfrfRhuw7eaB9AtkL5GxPITtVb9wtxXb6szDDbLGD55syWg/640?wxtype=jpeg&amp;wxfrom=0"/><p>近期DeepSeek等开源AI模型的爆火，引发了全球AI产业的变革。文心一言和ChatGPT也即将全面免费开放，这将有助于推动人工智能技术的普及和应用，进一步促进产业的发展和创新。在北京时间2月13日</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490266&amp;idx=1&amp;sn=9fe290a425ce0e4903a8323ebf7ea604&amp;chksm=fd2bbdb66194ea52eff9f7baddfda2f02ed07110c541014620a85f9e50146d4bae5749d4727d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 13 Feb 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[告别大头娃娃，东京大学开源数字人TANGO：能根据目标语音音频生成同步全身手势的视频。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2en6pFnDNvWHLiaftE66MEoWpW84gLODaQtSQ9LfJlyThsRQM3P6M7VVqstMXBRlYemO0FShZxQIGrA/300?wxtype=jpeg&amp;wxfrom=0"/><p>目前已经有很多面部和唇形同步的数字人项目了，但大多只支持头像和上半身，前几天介绍的Hallo2音频驱动图像生成视频小伙伴们都非常关心，后台也有留言问有没有支持全身视频生成的方法。开源EMO再升级！复旦</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490266&amp;idx=2&amp;sn=d0db2c119ec25d0c37388b65f8ac92dc&amp;chksm=fdba2b30f008829bbd532ebdfd78ff52d4fef4bfca20bb435a700df1203043a3688a1d357090&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 13 Feb 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[港中文联合清华提出AnyCharV：高保真高细节角色可控视频生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5J2uS3m424ZibViaoytDWCdJhZ3HK9hPriaPHP70Ofg2kf5zrzqhH2vmvWpUicsK0SYeGVicnWbOq62DjA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今日论文推荐论文名：AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance论文</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490266&amp;idx=3&amp;sn=085984010d8b40a3b5fba79025489096&amp;chksm=fdefc5da01414f29761a556faf18d4de1afce51e8b25127bd02cf68410fce490d543a795d2f0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 13 Feb 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[耶鲁大学联合Adobe提出SynthLight：智能重塑人像照明，打造完美光影！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elPyLquFq9rYTicjFkPwyh9fFVDfMwbeuJFlesWohTUXxZRSXxUpCJVwUUib0mdhjaia5sa6Ciaibic8AQg/300?wxtype=jpeg&amp;wxfrom=0"/><p>耶鲁大学和Adobe提出一种用于人像重新照明的扩散模型SynthLight，该方法将图像重新照明视为重新渲染问题，其中像素会根据环境照明条件的变化而变化。在真实肖像照片上可以产生逼真的照明效果，包括颈</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490266&amp;idx=4&amp;sn=545c126998e416657b5166013f3ef554&amp;chksm=fd6681203c6ec2f0519a63696b8f5c12df367c0aa52725299adb6ec87a634a9420e858be325e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Thu, 13 Feb 2025 16:00:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[港中文&amp;腾讯提出可穿戴3D资产生成方法BAG，可自动生成服装和配饰等3D资产如，并适应特定的人体模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enXFFuKUPQcYLlG3aibBhJtNoLAAyMTFkEzKyIa4peJNFk30ibR1KmzayDxAZ4t0fRa9UibEn0G6XJ0w/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一种名为BAG（Body-Aligned 3D Wearable Asset Generation）的新方法，可以自动生成可穿戴的3D资产，如服装和配饰，以适应特定的人体模型。BAG方法</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490229&amp;idx=1&amp;sn=c428b4685830e1a98602786dc6c84b18&amp;chksm=fd1e11660a577f0aac2ea9813053163d95136c4a19f205d1535f8a271a5703217b39223db486&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 12 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[革新在线购物体验：CatV2TON引领虚拟试穿技术新纪元。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elPyLquFq9rYTicjFkPwyh9fqoMCzGFkTKibqfibbp0PHsEMaCzm6IAgmnBoZwnb3S50vNq1UoP83sbQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>在这个数字化飞速发展的时代，图像与视频合成技术正以前所未有的速度重塑着我们的生活，尤其在在线零售领域，一场关于购物体验的革命正在悄然上演。想象一下，无需亲自试穿，仅凭一张照片或一段视频，就能精准预览任</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490229&amp;idx=2&amp;sn=9ae9e2c7a44df9bb67a14762acbc3a5d&amp;chksm=fdbbecc2c315349a7d546bce769ff50bacbf3818c4c1c63ea6051d57ac6db5bacc1c1abce9f9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 12 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NVIDIA提出虚拟试衣新方法EARSB，让时尚与科技完美融合！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2empB05GMsROweibZLQdTRdryhxs0ChYaSb8Q7MXgpODuC675ClEZrLFicPt5eWjzKjxOHWcsP2ic7rBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在数字化浪潮席卷全球的今天，科技正以前所未有的方式融入我们的生活，包括我们追求时尚的方式。想象一下，无需亲临实体店，只需轻点屏幕，就能轻松试穿心仪的衣物，这不再是遥不可及的梦想。NVIDIA联合波士顿</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490229&amp;idx=3&amp;sn=a8a84fc72fb44aa3085fbb65a70147cf&amp;chksm=fd435b3c55e400c9ad64920f6fc5a532af14e262a707672ca0fee51afb0da58d0dfc078e6dff&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 12 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[耶鲁大学和Adobe提出SynthLight：智能重塑人像照明，打造完美光影！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elPyLquFq9rYTicjFkPwyh9fFVDfMwbeuJFlesWohTUXxZRSXxUpCJVwUUib0mdhjaia5sa6Ciaibic8AQg/300?wxtype=jpeg&amp;wxfrom=0"/><p>耶鲁大学和Adobe提出一种用于人像重新照明的扩散模型SynthLight，该方法将图像重新照明视为重新渲染问题，其中像素会根据环境照明条件的变化而变化。在真实肖像照片上可以产生逼真的照明效果，包括颈</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490229&amp;idx=4&amp;sn=16e813e6d44447c6000f8dca111c9139&amp;chksm=fd4e89e5737bf0a737483a87bd6d659e9c31eff367e2200224425d69713444e4e031b83fb7ff&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Wed, 12 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[人类运动生成新方法MoMask：可将文本描述作为输入并生成相应的高质量人体运动动作]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enScf5fx9rxa1dXnvYHW4G8O9hft4BsqOTmwYEXsfgumPYvUYjCh2XTNsWpfHC7OEh6xL16D8nmMA/640?wxtype=jpeg&amp;wxfrom=0"/><p>该图展示了 MoMask （一种最先进的人体运动生成模型）生成的运动示例。MoMask 使用文本到运动范式进行操作，其中它将文本描述作为输入并生成相应的高质量人体运动。这种方法确保生成的动作准确反映给</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490228&amp;idx=1&amp;sn=19b3c9ba252ad0d07148128bc5f5aa4c&amp;chksm=fdf70680bf0571d6163112ff669911a1dd0ec6162d47d8ab359380dcedfbd65776a4bea8a742&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 11 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[清华联合腾讯提出全模态模型Ola！图像、视频和音频等多模态理解一网打尽！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Hcib3tqia6H5ILy9I4g9e9dFh1achfYgv3N1UvxyxaO5FN5kwaEYDvzicXohJyGkQPum02iaiaiaoIyUCYuw4f7VG7cQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名：Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment论文链接：</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490228&amp;idx=2&amp;sn=a3af62c9dd1a45eb995e69bc6cd10cc4&amp;chksm=fd00cafc16120b2463a6dd4d6f0ee0a5e58d0aaf69592cd58274bc74c845817669524b8fe0f3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 11 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Chirpy3D：用于创意 3D 鸟类生成的连续部分潜在特征]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4lMln2LbWadvjMkDrI7ibrh99jvyryugwPj1qqcWjQZFyvLicjUXMCIicg/300?wxtype=jpeg&amp;wxfrom=0"/><p>Chirpy3D框架可以将细粒度的2D图像理解提升至3D生成的全新境界。当前的3D生成方法往往只关注于重构简单的对象，缺乏细致的特征和创造性。Chirpy3D通过结合多视角扩散模型和连续的部件潜在空间</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490228&amp;idx=3&amp;sn=7980eeaa613fb1fba24be854721bc06f&amp;chksm=fd7c5f3f230d391bbd5c3cab341b5204d6f21c2357e545aaca248866bc526a51396de9a2c45f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 11 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Meta提出Fast3R！多视角快速3D重建新SOTA！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/Scy8opQtXAdQicl6tFbm7xyZxj3Q91AjRBzA4Vrr5253RCiabI6uiaibZMIdTyoq6TZdXe8AotLRlkE2pkJyqGQLfQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名：Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass论文链接：https://arxiv.org/pdf/</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490228&amp;idx=4&amp;sn=eafbc7ca8d9def73cb3c182479209587&amp;chksm=fdf2856ff279622584e966c1bd5978647c62874ab8cb8c059d0bffe318f39467a23b2493a958&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 11 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[IC-Portrait：打造逼真个性化肖像的新纪元！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emrL9coT0EQdTjZR7WCoOG6tZI0cwKFL48OjADTdSoL7AChsEwU2ZueiasNW7Rv9TMPIZMsldYJjmg/640?wxtype=jpeg&amp;wxfrom=0"/><p>在数字内容创作、虚拟形象、游戏和增强现实等领域，肖像生成已成为计算机图形学研究的热点。尽管近年来肖像生成模型取得了显著进展，能够生成越来越逼真和吸引人的肖像，但仍面临诸多挑战。今天，给大家介绍一种个性</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490227&amp;idx=1&amp;sn=4c8d6f7e9e4aa08b77fd445fe3ccf812&amp;chksm=fd4a78a86a2a6e45bcc916d55bb2f436d9666d1a91989265d0c45a3064cfdd7db267173555ae&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 10 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[耶鲁大学和Adobe提出SynthLight：智能重塑人像照明，打造完美光影！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elPyLquFq9rYTicjFkPwyh9fFVDfMwbeuJFlesWohTUXxZRSXxUpCJVwUUib0mdhjaia5sa6Ciaibic8AQg/300?wxtype=jpeg&amp;wxfrom=0"/><p>耶鲁大学和Adobe提出一种用于人像重新照明的扩散模型SynthLight，该方法将图像重新照明视为重新渲染问题，其中像素会根据环境照明条件的变化而变化。在真实肖像照片上可以产生逼真的照明效果，包括颈</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490227&amp;idx=2&amp;sn=deec88b17e3d47367002c25d7b6e7cd7&amp;chksm=fd96be9a032d9a4e854f790f03aeb2e61f74a982f3066062c925cfb3e8a9b9b0e04d6ef22560&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 10 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[单张照片生成3D头部模型！Adobe提出FaceLift，从单一人脸图像重建360度头部模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4qXFbiaEqoEPejUQNwuqLOrpIE3WmoKJBxjrMnCoHDn3huArYyaCa7Ew/300?wxtype=jpeg&amp;wxfrom=0"/><p>FaceLift是Adobe和加州大学默塞德分校推出的单图像到3D头部模型的转换技术,能从单一的人脸图像中重建出360度的头部模型。FaceLift基于两阶段的流程实现:基于扩散的多视图生成模型从单张</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490227&amp;idx=3&amp;sn=efd8f26069f2088957be78ca2e9ef957&amp;chksm=fd9a510aca7188840791df8674d14b1a4c840c9d6bfdbf82431bdcbd7a9b94e504afeb0f6882&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 10 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[东京大学 | Adobe 提出InstructMove，可通过观察视频中的动作来实现基于指令的图像编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsa4uWnhrMawFt9HHkxP0mNsA8WZRJb5wtxFQzRMjAicAjmryxF8Yliamw/300?wxtype=jpeg&amp;wxfrom=0"/><p>InstructMove是一种基于指令的图像编辑模型，使用多模态 LLM 生成的指令对视频中的帧对进行训练。该模型擅长非刚性编辑，例如调整主体姿势、表情和改变视点，同时保持内容一致性。此外，该方法通过</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490227&amp;idx=4&amp;sn=21bf2e5bfff2d6de084c503a2b964839&amp;chksm=fd3fa8d7be2c2b4a0b6804d4c1ede22f2d7f8cf91f1c9cb4deb765c38278fb43adc92127d90a&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 10 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[字节提出OmniHuman-1！单阶段pose加音频驱动的高保真人类视频生成！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enXFFuKUPQcYLlG3aibBhJtNuoThzcAksCGkicZfZSNcArNfk65YfeJYhQ2o3mdHianYL1zZEXu8x96g/640?wxtype=jpeg&amp;wxfrom=0"/><p>今日论文推荐论文名：OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models论文链接</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490226&amp;idx=1&amp;sn=608b4939a48bcb508412ebc1fa9becb0&amp;chksm=fd052212fb19a6e2ffa68dfd8fd4a1afae06d23e765a8c1bd29381ad5472c4076095c51c5fe3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 09 Feb 2025 22:03:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Chirpy3D：用于创意 3D 鸟类生成的连续部分潜在特征]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4lMln2LbWadvjMkDrI7ibrh99jvyryugwPj1qqcWjQZFyvLicjUXMCIicg/300?wxtype=jpeg&amp;wxfrom=0"/><p>Chirpy3D框架可以将细粒度的2D图像理解提升至3D生成的全新境界。当前的3D生成方法往往只关注于重构简单的对象，缺乏细致的特征和创造性。Chirpy3D通过结合多视角扩散模型和连续的部件潜在空间</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490226&amp;idx=2&amp;sn=1a38ccad0f400fc346cf125e03599885&amp;chksm=fda7abae1dc68a3e5f22c2c9732e799d17c25c2f183913ac614f6cbb1a14472dee84ab2219af&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 09 Feb 2025 22:03:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[实时高保真人脸编辑方法PersonaMagic，可根据肖像无缝生成新角色、风格或场景图像。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsExUD85bPCqAicxMSnpMibzPacGZ2JJmvUibAHrezkU5Now3FS5ib9Ibg1A/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍的是一个高保真实时人脸编辑方法PersonaMagic，通过分阶段的文本条件调节和动态嵌入学习来优化人脸定制。该技术利用时序动态的交叉注意力机制，能够在不同阶段有效捕捉人脸特征，从而在生</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490226&amp;idx=3&amp;sn=200286e4c3cd2bdde35a6f7011dfeb7f&amp;chksm=fd333d0fdc392064cc446876139e68736746d865524765a3d2f8523261ef9865566cc19238e6&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 09 Feb 2025 22:03:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[拖动图像编辑再升级！北大、腾讯提出DragonDiffusion，在扩散模型上启用拖动式操作。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emtHS7t5ic0uQWb1AOhKNDRVue2DuQtBzYWulYezda8dsItznwxGaPWCUmiafhNgHsVonAIj1dwlFlA/300?wxtype=jpeg&amp;wxfrom=0"/><p>之前的文章中给大家介绍过许多关于通过拖拽实现图像和视频编辑的方法，感兴趣的小伙伴可以点击👇链接阅读和收藏，整理不易，欢迎大家给文章点点赞和在看！StableDrag：一种基于Diffusion模型的图</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490226&amp;idx=4&amp;sn=88cfffcdf38504818e19e6331c508cd9&amp;chksm=fdac1cd8781f054582ac49d45c24c1ada2666e2114a8abf09ac928f7401749204649d18646fd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sun, 09 Feb 2025 22:03:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Qwen2-VL 的重大省级，Qwen 发布新旗舰视觉语言模型 Qwen2.5-VL]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enScf5fx9rxa1dXnvYHW4G8iaWo76jAeFqh1St3THUd4ZMibeYibUfBbYiagkPp1f58SuW1VC9MmItVRg/640?wxtype=jpeg&amp;wxfrom=0"/><p>Qwen2.5-VL 是 Qwen 的新旗舰视觉语言模型，也是上一代 Qwen2-VL 的重大飞跃。Qwen2.5-VL主要特点视觉理解事物：Qwen2.5-VL不仅能够熟练识别花、鸟、鱼、昆虫等常见</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490173&amp;idx=1&amp;sn=31bffc9bad39ceb1f54b0ce6dbc2e731&amp;chksm=fd11df21af1fbe64daa0386e14862582586b152eeae0add0df48ab5ceecd41dbb96732ad9556&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[单张照片生成3D头部模型！Adobe提出FaceLift，从单一人脸图像重建360度头部模型。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elbUxtWfuPV6pAhibibicT3oe4qXFbiaEqoEPejUQNwuqLOrpIE3WmoKJBxjrMnCoHDn3huArYyaCa7Ew/300?wxtype=jpeg&amp;wxfrom=0"/><p>FaceLift是Adobe和加州大学默塞德分校推出的单图像到3D头部模型的转换技术,能从单一的人脸图像中重建出360度的头部模型。FaceLift基于两阶段的流程实现:基于扩散的多视图生成模型从单张</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490173&amp;idx=2&amp;sn=0199d8b1a114fcd5d88de86465e0960f&amp;chksm=fdd64d5a720f1280353dfc7177c28525ce88e1c67f7cd42e235dd58f9320f54c46c6405be15e&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LineArt：无需训练的高质量设计绘图生成方法，可保留结构准确性并生成高保真外观。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emtHS7t5ic0uQWb1AOhKNDRVgJ9ReODibzN7pbSG8HiaFPKVEMD45h5nQHic2uaYSruibSNfMdcXWxiclIA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天给大家介绍一种无需训练的基于扩散模型的高质量设计绘图外观迁移方法LineArt，该方法可以将复杂外观转移到详细设计图上的框架，可促进设计和艺术创作。现有的图像生成技术在细节保留和风格样式一致性方面</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490173&amp;idx=3&amp;sn=24c92fcabd3f5197c4cabd11e452d2a2&amp;chksm=fdc92a006e3b93a6d2b449161e596142b0b38c03c1e00496a3b282af069e99d6dfd53299efd8&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[东京大学 | Adobe 提出InstructMove，可通过观察视频中的动作来实现基于指令的图像编辑。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2emeYg29ZW9ZRFeXmWsX2FIsa4uWnhrMawFt9HHkxP0mNsA8WZRJb5wtxFQzRMjAicAjmryxF8Yliamw/300?wxtype=jpeg&amp;wxfrom=0"/><p>InstructMove是一种基于指令的图像编辑模型，使用多模态 LLM 生成的指令对视频中的帧对进行训练。该模型擅长非刚性编辑，例如调整主体姿势、表情和改变视点，同时保持内容一致性。此外，该方法通过</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490173&amp;idx=4&amp;sn=c5aad0ca76dcaff2a70a3df2e84e5f17&amp;chksm=fd04bf6f41e47995a588200eec88b1b132388e2aa74985699c9e55b576572c5fd25f8ccb075f&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 16:00:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[StochSync：可在任意空间中生成高质量360°全景图和3D网格纹理]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2enXFFuKUPQcYLlG3aibBhJtN7xgJCpXZE7HoaWiahrDNLktV0doUSl1wRalx4MZej02YkgNsTVfSbpg/640?wxtype=jpeg&amp;wxfrom=0"/><p>StochSync方法可以用于在任意空间中生成图像，尤其是360°全景图和3D网格纹理。该方法利用了预训练的图像扩散模型，以实现zero-shot生成，消除了对新数据收集和单独训练生成模型的需求。St</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490172&amp;idx=1&amp;sn=45b7f6a2f2a3d9f0eb39c0e41cfeb6dd&amp;chksm=fd3e36b8b4fb8973601d6a500c5b70eac52eff17c551d1547ba52f09e8348c953027ca1bebc7&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 00:24:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[ICLR 2025 | One Prompt One Story！基于单个prompt实现免训练身份一致图像生成]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/mkhictoa3icognghe3s2zmv0icZcyxgqgI3NLkqZwT7nwmu7Vhm09bnPXDdOSPHOs0ayh4Ol9tvfbOZITeNhKS94w/300?wxtype=jpeg&amp;wxfrom=0"/><p>点击下方卡片，关注“AI生成未来”文章链接：https://arxiv.org/abs/2501.13554GitHub代码：https://github.com/byliutao/1Prompt1S</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490172&amp;idx=2&amp;sn=d7ee6772c5e13db4b80b8984c8dd92da&amp;chksm=fd7e8e705eb49da180cd8dd99a54328e38ba4f8f653ee7cee070e0be13178bd1f901e7264e42&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 00:24:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[革新在线购物体验：CatV2TON引领虚拟试穿技术新纪元。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2elPyLquFq9rYTicjFkPwyh9fqoMCzGFkTKibqfibbp0PHsEMaCzm6IAgmnBoZwnb3S50vNq1UoP83sbQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>在这个数字化飞速发展的时代，图像与视频合成技术正以前所未有的速度重塑着我们的生活，尤其在在线零售领域，一场关于购物体验的革命正在悄然上演。想象一下，无需亲自试穿，仅凭一张照片或一段视频，就能精准预览任</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490172&amp;idx=3&amp;sn=a5ae27e88baada4a50e89262748ab01d&amp;chksm=fd30db2d0c72be302f5341e99cabddb827862d55789ec16a6db983ec70b85733cd9ea821c008&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 00:24:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[NVIDIA提出虚拟试衣新方法EARSB，让时尚与科技完美融合！]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/0YFkDxAK2empB05GMsROweibZLQdTRdryhxs0ChYaSb8Q7MXgpODuC675ClEZrLFicPt5eWjzKjxOHWcsP2ic7rBA/300?wxtype=jpeg&amp;wxfrom=0"/><p>在数字化浪潮席卷全球的今天，科技正以前所未有的方式融入我们的生活，包括我们追求时尚的方式。想象一下，无需亲临实体店，只需轻点屏幕，就能轻松试穿心仪的衣物，这不再是遥不可及的梦想。NVIDIA联合波士顿</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MzU2OTg5NTU2Ng==&amp;mid=2247490172&amp;idx=4&amp;sn=588f85dc4deb1a257d895c205c0d55d0&amp;chksm=fd390777c16041571611056abf40f60a2204f787cf88501f019d56d07d1c3ef4464a0f616e14&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 08 Feb 2025 00:24:00 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
