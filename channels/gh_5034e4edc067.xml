<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  

  <channel>
    

    <title><![CDATA[AINLP]]></title>
    

    <link>https://mp.weixin.qq.com/</link>
    

    <description><![CDATA[AINLP公众号]]></description>
    

    <language>zh-cn</language>
    





















    <item>
      <title><![CDATA[一文讲清LLM大模型x知识图谱最新SOTA方案]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSL4VuvWZs0KTbSGmqnSLaXAcnVCVnfjHX3XOuQLBazTqpLNpI8syE6gCXqUxR4ZXadOJheCahVJIA/640?wxtype=jpeg&amp;wxfrom=0"/><p>大型语言模型 (LLMs)，正在自然语言处理和人工智能领域掀起新的浪潮。然而，LLMs经常因为其幻觉问题，以及缺乏可解释性而受到批评。首先，它们在专业垂直领域的知识方面仍然不足。其次，生成大模型容易产</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=1&amp;sn=850bdf735de06eda073c15ae558555a8&amp;chksm=bff7f0b40db0bb0c4279083cde2d3d50e56a46912fd8def9b8aa840a20eb70de69cc9c9a69c9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[理解Attention:从起源到MHA,MQA和GQA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/Aj0FZbibW467C3t0CCgtKNWCDpCejE8aB0LWo53Piazn1clniaiaauhtyM4ElLdqKyibKicFxDeF1Rc2mG5EwdPBKI4A/300?wxtype=jpeg&amp;wxfrom=0"/><p>Attention模块是现在几乎所有大模型的核心模块，因此也有很多工作致力于提升注意力计算的性能和效果。其中MHA（Multi-Head Attention）、MQA（Multi-Query Atte</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=2&amp;sn=d3cc0c76e0ee30a5bc1261b7266cd144&amp;chksm=bfc925eb413c98d3e1f642a67b831194c65a01af3fd5b5d2bc5f759c43b2411869a33420d84e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[LLM之Agent初探]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/h4lbevcvkgyRsPAEfBC9kxPibdNC4jicXEhjJHQwmeu9HYJcT33onXBEh0jJKcpT3mBxVtcrLjTUM224wE4cwTDw/300?wxtype=jpeg&amp;wxfrom=0"/><p>Agent是什么？Agent一词起源于拉丁语中的Agere，意思是“to do”。在LLM语境下，Agent可以理解为在某种能自主理解、规划决策、执行复杂任务的智能体。Agent并非ChatGPT升级</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=3&amp;sn=63656a3a44a6e6cc0ecd4eb2a45171fa&amp;chksm=bf9cdcb2fa2575291c4cfb72c0baeef2274b68dae96b93bc9eafc10e6261668f5df20b9b408a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[【北京实习】百度自然语言处理部｜ERNIE大模型实习]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSKmkYpddjDlYm3A8yKEIqTmUPVfbNXBePHbMO9ib1epdWD0ic8UY0H2o1NNSGoAicmxiaic2GUY44krjGw/300?wxtype=jpeg&amp;wxfrom=0"/><p>【北京实习】百度自然语言处理部｜ERNIE大模型实习团队介绍：百度文心（ERNIE）模型团队致力于预训练大模型基础技术的研究和应用，在预训练大模型领域具备深厚的技术积累。文心ERNIE自2019年诞生</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=4&amp;sn=a24841cf0fead0c37b0d38867c44fdd6&amp;chksm=bf6eceb37c46ed9e86695575809635a177e9010291a98be4c934f7c5276937439fb5a36d0140&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      <title><![CDATA[一张图系列 - "speculative decoding"]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/2KUCQBHkydl3fhJTJJSGxKByYqY1nVlibIvbbA1nUggZNaKI7kU9FWicH8mZ3UFaibmnPBVHMbNzElAEtGeeYQKiaw/300?wxtype=jpeg&amp;wxfrom=0"/><p>目录：算法逻辑自我测试反思总结部分截图算法逻辑投机采样(Speculative Sampling)算法的整体流程和核心逻辑:1. 生成 draft tokens    - 使用一个较小的自回归模型(d</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441282&amp;idx=5&amp;sn=c90b7fc7d61d00370b6fdac498188102&amp;chksm=bf7f07c4dc5ce6d2ac2819dec251a5265ceb4e19c1a8a058396da003d6deb0c9fe48ca9d2dce&amp;scene=0&amp;xtrack=1#rd</link>
      <pubdate>Wed, 13 Mar 2024 04:10:00 +0000</pubdate>
    </item>
    <item>
      

      <title><![CDATA[YOLOv9来了！目标检测新SOTA]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSKK6oV4h2vcpmibSlD1gElyCMiaG3AOzMeRk7FLicW7MibAaU98aKRJxXftpM0TiafseQDAian9p91g3IXg/640?wxtype=jpeg&amp;wxfrom=0"/><p>目标检测新SOTA：YOLOv9来了！本文系统梳理YOLOv1-YOLOv9全系列论文及代码，以及YOLO12大必备项目。还有三大CV顶会CVPR、ECCV、ICCV 85篇目标检测论文与代码；以及2</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441265&amp;idx=1&amp;sn=1e2c225ea469333a33b01e5f32a24c6e&amp;chksm=bf29850282752a03d056c9a4939a303af67fa8257c811766e711849a40b8b1f122a4721221fd&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 12 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[图解Mixtral 8 * 7b推理优化原理与源码实现]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/GmyBmIxnRkOzufcyGhc6P8q4tff1nUsfdPKjVMuDABnLrQPHaXdic9kmmZn8EnPN7fIRTr28jxy28rL3bicseHibQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，在写这篇文章时，本来是想打算介绍Mixtral 8 * 7b具体模型架构的。但是代码读着读着就发现：最精彩的MoE部分，其相关原理在之前的文章中已经详细介绍过整体来看Mixtral 8 * 7</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441265&amp;idx=2&amp;sn=bc602f952fe18fc714d939ac1d7a7844&amp;chksm=bffc34c882bebcfc8a5cb44bde203ed1c0788098c7708f7c9c96e897a1ecfb010dc96f69d7be&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 12 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM 模型量化推理速度评测]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/2KUCQBHkydks0ndyyGKhwF92MC7L9tLyQuf3s9kUTzueStrTcPqIkr2ArYvUDboZiaIj9NIaXwXmfSKMicVfqCuA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近了解了下些常见的推理和加速方案：量化方案：gptq、quantization、int8、int4、AWQ、Speculative Decoding、GGUFAttention加速方案：atten的</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441265&amp;idx=3&amp;sn=b079424552ce6de478a74525c0530e04&amp;chksm=bf87877743dd66b6e966f8a5053c1755e4e45e37070b33c884fed145ec720b7d295617995aa3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 12 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[自我蒸馏方法-减轻大模型微调过程中的灾难性遗忘]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5nguoIx5q8CXRwYVKTfVDOqrllGvuLHEkKGiaLsAMObaKXHf3q8gqgfAdiaPJ2xQ8Y5txFkMvMGxDMQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>大模型在指定任务上进行微调后，会取得较为不错的效果，但同时可能带来模型原有能力的下降。今天给大家带来一篇通过自我蒸馏减轻大模型微调时的灾难性遗忘的方法-SDFT（Self-Distillation F</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441265&amp;idx=4&amp;sn=7563d6b4cc4728df6141c7ccc18b9e4f&amp;chksm=bfca04ee273536568df5056bf39828d551992f51e1042ef066821ecada3d85901b57785acb37&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 12 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LM-Combiner：通过模型改写实现更精准的语法纠错]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/58FUuNaBUjrSKEPdYcHU2uibHHX4HULTmG8xzHhdsKgrwFZO5OUNtl10NPtXfWUxrqb7qib5JsvlN84IdhqLkBKA/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文名称：LM-Combiner: A Contextual Rewriting Model for
Chinese Grammatical Error Correction论文作者：王一轩，王宝鑫，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441265&amp;idx=5&amp;sn=e0d4d39bbf00d80a01c262adecd65806&amp;chksm=bf69330e963a228d10d0ae4dae63c6e9fc8fcd224845d604babc004ea4dc99e7375fe37c18e3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Tue, 12 Mar 2024 04:10:00 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[没房贷的下属太可怕了。。。]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/icmWrEONNM8WOku27Jc5jokUP1nIqsV1SnZq7Gwg42OYupRCgPH6xblqEzCfAmuuEdLC1nTntqiaT7fxRRxjggew/640?wxtype=jpeg&amp;wxfrom=0"/><p>在微博上看到一个热搜话题：没房贷的下属太可怕了。一些高赞评论说到了大家的心坎里，三无青年无所畏惧。什么是三无青年？无车贷、无房贷、无后代，如此，也就没有太多软肋。随着越来越多的00后步入职场，满足如此</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441253&amp;idx=1&amp;sn=a535c78dffbdb454fe12e0333a3461e0&amp;chksm=bfa7df0b62c9149a920158b9c8251cf7d458bc1bc49e8d0653fe8bd7e812f6df578095dc8a04&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 11 Mar 2024 11:33:52 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[大模型的智慧之源：图技术的崛起 | 文末赠书]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSKK6oV4h2vcpmibSlD1gElyCrHNE5pgWz3B4t7DVticZetwfQf89BicWU7vkSTvdvgk7mQfk9crr9f8g/300?wxtype=jpeg&amp;wxfrom=0"/><p>--文末赠书--自2023年以来，大语言模型（Large Language Models，LLMs）的兴起已经改变了科技行业的面貌。科创公司如果不涉足这一领域，似乎就不好意思称自己是科技企业。然而，随</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441253&amp;idx=2&amp;sn=06e867331fa89902c9e4860115735128&amp;chksm=bf4ae165517570f00ebdf73e627fd73e9937717289c46298bd15221793991a052f20a8d67d39&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 11 Mar 2024 11:33:52 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[当LLM面对囚徒困境，阁下又该如何应对？]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/DHibuUfpZvQeR43UtykbrGXoHC7TkRpfFZbPeT7x3Nd6CMC4WYb0l8VACBzWAXDphxAGKLWWjBRBgqs7Ak91FPQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>作者 | Conqueror712 https://zhuanlan.zhihu.com/p/682698121大家好，这里是 NewBeeNLP。今天分享 LLM 在博弈论框架下的 战略决策能力 。</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441253&amp;idx=3&amp;sn=fc20222e22ffd531087baade2c6dfe50&amp;chksm=bf48b38522fc0980f76c00fec66583bf9e291cf733126e4054a27b438e4b41e8b31b284cbfd3&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 11 Mar 2024 11:33:52 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[阿里云人工智能平台招聘算法研究实习生]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/nW2ZPfuYqSKK6oV4h2vcpmibSlD1gElyCx9D2502HKFanrY3ow5k222srMMtS1Yxnp06cEVLia3qABUdK8icP7JPw/300?wxtype=jpeg&amp;wxfrom=0"/><p>算法研究实习生（base杭州，可直通秋招转正）我们是阿里云人工智能平台（PAI）深度学习团队，专注于人工智能算法及框架开发，已经开发并且开源了面向NLP的框架EasyTransfer和EasyNLP，</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441253&amp;idx=4&amp;sn=af3945c63b0e1b14b3e78698bc492246&amp;chksm=bfe824a98db71bf69a519126ecfbf21f3d5b497bfa9d46f48c398e93de048e6b8ea7d43eeeef&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 11 Mar 2024 11:33:52 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[融合RL与LLM思想，探寻世界模型以迈向AGI/ASI的第一性原理反思和探索「RL×LLM×WM>AI4S>AGI>ASI」]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/yToxjhYT5ibZnMJpW8Clke62STYD7zYM9zibhTVWp2k8Rj1SfwMQIklGdGxtNPIRTkpp9sq1yWem5Sgfyhjco94g/300?wxtype=jpeg&amp;wxfrom=0"/><p>本篇文章与2023年底尝试挖掘并探寻以chatGPT为代表的LLM和以AlphaGO/AlphaZero及当下AlphaDev为代表的RL思想的背后底层理论及形式上的统一，同时与最近OpenAI暴露出</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441253&amp;idx=5&amp;sn=a67aaddfb506670093a96cf3cc368107&amp;chksm=bf70399189d8cc4f50a135191ea394a6c81245d39b200284589fc02ba73640f3defd48ecc3c9&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Mon, 11 Mar 2024 11:33:52 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[LLM长上下文的问题]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/Aj0FZbibW464oX1Zic6HTunOUZUZWWMibAT6MXM7r1CKqk9j1uM6mYOKxkEHQDhia4duXLXrgFdoOCEgB2vuqFEPvw/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近长上下文的业务需求越来越多，刚好把这个能力现状和主流方案的基础内容简单梳理一下。跟长文本最相关的自然就是位置编码，现在很多模型都使用了RoPE这种位置编码，之前已经把RoPE的基础内容梳理了一遍：</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441219&amp;idx=1&amp;sn=5f0b179af52f8ab4cd3666298f80250d&amp;chksm=bfd4a440fbe152454c989c9b65a1852d533ab44169374acae49d0b3ce7887eb305f33334b663&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 09 Mar 2024 13:15:43 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[俄罗斯套娃 (Matryoshka) 嵌入模型概述]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/5LJDib8HPR2pa0Mkhjg845BnhA43As6pehic8auFzPAXx3boyibJG9pSvMDYcVTd1hZVnfpdicE1b7nrytn9IbKRaQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>在这篇博客中，我们将向你介绍俄罗斯套娃嵌入的概念，并解释为什么它们很有用。我们将讨论这些模型在理论上是如何训练的，以及你如何使用 Sentence Transformers 来训练它们。除此之外，我们</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441219&amp;idx=2&amp;sn=190bff6dd42a8f3149401657a9ab4723&amp;chksm=bf299452e7637844e1262d54f868516a4f84333f188f0e1d45415023ef4c79ccb363792920b0&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 09 Mar 2024 13:15:43 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[OpenAI：Superalignment的一种途径——Weak-to-Strong Generalization]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/SdQCib1UzF3ufJsv8rjl63OiajzlUxWXEibufG2icbRFKeI0DC6xVovmMglDodX47MWJS7TJxDoQoPthJIw0MmQv8g/300?wxtype=jpeg&amp;wxfrom=0"/><p>OpenAI：Superalignment的一种途径——Weak-to-Strong GeneralizationIIya在OpenAI出大新闻之前在好多场合讲了要推进“Superalignment”</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441219&amp;idx=3&amp;sn=c47ed3f7f2fb3f5a3a2401f8fd7dc9fd&amp;chksm=bfc819b497fb610ea4b547e6d628eb5ccbcda03a2c07e2cd0027f8ecd7d3dd64da4bc9110433&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 09 Mar 2024 13:15:43 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[Yi技术报告细节分享]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/iceGibVicRfib5lWUmKVEqlaV7jv0QqEJ79uADXzlUicBDiakDoLicIb81U4uw17JnhodqxCjA8o6VaR7ZW9sE74C6w8Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>写在前面Yi模型很早就发布了，但技术报告昨天才出来。之前分享过Llama2、Baichuan2、Qwen，今天来给大家进行细节分享。Yi模型在开篇就强调了模型设计思路是围绕模型规模、数据规模和数据质量</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441219&amp;idx=4&amp;sn=2cdc105ea9c9893be8286d0f3f791058&amp;chksm=bf70c0d1b91611e3b02a4e0214ca26e289fcb31814c872df4ea737985e387a0a7fc7a1c17640&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 09 Mar 2024 13:15:43 +0000</pubdate>
      

    </item>
    <item>
      

      <title><![CDATA[【文档智能】再谈基于Transformer架构的文档智能理解方法论和相关数据集]]></title>
      

      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/kJguDvfjOGDgGIB4dVsVQKstxibLQ8mfEuP3wB04CVZRibggfQTclIxwaTak4Tric4jdGqUG3iawibXdicic9oOfb2iagQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>前言        文档的智能解析与理解成为知识管理的关键环节。特别是在处理扫描文档时，如何有效地理解和提取表单信息，成为了一个具有挑战性的问题。扫描文档的复杂性，包括其结构的多样性、非文本元素的融合</p> ]]></description>
      

      <link>http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650441219&amp;idx=5&amp;sn=bb357691898282f1eafa1a1fd383b180&amp;chksm=bffbc7f189365ca948efaaf7fb10eeda3a885cac7617dbf9780c8350dd2cb95262d822acdb45&amp;scene=0&amp;xtrack=1#rd</link>
      

      <pubdate>Sat, 09 Mar 2024 13:15:43 +0000</pubdate>
      

    </item>
  </channel>
  

</rss>
