<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_e07180c244d1.jpg</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[ICCV 2025｜南大南开新工作DiscretizedSDF：高效鲁棒的可重光照高斯模型，代码开源！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvY1q9XV7weS9RH6iblEibpVbALeCKRicg9phbeQF1bdG9wGE5SydVBusExR5dfn22Vib18fWYMC7SQcg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文分享一篇南开大学和南京大学在ICCV 2025上发表的最新研究成果《Gaussian Splatting with Discretized SDF for Relightable Assets》。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631585&amp;idx=1&amp;sn=16594ea770c590d9ab155068827cb14f&amp;chksm=97814528f4a47b1f83196b23516b9f31aa6e7b1961ac1f3c92e278003681ed628034b02d1405&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 02 Aug 2025 14:04:21 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | CMU提出SmartCLIP：模块化对齐，解锁更智能的视觉-语言表示]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs39CMOs3YPpB8LrxLPeficYyYibqCh9CKUfdUuR2CGziab9ZSVvlZiaSw7GrjG1ZRkhAdO1GF86jK0pA/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇来自卡内基梅隆大学、穆罕默德·本·扎耶德人工智能大学和悉尼大学的学者们共同发表于 CVPR 2025 的最新研究成果。这篇名为《SmartCLIP: Modular Vision-lang</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631570&amp;idx=1&amp;sn=ff4a6d423b66f2f11c9faa7206fc236b&amp;chksm=972a3b8f586283b3ba5ce4a6106ee582ce0ae61fefcbd417c9607c2da63ad66ce3ffee38daed&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 01 Aug 2025 04:31:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[6大学术会议汇总，提交EI数据库！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsQ2UPicNOEg1AvU3fpia1B4zicB4tia2AXTBDic4SQ5UpgiaRIPCXBy35V6W26lOn8crLpmv9V9JG5rSnw/300?wxtype=jpeg&amp;wxfrom=0"/><p>组委会尊敬的学术界同仁：AC学术平台谨此发布2025年下半年重要学术会议信息公告。经审核，在AC学术平台发布的所有会议均将邀请全球知名学者莅临现场，分享最新学术研究成果，且会议论文均将提交EI数据库。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631570&amp;idx=2&amp;sn=1f89aaa1f09eedf57e54f1d31de75fb1&amp;chksm=97edb0a1c81adf0b1c941e88e0c8b564944a205a3d68023791c3fa4687c3e1201b9390ec8370&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 01 Aug 2025 04:31:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[别再“一视同仁”！川大提出SMFNet，选择性融合运动与深度，解锁视频显著性检测新高度]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsQ2UPicNOEg1AvU3fpia1B4zCwwBfvMWJe76WkS0eURIbeUCZdAuKic3ZIR9fhKpueGKk2Jh37OMIeQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在视频中准确地找出最吸引人眼球的物体（即显著目标），是计算机视觉的一项核心任务。近年来，随着带有深度信息的RGB-D摄像头的普及，利用额外的深度（D）和运动（光流）信息来辅助传统的RGB图像，已成为提</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631555&amp;idx=1&amp;sn=609dcfbc1ad45fd8c9375111e6a38a5c&amp;chksm=97c62d47df0d7867bb374dac4b71f27e2ebd001fadac64d215032ec4407939f12f50cc906984&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 31 Jul 2025 10:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[SAM/SAM2赋能视频万物分割与追踪：一篇“过去、现在、未来”的全面综述]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsQ2UPicNOEg1AvU3fpia1B4zWm7mzmXAQMCS3saNnOWg6jUhvPFzCYW34EicicymbsWUTZj2AW01eJHw/300?wxtype=jpeg&amp;wxfrom=0"/><p>视频目标分割与跟踪（Video Object Segmentation and Tracking, VOST）是计算机视觉领域一项复杂且至关重要的挑战，它要求在动态变化的视频帧中，将目标分割与目标跟踪</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631555&amp;idx=2&amp;sn=e5a8db9572fd5fb15f324b5238674f82&amp;chksm=97b5636ab2f03c08e26b2058c762d1a5c020785ab9357b7af2e6558dde5bce3bee645b03cfc2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 31 Jul 2025 10:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ForCenNet：ICCV 2025 | 抓住前景“C位”，文档图像校正迎来新SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu7JwFEicXiaD5UInuOFueEyxEU4XsToxrw8bp9CxlkXWr6Rkzcd08wAm0j9OmEfGmvvY4C12ialRt6A/640?wxtype=jpeg&amp;wxfrom=0"/><p>用手机拍摄文档时，由于角度和纸张弯曲，图像往往会产生烦人的几何畸变，严重影响后续的文字识别（OCR）效果。现有的文档图像校正方法致力于“拉平”这些图像，但常常忽略了一个核心要素：文档的前景内容（如文本</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631511&amp;idx=1&amp;sn=b57ec2670e93f5a1324ca10f0e377d42&amp;chksm=975c8581e907d5342398c709748f44abed8eb5a6de8c40d5b79a07b64353ea4c7db7745451af&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Jul 2025 10:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | TVT：迁移VAE训练，让Stable Diffusion超分模型看清“精细结构”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu7JwFEicXiaD5UInuOFueEyxjW1ryoRqtdUWsic93Ya9RlkA7gg01feUMvIHtsriaLL07hTOQaGH288A/300?wxtype=jpeg&amp;wxfrom=0"/><p>基于Stable Diffusion（SD）的图像超分辨率（Super-resolution）技术已取得惊人效果，但它们普遍存在一个痛点：由于SD模型内置的VAE（变分自编码器）采用了高达8倍的激进下</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631511&amp;idx=2&amp;sn=d4c181cc51db520633260e9e046d8ff2&amp;chksm=977b0bc7927706ddb33d4529df309c98bf7480a8e52363371daf3ff1727350e437e5de9521d4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Jul 2025 10:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 Highlight | 清华等提出CoopTrack：“端到端协同跟踪”新范式，开启自动驾驶感知新篇章]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuzCJ1ibNFjUcvGhlJP2ucbtEBSicTq9R2C2Q90DIahmlH3fX5odtkT1ibndK7NB9qWbGgZb3m61bibLw/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文分享一篇被计算机视觉顶级会议 ICCV 2025 评为 Highlight 的重磅论文——《CoopTrack: Exploring End-to-End Learning for Efficie</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631472&amp;idx=1&amp;sn=fbbce2db04cd517ddfeac3b373b833cd&amp;chksm=97e9bdb783a82ccec309e4706125f817cac9a3372f397ca4941907dc1fef89b06397ae8280a8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 30 Jul 2025 00:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 哈工大等提出SENTINEL：在 MLLM 幻觉萌芽时"干预"，将"对象幻觉"降低超90%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuzCJ1ibNFjUcvGhlJP2ucbthBufbicWHuCLkVSgLznu2mUzDCibHLRyM31RqtwLvOTPC3J2RViaW0VlQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>多模态大语言模型（MLLM）在理解世界方面取得了巨大飞跃，但“幻觉”问题——即生成与图像不符的内容——仍然是其关键弱点。来自哈工大(深圳)、港中大和港中大(深圳)的研究者发现，幻觉往往在文本生成的早期</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631458&amp;idx=1&amp;sn=1db8dd6e5843334ba63e37284f39a25e&amp;chksm=97ae61fdc24376cadbd3b1720269dceb13f8317bb34fdc0c6902b0d3c108acb861ceaf4d8b2d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 29 Jul 2025 15:43:07 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 超越文本对齐：全新奖励模型ICT-HP，引领图像生成走向更高审美标准]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuzCJ1ibNFjUcvGhlJP2ucbt9Ftljjmaol26oydcicVl9AdBnMY6tVR4yTXMuaAQsDicke75iaskngSaQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>本文将解读一篇来自中国人民大学和iN2X公司的最新研究成果——《Enhancing Reward Models for High-quality Image Generation: Beyond Te</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631458&amp;idx=2&amp;sn=66cebd3f3f846280eb273bece0d12ac9&amp;chksm=97ba3d2dcdd16bca805c27259885e126991b6b5633f272c4c7fef6154aeff485e43d79b9cc62&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 29 Jul 2025 15:43:07 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Meta  Yann LeCun 团队最新研究：“回归特征：DINOv2为基，铸就强大的视频世界模型”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsOeicq7VYupmzibfHEYsvPPJnkySK6fR1qTB5ribRQiblDZAFmuTGDf09xxAqa2TYfr3XhXREGjMRvGw/640?wxtype=jpeg&amp;wxfrom=0"/><p>来自Meta Yann LeCun 团队的最新研究《Back to the Features: DINO as a Foundation for Video World Models》，带来了一个名为</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631414&amp;idx=1&amp;sn=4ece6894dadd55f4454faa28cd88f2da&amp;chksm=9712df853568a44b0c2b2badb44290fb6feacf3547cc76247594e46d67cf43a7355eeceb61f3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 29 Jul 2025 00:06:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 oral | MuStD：融合激光雷达与相机的3D检测新SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsOeicq7VYupmzibfHEYsvPPJBwMbBS0AuJ9jiamgro3Vticz1bqKKoiab6F5nWnibKZowxQJRgQMLjmgtQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在自动驾驶和机器人技术中，精确感知周围环境是实现安全可靠运行的基石。其中，3D目标检测，即在三维空间中识别并定位物体（如车辆、行人），是核心挑战之一。为了提升检测精度，融合激光雷达（LiDAR）提供的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631402&amp;idx=1&amp;sn=24188f29ad6865dfd026153f995fe603&amp;chksm=97cd37ff88b0d6eef31e7ab91e4efe781bbbf2d6cfc56b3128857519cd325b7e3816a8422bd0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Jul 2025 13:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 北大提出UPP：告别繁琐预处理，用“提示”让点云分析更鲁棒]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsOeicq7VYupmzibfHEYsvPPJXyp9sPLbFuYibFdqMKLkiafwlWibrc2Oa6SGXGjNfXPkEuheozJicdePSQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>点云作为3D世界的数字基石，其分析与理解是自动驾驶、机器人和AR/VR等领域的关键技术。尽管预训练模型在点云分析任务上取得了巨大成功，但它们有一个共同的“软肋”：在面对真实世界中常见的低质量点云（即充</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631382&amp;idx=1&amp;sn=7e7336feda03a637f328ea219a946922&amp;chksm=9722246b50fd08e72e6c4ca262d2d91be65b53206bd664ea55595a4e4766856f310e92efd270&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Jul 2025 10:08:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[中科院等提出RealisVSR：细节为王，用扩散模型挑战真实世界4K视频超分]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsOeicq7VYupmzibfHEYsvPPJQceMhAIU2rRjMbUPfsx2iaf7vqiasHic4dNmzsSUiaXGbSpRRrnB1BFEHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着高清、超高清视频内容的普及，视频超分辨率（Video Super-Resolution, VSR）技术变得越来越重要。它旨在将低分辨率视频提升至高分辨率，同时恢复丢失的细节并保持时间上的一致性。近</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631369&amp;idx=1&amp;sn=d17deccfb32ee11726ac4fb8201623e0&amp;chksm=97e0efa13445410b0c8b8bd101407d844c9923346ea6cfe00f949d5cc0d747c3944ca43d39e9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 28 Jul 2025 08:01:55 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 浙大等提出 SGCDet：自适应3D体素构建，重新定义多视图室内3D检测]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvRp1kTauGNq8qTicibt7gRbIDzVsKodMVicuC1oDWrZeEYfxJiblslicuYBhrZdr4JSgT6yhgYPicia4ia8A/640?wxtype=jpeg&amp;wxfrom=0"/><p>多视图室内3D目标检测是实现场景理解、增强现实和机器人导航的关键技术。然而，如何高效且准确地将多张2D图像信息“提升”到3D空间，一直是该领域的瓶颈。传统方法通常采用固定的投影方式构建3D体素（Vox</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631344&amp;idx=1&amp;sn=f46a8fa3d8fae0adba94e48f84e4692a&amp;chksm=9793e48f2b29a9948b65bffd2b94da143a813d59ebaa1d155afa87f43d06dfa814307b7b3408&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 27 Jul 2025 22:19:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | “看一个，认所有”：OP-SAM让SAM模型仅凭一张标注，即可实现全自动息肉分割]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvRp1kTauGNq8qTicibt7gRbIxGwI5l2Urt8WhdDEWs6sJSnKHxTlleB5gRWiaYPLJqy19Xn6CqEbtJA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在结直肠癌的早期筛查中，准确、高效地分割出肠道内的息肉至关重要。然而，传统的全监督深度学习方法不仅需要大量耗时耗力的精细标注数据，而且在面对形态各异的息肉和不同设备带来的领域漂移时，性能往往大打折扣。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631330&amp;idx=1&amp;sn=e084ee7d24f86a1c059a595df10c7ba1&amp;chksm=97aca1a36da5b984b1e662525237b4ac13dfbc2fc1a10f4fbafaf0ab5fe2fbb70a41e2795181&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 27 Jul 2025 14:23:15 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 告别数据依赖：专为目标检测设计的任务特定零样本量化感知训练]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsVG3khuCYwI1UrJK2TxTfgPPBxjjFJV89IH7Gq9YFTeSW8icVpKpjMy0icVBKnMgDic8YVia4Wfmt3zw/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文解读一篇来自佐治亚理工学院和清华大学的研究论文《Task-Specific Zero-shot Quantization-Aware Training for Object Detection》。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631273&amp;idx=1&amp;sn=229a389c390d2317f63cdcdeca9f4acd&amp;chksm=9777c91a3e68628cb0c3387387a5d997801e4ecdd650b26f338980c034220234ab13a7c59459&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 26 Jul 2025 15:12:31 +0000</pubDate>
    </item>
  </channel>
</rss>