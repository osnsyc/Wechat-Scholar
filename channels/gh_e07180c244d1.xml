<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM6aYkwkiboia6lA9D7ANy49WBe9icxn5NQqJjvn4Pyntzvfw/132</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 教机器人时间管理：华科大&amp;小米提出GRANT，教智能体并行执行任务，效率提升超30%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsibCBrncbfyiaybeVStZYvqjE96RSC1lXQCEu2Licicbol6ibXaa72Rob4G88GrK0lV0sLLbqOibamUdgw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution作者: Dingkang Lian</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647335&amp;idx=1&amp;sn=98095d818e1995657a7ea62f445d9f02&amp;chksm=9749cdf645a0af274b9aa7c09cb41dfe3ebebe448bb67af4afe0328a9af33061d06c86bb5ae1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 25 Nov 2025 20:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[笔墨未动，草图先行！UNC提出SketchVerify，让视频生成告别“物理翻车现场”，规划效率提升超15倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsibCBrncbfyiaybeVStZYvqjHkIdCqiaEN3SEzLdbCYcNELn8kN1JML8KoxWzVQ4VutNJBMnHfSXWnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，视频生成领域真是越来越热闹了，但不知道大家有没有发现，AI生成的视频虽然越来越惊艳，却常常在一些基本问题上“翻车”。比如，物体运动轨迹飘忽不定，或者干脆无视牛顿定律，上演“反重力”奇观。这背后其</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647335&amp;idx=2&amp;sn=cc7619cbff3fc0cbbbb7b33891677a3e&amp;chksm=972be9bf073d814dea6479c03ab4dfd321d0c34d3242f852ca072043a3a5a7c53522ce454112&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 25 Nov 2025 20:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 浙大&amp;上海AI实验室发布RacketVision：首个跨多种运动的球拍分析基准]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTtNY34WOnAOibQmD2ezbY7owG4k7xyibQpTUWzvc0mku1P3zicIex8v4Xtxr78dzbzyNpXRe5aSyPg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis作者: Linfe</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647200&amp;idx=1&amp;sn=e947ebc47021ba55b9aafb97b0c959c0&amp;chksm=97b9e0d2170b36d5ee8231e638ac19476fd98a6c2be457f34bbea6474b18292b58be2511e9b3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 24 Nov 2025 18:25:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[图像去模糊新突破：同济、港浸大联合提出四元数卷积，精准建模通道耦合关系]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTtNY34WOnAOibQmD2ezbY7U5HPTmESTiaIncPmtCgAdggSpbpiaCaSEIlGv35s8ZfmM9viasTSUwHtA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇关于图像去模糊很有意思的新工作。我们都知道，照片拍糊了是个很头疼的问题，而“盲去卷积”技术就是为了在不知道模糊具体成因（比如怎么抖的、怎么失焦的）的情况下，把模糊照片恢复清晰</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647200&amp;idx=2&amp;sn=a585f8ec3f7a91707b95c94e6397fd28&amp;chksm=975e4fb0b812b05810c705145ec206f8c40e4507c947c55467fe1970aad5b8ad9331c055170e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 24 Nov 2025 18:25:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[基础架构的新探索：清华提出Step by Step Network]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTueI12bhdACsFO0uzaMHENEEmt5hmVeTBklpny5CNAGoHe83NdwH6rL4ddYQrVDfq1XP4Ba2jyuJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天我们来聊一篇非常有意思的新工作，来自清华大学和华为诺亚方舟实验室的研究者们提出了一个名为Step by Step Network (StepsNet)的通用网络架构。它的核心思想非常直观，就是让神</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647018&amp;idx=1&amp;sn=68f01a9ad012afde26db76cbaaff7ead&amp;chksm=97f8f4aa4ab29d02f4e572db4741efcf2a5095929666be64c112569019bc0567dc4e69bde76c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 23 Nov 2025 11:38:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | MIT新研究：数据集蒸馏迎来“线性时代”，一张图顶半个ImageNet？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvricRZfrtrTuIU5XiaOa5y1nJ16JGZroAZraL01eLTRFHiaJGfIDZ9bVEUsm52IdiatuichqJibdQBwn1w/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Dataset Distillation for Pre-Trained Self-Supervised Vision Models作者: George Cazenavette, Anto</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646966&amp;idx=1&amp;sn=88dac4b3c22e561dd422c109f18a1bfa&amp;chksm=97d75786643f827369fa08a13a1390cce16fca25861276e62c4acfb697736b73899bdbff0e75&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 22 Nov 2025 12:37:16 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[遥感变化检测，ChangeDINO来了：DINOv3驱动，IoU、F1指标全面SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvricRZfrtrTuIU5XiaOa5y1nfYe0HJ2OicIPIs0fsCcNDXcWm42t7W94IYPbHxGudLuCibKMFIVK6yZQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery作者: Ching</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646911&amp;idx=1&amp;sn=7a79c513509f614e9b7126c9c75ed38e&amp;chksm=976a8376dffa53e2276c07c98163a9572e1dfc2120f0c7351d53b43cc55a65f8eddc31c863c2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 21 Nov 2025 17:38:11 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[解耦骨骼与体型，实现前所未有的动画真实感！Meta SAM 3D核心技术：开源人体参数化模型MHR]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvricRZfrtrTuIU5XiaOa5y1n7Qs2fKZCdzws88icoZZqzoRYj5h6T0s2zvlnWzEGhiaYiccZ8MNDjNQXQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天想和大家聊一篇非常有趣的新论文，它来自Meta，之昨天发布的SAM 3D的关键技术，题为《MHR: Momentum Human Rig》。这篇论文介绍了一种全新的参数化人体模型——MHR</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646911&amp;idx=2&amp;sn=1d9ef3256c8519b1337480753c3ca650&amp;chksm=9714c7b76b940088e5ed66c390ba16c397c4ba35d64adf1fe5e1cef5a06f5188a3a561aed3b5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 21 Nov 2025 17:38:11 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[发布即产品！SAM 3D横空出世：Meta再次颠覆3D视觉，单图即可实现高精度三维重建]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtroia98h5rf12wVfAsP1m3WnVQVamGKYG8WjTyjsUtxtW6JYGAf4fjbGkjIa4nzVVianGiaa2zJj5fw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天，Meta AI 推出了其 Segment Anything Model (SAM) 系列的最新力作——SAM 3D，一个致力于理解和重建物理世界三维形态的开创性模型。无论你是探索 AR/VR 前</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646694&amp;idx=1&amp;sn=b812cbbd33278ddb551d1bface962b15&amp;chksm=97f4ba5b7a5beb7a9e6ac57304f5a9b3a5b2eb97cd8627d2c7babf49bfa7fbce97bf16856570&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 20 Nov 2025 15:11:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[很强很惊艳！Meta重磅开源SAM 3：可概念提示，统一检测、分割与追踪，性能提升2倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtroia98h5rf12wVfAsP1m3WTh0icdZpPRtFAubJwR3ljTNsp2sBqpcCa9ibibzxVMmiaGU2vmz9V03adA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天Meta AI终于公开发布了他们“分割一切”系列的最新力作：SAM 3（之前在匿名投稿阶段）。这不仅仅是一次常规升级，更像是一次“王炸”级别的进化。简单来说，SAM 3现在能够在一个统一的</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646694&amp;idx=2&amp;sn=e451f05131a76f3d52450a0add0822ff&amp;chksm=97fc2e2703ef5146eb253d9b7ada13f1e37b6c3b5de75bfab6b5a239c5fa0536af3d0f02f3aa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 20 Nov 2025 15:11:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[何恺明团队再出手！将ARC视为视觉问题，ViT从零训练60.4%准确率，达到人类平均水平]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsgQHqgLkzkHJEyvk4cicj0n68BUCzoIpMOUhluPUsf9EibqicZ2UlaSvXr9wNzxBciadk6MTBTibqkiaTw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: ARC Is a Vision Problem!作者: Keya Hu, Ali Cy, Linlu Qiu, Xiaoman Delores Ding, Runqian Wang, Ye</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646512&amp;idx=1&amp;sn=8c6aa75138f40097522c1fe5378aff69&amp;chksm=97ec23f9a299dcdc02e80e4de01edf86ff17a8c538f367312b8ca30be07d2d0763516a1651a3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 19 Nov 2025 15:57:10 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CMU新作Co-Me：无需重训，VGGT长序列迎11.3倍加速！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsgQHqgLkzkHJEyvk4cicj0nZwZCzxlb3h3fWK8s6KT2XRRicdPGknHWeTsCcRyUg0QRC2H69bwIFibw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers作者: Yutian Chen, Yuhen</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646512&amp;idx=2&amp;sn=8441816f904d834fbff257f53a367549&amp;chksm=97a37b5d060525f115a2f983b90e82704b3d560013849563aaf1a07881bb0b28a21ebc598cb8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 19 Nov 2025 15:57:10 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[MIT何恺明团队新作：让扩散模型回归“去噪”本质，简单Transformer即可实现SOTA性能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbfkzY7GbB6TyibzDBq4OFRvCGpbDr889LoAyrCgQnhsWVfVkMhGjQMct5OMuf9KjdnYVMsNv6W5A/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天来自麻省理工学院（MIT）的何恺明团队发表了一篇引人深思的技术报告，对当前主流的扩散生成模型提出了一个根本性的拷问：我们真的需要让模型去预测“噪声”吗？论文标题: Back to Basics:</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646237&amp;idx=1&amp;sn=88640d1ae12bfb2b12f2c594280a0ada&amp;chksm=97bf3da5a7372749ffeb673f0c25ee42f4be233ba90dda8639c12b5cacf3923ef86345e74386&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 18 Nov 2025 15:20:14 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Yann LeCun团队新作LeJEPA：仅一个超参数、50行代码，实现可证明、可扩展的自监督学习]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbfkzY7GbB6TyibzDBq4OFRibnBdibM0IhA9JyCZZeWXBoyGmMEGoZmyaibA5biaFJiaU2TS4jubiaKeEcA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有趣的新工作，来自图灵奖得主Yann LeCun和他在Meta-FAIR及布朗大学的同事Randall Balestriero。这篇名为《LeJEPA》的论文，可以说是给</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646237&amp;idx=2&amp;sn=df009aba33b412c14c1cd9c2e8c0758f&amp;chksm=97ddab3eb5e90b8396b209d9e96ba692bc12d15a8a22a44c0f7805aee82389e5751cc6c57202&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 18 Nov 2025 15:20:14 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral MiniShift+Simple3D：面向高分辨率3D异常检测的可扩展数据集与实时检测新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv4nqqn4BfjKjxporPdLG1nqQib1o4MicL54OpsAru0FgMXaR5lia5UbD2hE53A9tmSb0ApuIeCzNINA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在工业质检场景中，细微凸起、微划痕等缺陷可能引发严重安全隐患，例如航发动叶片的微小裂纹可能导致高空故障，精冲齿轮的细微划痕会造成啮合失效，而现有3D异常检测技术面临着效率和精度的双重挑战。来自华中科技</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645986&amp;idx=1&amp;sn=be0744a6e6cfec51bdf475e67ac3eb32&amp;chksm=970e02a2f82899562550c322a937570c7ef5e4f76867a4ceefeb62557b70d51a43e605906e13&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 17 Nov 2025 12:35:54 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 山大提出DiveSeg：为DINO注入“水下感知力”，实例分割性能提升显著]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv4nqqn4BfjKjxporPdLG1nJy4LIQY3jRjLLwFUXfvSn3Tv1Av0Leics08cFh1EORe4eHrtdJBAG9Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天我们来聊一篇来自山东大学团队发表在AAAI 2026上的新工作。他们首次将强大的自监督视觉基础模型DINOv2引入了水下实例分割（Underwater Instance Segmentation,</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645986&amp;idx=2&amp;sn=59400c3fe66754ddc0484efceaeffd14&amp;chksm=97d6277a0fa92e8ddcb2fb344d4b390d43bc00e03151125f947763389a079600eb29fe67e366&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 17 Nov 2025 12:35:54 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 清华大学等提出SpatialActor：解耦空间感知，重度噪声下机器人操作鲁棒性暴涨19.4%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsWAJSALYJdHPJAWIZbibY6tbtPyePuLgOibiaGjxB9zRiaSdrqbY4m3wXt0m7vlUZBexibfvjeP9xmjjg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation作者</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645905&amp;idx=1&amp;sn=3c7289e60c48bd124015cbc3b498e965&amp;chksm=972017ddc5e6096f5956d5308cc155b725727bd38cab6885dff2c998bb5030fc29f13e6a79ec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 16 Nov 2025 07:02:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 中科院联合港大提出ARRA：外部视觉表征对齐重塑全局一致性，让自回归大模型充分释放图像生成潜能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu89QTKP8qGnCS5hcw8XXiaW1qdD5OEiaiaPmzbOrgMeLs0zvmE9VbkLl2E2AhKPyBlQL9APFEomFZJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregr</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645870&amp;idx=1&amp;sn=bcaccdf418731a885668263d3505d55a&amp;chksm=9733f68b9a2c044c8a4cd8352ea53779990a088234826e2d21e0c3e08d2400a95adaa5725ed6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 15 Nov 2025 11:31:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[“全模态”3D视觉基础模型OmniVGGT出炉！即插即用任意几何模态，刷新3D视觉任务SOTA，赋能VLA模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuMLJSWapND0kWBs66j0X7pHrPibsHN4m7ezYs3E9KSvJGMspoRQibBKCxvvsXp5h2ZJicles6Gehiajw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer作者: Haosong Peng, Hao Li, Y</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645843&amp;idx=1&amp;sn=0edb5c298f336157e2292730ef66b717&amp;chksm=9721417cce14fcef72fa073fd3e7aedad77c6763650475b011af3b84a1df8900e1ab419e4a4e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 14 Nov 2025 22:13:58 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[WACV 2026 | PALMS+：融合深度基础模型，手机室内导航的技术再突破]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuMLJSWapND0kWBs66j0X7p6l9NxKlpeKTBbemMjvYFYpniasbYZaicviaexUOHicB6kMwBZH9mdNtSEA/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model作者: Yunqi</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645843&amp;idx=2&amp;sn=73b50f9ee7ae99d3ee07a92f54a36d4e&amp;chksm=97308fcd0133763533aaf77d684b27ec19faf1931cca1bca9dfc54c0495086ef775ec9586303&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 14 Nov 2025 22:13:58 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral 中科大联合西工大提出RSKT-Seg：专为遥感打造的高效开放词汇分割框架，推理提速2倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsWAJSALYJdHPJAWIZbibY6tlWicp1HcNSr7OgibaCH6wp0OWeDrP16Kv2rsfBLL8K5sUTQutfj5DFsA/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing作者: Bingyu Li, Haocheng</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645477&amp;idx=1&amp;sn=0200dcedfbe62bb27116467ce0a70e41&amp;chksm=97dcf42db5a6726ebf367b745c284fb38ebe645b43f6b2e0c753d5ca18dcd760ada1d45d33ec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 13 Nov 2025 14:07:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Roboflow&amp;CMU论文披露RF-DETR细节：首个COCO数据集突破60 AP实时目标检测，速度飙升20倍！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsWAJSALYJdHPJAWIZbibY6tStVff1xFmHLWCAe3ickteqLJwiavJWL8xSMftxxbC5Vhr1jicmZs0Gcyw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: RF-DETR: Neural Architecture Search for Real-Time Detection Transformers作者: Isaac Robinson, Pe</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645477&amp;idx=2&amp;sn=7b7fe5113c80c45571e8857a19035512&amp;chksm=976dbefabde56fe9be70fb254e2c18294893630ab66bb04ea03d3ffcfa171043b6a1747064a3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 13 Nov 2025 14:07:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ConsistEdit：重新定义AI视觉编辑，港科大、清华等机构提出MM-DiT注意力控制新方法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvfujXVz1s1hQzXiaTTtUuyBFiaS5QgBDR1EicibqvgSG73ZatRickSmO9l4AVZNEA41whdDq3Uz9ibn8tw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: ConsistEdit: Highly Consistent and Precise Training-free Visual Editing作者: Zixin Yin (香港科技大学),</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645297&amp;idx=1&amp;sn=98310e0c2d9a8e212fad343cba1d786c&amp;chksm=9781220e8bac1deea486d173530bf2fd54e5af5bbe44dbf25ae792a6832a39be9dffd82e1705&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 12 Nov 2025 19:49:17 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 中科院携手快手发布LiveStar：首个“会说话、懂沉默”的直播AI，推理速度提升1.53倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra69vVQrm5ibKGfLxls9CGQyGcSVGROkOSRbeK1wWkJuzau4sqj3dDjiazQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: LiveStar: Live Streaming Assistant for Real-World Online Video Understanding作者: Zhenyu Yang, K</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645297&amp;idx=2&amp;sn=4d6f1fcdae54ff354ef1b0eb42760208&amp;chksm=97c77c26ad061b07f5e8d888218fa15cbade6ede49bde605005cf69430368e5c7d1927c3330d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 12 Nov 2025 19:49:17 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[直到毕业我才知道，原来博士延期是常态~]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuO6szg3WXCv2hOCo8SAevdqdkYq4G1YNX6WqJ2YX07Y0vGnFhdKJVedQtqZX6ulFUCiaQKxiblyIiaA/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近经常收到读者的留言 : 抱怨科研真是太难了，竞争压力大，导师不给指导、不开组会，一年见不到导师几次，对于论文初稿、毕业毫无建议! 其实他不是个例，大家也会有这样的烦恼：前沿顶会、期刊论文、综述文献</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645016&amp;idx=1&amp;sn=1a2f184f79b33e82007d5bed73c27043&amp;chksm=973bd2e8a7d2d389bcc1b52b165466700ec31f6600258930a5211c80d2fc94e8c9ebc3702052&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 11 Nov 2025 12:17:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 上交大、南农大提出ADPretrain：为工业异常检测量身打造的预训练“超能力”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra6HMzozlQqPIuibwM6ausRpnAxduwEQib3wOyKHsFLIIO91rfCeBmXFMeA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有趣的新工作，来自上海交通大学和南京农业大学的研究者们，他们提出了一个名为 ADPretrain 的新框架。简单来说，这是一个专门为工业异常检测（Industrial A</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645016&amp;idx=2&amp;sn=660a0c3459f4f8f9744de4d98300cbee&amp;chksm=9751b3bc5b0f89f58c2bce511ea77314d3e69665110ed80a772481b72df747124c7a2b215800&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 11 Nov 2025 12:17:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[3DV 2026 | 特伦托大学等提出DEMO：让AI看懂复杂人体动作，实现密集描述新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra6jR7UYewmOGZMHTX7Fh60O5txOJ5z7HxsEoD8px1yS2odqSAVkicpOOw/300?wxtype=jpeg&amp;wxfrom=0"/><p>3DV 2026 | 特伦托大学等提出DEMO：让AI看懂复杂人体动作，实现密集描述新范式3DV 2026 | 打破动作理解次元壁！DEMO模型携CompMo数据集开启3D密集运动描述新纪元特伦托大学</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645016&amp;idx=3&amp;sn=5fe7bd6d9cf772bf818add4b5d0a18c8&amp;chksm=97096d9a6a61d4d98b339f87b960c198411eeaf750a7a6b69a49e8cf9dc2e3cb5227554cb3d7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 11 Nov 2025 12:17:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AI“世界模型”离真实手术还有多远？首个外科视频生成基准SurgVeo揭示“合理性差距”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra6KlenJ0ZIBCtZlCnwtoba0LkQLLePrWHmj3MUiapHibiaUtkXk4ibOxqS2Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，视频生成领域的基石模型正展现出作为潜在“世界模型”模拟物理世界的惊人能力。然而，当这些技术被应用于像外科手术这样高风险、需要深度专业因果知识而非普适物理规则的领域时，其表现如何？这是一个至关重要</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644891&amp;idx=1&amp;sn=9cd8b1694c6911be8a1015249b3a4602&amp;chksm=9712ac245417211234e79740db7d9a5accdc685b95a7d09f82ad1b9929afc484fc623a203d3c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 10 Nov 2025 13:00:57 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ACM MM 25 当MLLM遇上行人重识别：是“降维打击”还是“水土不服”？深度评测来了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra6x3iaqc1POmukJGcBibHcKibYwcgZn3pD6KFLJoYiaVvpiaodSvoMC2GSUlQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，大型多模态模型（MLLM）的能力边界又一次被拓宽了。当红的 GPT-4o 不仅能说会道、看图作文，现在，来自武汉大学的研究者们想知道：如果让它来做一件非常专业的计算机视觉任务——行人重识别（Pe</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644891&amp;idx=2&amp;sn=c73fce6d4568a3f68cd4239176c9c281&amp;chksm=975fffa48918c4f9452d25a705e4a08dfb43417617edebeecf90b05a691f7608436980e0ddda&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 10 Nov 2025 13:00:57 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS25 | 清华&amp;北大提出LinearDiff-ViT：让Transformer学会“找不同”，实打实提升模型性能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTux5HIyhHZa9cXErvk8Dq97zoichKw2gYOZmC2Uul6TQqxdqicicnEFmZibVj1CejzJdqdTZV3qkFEWdQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是CV君。今天想和大家聊一篇来自清华大学和北京大学的最新研究，它给热门的Vision Transformer（ViT）带来了一次相当漂亮的“线性提速”。这篇被 NeurIPS 2025 录用</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644847&amp;idx=1&amp;sn=86679fbc907f3342ce714b35d8895019&amp;chksm=97bfcf7333d6a2959c31f7275d458517fe3abbcfb1ee2218d3cc382526fecafee4f2572c4c27&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 09 Nov 2025 11:32:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TRO'25开源|机器人建图的终局？一个框架搞定光学、几何与语义！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTux5HIyhHZa9cXErvk8Dq97xdnQZfr0Yb0ibSRHUq0Hf23K5vxY8bfe2hvgnFdWq5FmRKCvmlDlicQw/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常扎实的工作，来自北京理工大学团队，并已被机器人顶刊 IEEE Transactions on Robotics (TRO) 接收。这篇名为 OmniMap 的论文，提出了</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644814&amp;idx=1&amp;sn=d11cb2f4ff8460653807d90beb1619d7&amp;chksm=97a2c62d83d9bb2a0cc40657a455f7ba0be0b5b012c0246f73c663c00e98e05f1e6e95656922&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 08 Nov 2025 20:32:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 | 北理工提出BoRe-Depth：仅8.7M参数，在嵌入式设备实现50.7 FPS高清深度估计]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTux5HIyhHZa9cXErvk8Dq97s7Dk4nqovOsRM7LHGQOAcecibWxR7ic296tAoCR1h66jIdRtt3Ae4KGw/640?wxtype=jpeg&amp;wxfrom=0"/><p>单目深度估计是无人系统实现3D感知的关键，成本低廉但效果常常不尽人意，尤其是在算力有限的嵌入式设备上，生成的深度图往往模糊不清，物体边缘细节丢失严重。来自北京理工大学的研究团队针对这一痛点，提出了一种</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644765&amp;idx=1&amp;sn=7559e56f0d7ff4b0de2f09b530b10b63&amp;chksm=9731feefafb82a5358e59983abadc053f069f30a7bd1cf9b625b48cac59ca3e7c94aa389c3eb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 07 Nov 2025 21:32:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TCSVT 25 | 宁波诺丁汉大学等提出De-LightSAM：仅需SAM-H 2%参数，实现通用化医疗影像自动分割]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTux5HIyhHZa9cXErvk8Dq97iaicOuFSVibkh25k47fOVSOEiaOPyqiaUibzdZ9WLXDr3Qcib4ibdfTkIP8ZVA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，分割一切模型（Segment Anything Model, SAM）在计算机视觉领域掀起了一股浪潮，它强大的零样本分割能力让人印象深刻。然而，当这位“通才”进入严肃的医疗影像领域时，却显得有些</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644765&amp;idx=2&amp;sn=5f3c424de43191606872ccef8dc34d29&amp;chksm=97bf49e478f171f5211265cb71acb87f9bc2eb0a744d62fc34362a8666d0d9a3eb353e5b4183&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 07 Nov 2025 21:32:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦大学&amp;StepFun提出WithAnyone：告别“复制粘贴脸”，实现可控、高保真的多人ID一致性生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsyba4iavErNFtUTDh5donxrhibWSpOFpJs7icmpSNwbx896qzWekds4snKcPq4l37hdMF04gNicQ1mDw/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，AI图像生成领域最头疼的问题之一，莫过于如何让生成的虚拟人物不仅长得像，还能在不同场景、姿势和表情下保持身份的一致性。很多模型生成的“写真”，仔细一看，总感觉像是把同一张脸生硬地“复制粘贴”到不</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644594&amp;idx=1&amp;sn=09e73999afe70cb10e14994c2dcac033&amp;chksm=971e7579bf8ce4229abfa867f23c5408b28efefc1bb2264613e7a6f457fb6fcb304bcaf6c331&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 06 Nov 2025 17:35:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[大道至简，中科院等提出OneRef：统一视觉定位和指代分割]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsyba4iavErNFtUTDh5donxrz5oaKQTRdzkoI5MWC0E6Sf3rEgXtmZSViaDHcqtDshliafNScpOdcBZQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>在很多工作中常看到“统一视觉与语言”表示的论文，今天分享一篇语言指代定位与分割领域的工作，来自中国科学院、鹏城实验室和哈尔滨工业大学（深圳）等机构的研究者们，他们提出了一种名为 OneRef 的框架。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644594&amp;idx=2&amp;sn=558ac581a8c7f8546fac26361a9ff20a&amp;chksm=97989716a26f8e34e00dadffebec7d6564b3667b9f74930efc9a9f7374f01418056a3d5663c8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 06 Nov 2025 17:35:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[低光图像增强新探索：SASW-Loss，无需改网络，即插即用提升多种模型性能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsmKeClPicNhOicAH45nrxVtxORSN8ptNCIjibdN1xJPsxg1ibFscWicxvcL8cECECeB11DryYFK3FSic4Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>晚上拍照，光线不好，照片总是黑乎乎一团，还伴随着奇怪的色偏？这在计算机视觉里，是个经典的老大难问题——低光图像增强（Low-Light Image Enhancement, LLIE）。为了让黑夜变白</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644445&amp;idx=1&amp;sn=600de0b3040c79781ecc3b5ae1037467&amp;chksm=97db0215c710a22a61c879574ddd555c578a2fd23b0595f24d1333a586fd951d06e35881d589&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 05 Nov 2025 13:12:07 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[清华&amp;南洋理工等提出稀疏模型反演：ViT反演加速高达3.79倍，告别无效背景！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuYiaiaMch4Pic3bch8qTystJ41GyTaoEEumEt4KPH0QY2771HBBrefrHrCB9TUx4ibgxEe30ficR3EsFg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近和大家聊了不少关于数据的话题，特别是在数据隐私和版权日益重要的今天，如何“无米之炊”——在没有原始训练数据的情况下，让AI模型继续发光发热，成了一个热门的方向。今天，我们就来深入探讨一篇非常有意思</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644445&amp;idx=2&amp;sn=3cdb9e3169f1ae97c0d18e044e9664c3&amp;chksm=97665cd7a3e33172e25263b702ee96aa7e82643382de87f3786447cb00279bc6f57753077dcc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 05 Nov 2025 13:12:07 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS25 | 香港理工&amp;OPPO&amp;哈佛提出DNAEdit：直接噪声对齐让Rectified flow文生图编辑更加准确]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvq2eUtOSXtcYsjFa6tJeRn3XEqjEI8EZAyRYfYapxcFbIVflPhWPYLzFgpyN1pydiaxTT6SOWzgHQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow EditingarXiv：https://arxiv.org/a</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644428&amp;idx=1&amp;sn=2d64e4df7e359debbb1679e23e22a6fb&amp;chksm=976246d648c0254dde23a43085e19e935a9c3b9fdc419d5619455baf2a68454fdadf4c22968d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 04 Nov 2025 20:16:06 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[为物理AI打造世界模型！英伟达发布Cosmos-2.5：模型缩小3.5倍，性能媲美SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuYiaiaMch4Pic3bch8qTystJ4QibJuDCDlmAdEjibBqZBZibxCd5SEGAtiaqWt8WHibmct2MP0ecVotiaQxbQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>NVIDIA的研究团队再次惊艳了我们，今天他们推出了最新的物理AI世界基础模型——Cosmos。这个系列包含两个重量级成员：Cosmos-Predict2.5 和 Cosmos-Transfer2.5</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644428&amp;idx=2&amp;sn=cf7bbe288f083a0fb58087577d6071df&amp;chksm=973828535fe7b19f74e4d0c64126c2ae225212a67fcf0728e17a92dd49633ea70fe29a425721&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 04 Nov 2025 20:16:06 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Sketch-to-Layout，从草图到布局！DeepMind &amp; EPFL 在设计领域的新工作]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuYiaiaMch4Pic3bch8qTystJ4ibuIhCicbgIPMRd3iamkhw9wfrcoholVwqAvGJVCfYb6dru6q32Rtv8EQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是CV君。今天想和大家聊一篇非常有意思的新工作，它来自谷歌DeepMind和洛桑联邦理工学院（EPFL），发表在ICCV 2025的研讨会上。这篇论文叫《Sketch-to-Layout:</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644428&amp;idx=3&amp;sn=ce9c0820372c0953473262541a6441c4&amp;chksm=97ba618c71092a1afb0c5431ad50fbb01fa5094a7e4da0c7d4044bb89c92060f991110ee5ad0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 04 Nov 2025 20:16:06 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[多媒体顶会ACM MM 2025 最佳论文公布，从“看懂”到“会用”，再到“会思考”：揭示AI与物理世界融合新篇章]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvq2eUtOSXtcYsjFa6tJeRnJyV2qJ80NRTw3jiaqOd7vc0gvVCgAQXkSIblGzTTJaO6SJWSbXpCdHQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>作为全球多媒体技术的顶级盛会，第33届ACM国际多媒体会议（ACM Multimedia 2025）在爱尔兰都柏林圆满落幕。本届大会汇聚了全球顶尖的学者与工程师，以超过7100份的论文提交和突破200</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644265&amp;idx=1&amp;sn=dda6c5f67e31e157ee03ae59c7f0fb47&amp;chksm=97ea7bf58200669015b4b5e4f0eac0b8912c8be69cdc3ea5b2fb6411edef4bb1df8dc7972bbc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 03 Nov 2025 14:53:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[顶刊ISPRS (IF 12+) | 400+参考文献，遥感图像超分辨率最新综述！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvq2eUtOSXtcYsjFa6tJeRnSeCEbGbeVamweFc3jytCYRgHicqUWfkxgmyHAo7DfLM2cJ4AkbGibUog/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文题目：Advancing Image Super-resolution Techniques in Remote Sensing: A Comprehensive Survey发表期刊: ISPR</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644265&amp;idx=2&amp;sn=3e335bdec0139a1cc360cbdd110cf927&amp;chksm=97f2ab1da0ff3560d614199f9ead7e83a7aee407f25d3b2187012aadd8525e11c1cdf515ccbc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 03 Nov 2025 14:53:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AMD发布E-MMDiT：仅304M参数，单节点1.5天训练，实现高效图像合成新基准]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvq2eUtOSXtcYsjFa6tJeRnJcO1ON7sN4sB5FTic29QUg26C1iboepicjTHCDlicQEcYjcGL5YdLsanSw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自AMD的研究团队带来了一项引人注目的工作，提出了一种名为 E-MMDiT 的新型文生图模型。这名字是“高效多模态扩散Transformer (Efficient Multimodal Dif</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644265&amp;idx=3&amp;sn=42db45ac88cc5ab8477a64d126cb2882&amp;chksm=97a2ccf4bd51e9cf93a2a6c5bbee3326e339fe81d8c067f7ee9871bafaa565f00ee841f06d85&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 03 Nov 2025 14:53:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 卡内基梅隆大学空间可变自动对焦：单次拍摄实现全场景清晰，重新定义计算摄影]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuys6qRxFTuDlhM93JP4mXy7UzCnmJic4vl0szHYTX2jyiaaSCiafgE7574ZCnGJJaS7ibrsMZq1P5vqQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天CV君想和大家聊一篇计算摄影领域的奇妙论文，获得ICCV 2025 最佳论文提名。你有没有想过，如果你的相机能一次拍摄，就让照片里的每一个角落都清晰无比，无论远近，那该多好？或者，能像变魔</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644195&amp;idx=1&amp;sn=6fd179fc688e255484ff9f1f65bd5e68&amp;chksm=970092b6d23d8e5fa64d65f8335c68f2db060369805bf73883f801299a438c37d1db999b07ff&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 02 Nov 2025 12:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 电子科技大学联合A*STAR提出SCOPE：兼顾显著性与覆盖率，实现高效多模态大模型令牌剪枝]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsd9QUPq9NZeUdBlOaqwSq0t8KkHiaJOTKG8V31QKw50VruYowVS3mYXlF8spXhZy5bVBBwaTH5vicg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，多模态大语言模型（MLLMs）的发展真是日新月异，但它们在处理图像时遇到的一个“甜蜜的烦恼”也越来越明显：输入的视觉信息太多了！大量的视觉令牌（visual tokens）不仅增加了计算负担，拖</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644195&amp;idx=2&amp;sn=0b1d29f007e7c0d016f5ec0ae0ef6e62&amp;chksm=97ab801fda27786863670583e1994f5f5c13854933f5e3aa8f9fadf8a1010e6a8fbb1748593e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 02 Nov 2025 12:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[从 「会思考」到 「善创造」： 多模态大模型的深度推理与协同进化]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuJlaJicNdBOUbQ5tvSg3aL6iagibGNibibpmCjfTzc7KjCrQTMQibn1B14yD3L47zKYoicR2qGZ0hgwPSfg/640?wxtype=jpeg&amp;wxfrom=0"/><p>多模态大模型（MLLMs）的浪潮席卷而来，在基础感知、理解和生成等任务上的表现令人惊叹。然而，使MLLMs从简单的感知跨越到真正的认知，做到「会思考、善创造」，仍是多模态AI生态中的应用难题。面对复杂</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644182&amp;idx=1&amp;sn=4e62b406a77c1f0201d061b7fb7b8952&amp;chksm=97982c5403edf54106c9c7a20a0a51e104521d1a2d7a71c32d9b68f3a36ba280ad12b2ebdab3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 01 Nov 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[港中文&amp;港科大等提出FullPart：让3D部件生成达到全分辨率精度]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuJlaJicNdBOUbQ5tvSg3aL6w3FI9oVhicrBvVHZXuwibIxkGfqY3rDwIP4hoc9zX1Ujt34icZriafar1g/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，3D内容生成领域又迎来了一项令人瞩目的进展。来自香港中文大学、香港科技大学、商汤研究院和重庆大学的研究者们联手提出了一种名为 FullPart 的新型3D生成框架。顾名思义，FullPart的核</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644182&amp;idx=2&amp;sn=8b53d341c7c3f203826cb87a99a22ce6&amp;chksm=97d615d456d8ce7f176b2513e8418b7473c4ac516e104000380f457479aaa62e03e106ed9daa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 01 Nov 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[港科大（广州）等联合发布多模态空间推理综述：为大模型画下空间理解的未来蓝图]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuys6qRxFTuDlhM93JP4mXyX1MXic5kVVmAjQGgnsPyTraWDqarH6JkjtJPbS7ooSdxM4EmwyQnJ5g/640?wxtype=jpeg&amp;wxfrom=0"/><p>我们生活在一个三维的世界里，理解空间关系是与生俱来的本能。但是，对于近年来飞速发展的大语言模型（LLM）而言，这似乎仍是一个不小的挑战。它们或许能对答如流、妙笔生花，但在被问及“桌子上的苹果左边是什么</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644088&amp;idx=1&amp;sn=8e7153b9415ea9804bc43d3e3619e1d6&amp;chksm=9789510698228bc3a5044c20c8fe44a87321d4879f29cd34c6449eab9c416e6154c06ffbdb9b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 31 Oct 2025 16:37:59 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ReDiff：突破并行生成瓶颈，多模态扩散模型生成质量与效率双提升]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuJlaJicNdBOUbQ5tvSg3aL6S6x2SFFC9d2aSbrBZFvId5wT9Ar5A8FJic0xcWkU08WZKptXlicpBEgA/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model作者团队：香港大</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644088&amp;idx=2&amp;sn=90e7703081a8d60f29092118c01d53fe&amp;chksm=971207d72a285cacea38ef1758ca10f9fe0fd0042581845ea6440f219932a952a68b31bba46f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 31 Oct 2025 16:37:59 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[谢菲尔德大学提出Region-CAM：mIoU提升13.6%，破解弱监督学习中的目标定位难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuys6qRxFTuDlhM93JP4mXylVjDRzS7jDJv5UJOuMibfb1oNxtFjOz2MkJOPiawcUaDbhWVIoChicd2w/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天想跟大家聊一篇非常有意思的文章，来自谢菲尔德大学的研究者们提出了一种新的激活图生成方法——Region-CAM。对于做弱监督学习的朋友们来说，类激活图（Class Activation Mappi</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644088&amp;idx=3&amp;sn=eb2ac1777b71dd90a5c2f9b19c03b036&amp;chksm=973adc8e532533a571b4b5f1fa29b85d41f9f2ece55d59d07b118c6806f55ca09f9c25cc1155&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 31 Oct 2025 16:37:59 +0800</pubDate>
    </item>
  </channel>
</rss>