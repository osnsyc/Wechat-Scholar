<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM6aYkwkiboia6lA9D7ANy49WBe9icxn5NQqJjvn4Pyntzvfw/132</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[告别“边想边看”的高延迟！Zooming without Zooming 登场：10倍加速，小模型感知力反超千亿大模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/P9MWBRYIGWibm5ibxqXBY5tZHWickavibb6GSNm5dvjbL9scibmoicFzI9MicHjY2JD6AwSWetoAC8fVS9iaemAvV4jytZ62ZhC6Ak3hkEf2Ee3WNfE/640?wxtype=jpeg&amp;wxfrom=0"/><p>在多模态大模型（MLLM）的研究中，如何让模型看清图像中的“蛛丝马迹”一直是个难题。虽然最近流行的“边想边看”（Thinking-with-Images）范式通过让模型自主调用缩放工具（Zoom-in</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247656150&amp;idx=1&amp;sn=dd6a0ad9b582ceeec6325a9f8371fab9&amp;chksm=97ca786217a2e5f1a1075a7f784626d1dcebc0ba2b91c1af9822fadc8fca6686c74bc1b3ab6e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 19 Feb 2026 12:21:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[在“压缩域”进行视频理解，斯坦福&amp;微软提出 CoPE-VideoLM：视觉 Token 骤降 93%，首帧延迟降低 86%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/P9MWBRYIGW9EzBaibVT1YheQwe3yG84xibuk4DzCmzYkwo2fsSE4VHlbV0icvC1mmOicygS3NjJViasHj30CpZVvP7ic8iahsAnElQApCvfU8yJaBs/640?wxtype=jpeg&amp;wxfrom=0"/><p>前几天分享的一篇文章像 H.265 一样‘看’世界：OneVision-Encoder 开源，重新定义视觉 Token 的稀疏性引起了很多人的关注，将视觉Tokens稀疏化与视频编解码相对齐（尽管大过</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247656137&amp;idx=1&amp;sn=5b9e1be658e8efeedc22e51bac8f2243&amp;chksm=974e746060345fb7818e8036876808e07d383ea08034595aae8df65dbe8fb741fc8cd51a91b5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 18 Feb 2026 12:29:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[表格基础模型新标杆！TabICLv2 发布：创新 QASSMax 机制，纯合成数据练出最强表格 AI]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/P9MWBRYIGWibSHmxKUEbBaxt5UicoBjTqibohQHQDexeaRqwEekEml6aqlMfjsiazxCgECM0NEHib8ZqqKEZqFImicAfG81Oc4f8bcEuiaRw16G4FU/640?wxtype=jpeg&amp;wxfrom=0"/><p>在机器学习的版图里，表格数据（Tabular Data）一直是个“硬骨头”。尽管大语言模型（LLM）在文本和图像领域呼风唤雨，但在处理医疗记录、金融账单这类结构化表格时，传统的梯度提升决策树（GBDT</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247656094&amp;idx=1&amp;sn=988d37dda15aedce92497d21bd88d79e&amp;chksm=97342c88abdf016932c24131213482d01d579c9af4281c8471f54c7ed458257baaa160f0baea&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 17 Feb 2026 13:40:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[11.8倍加速！CMU等提出 MonarchRT：让 DiT 视频生成真正跨入“实时”时代]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/P9MWBRYIGW9hRSVibicuZGSFElkb2xkZkJtyCFjgKbZjWhdckzYj3mZ73leyAflChBccicqvDtFbeToeDydrgugkwPqoPcubUnKmXkSypdjvXw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在生成式 AI 的浪潮中，视频生成正从“能画出来”向“实时互动”演进。然而，想要在毫秒级的时间内生成一段流畅的视频，横在开发者面前最大的“拦路虎”就是 3D 自注意力的计算开销。随着分辨率和帧数的提升</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247656092&amp;idx=1&amp;sn=19c5f8078f13c3a29232347367749944&amp;chksm=97516a5cd3cbf97155144c66d90a95a42252ef982bcd096b3587b4259d4597f8684f7f33ac3b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 16 Feb 2026 23:52:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[像 H.265 一样‘看’世界：OneVision-Encoder 开源，重新定义视觉 Token 的稀疏性]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/P9MWBRYIGWicGdU94W5Ph0Yiaudmljnjic5wAXClN4MzLIszO3ERicWuEaLEZe1dXNtjoAdrb48S7CAHhyiab4rhlFxXicBWmIH5QAurIjBE0iaLvc/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligen</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247656003&amp;idx=1&amp;sn=fd401cd625a89ffefab9e4e5335245cc&amp;chksm=97e8e4afebab2d91eff32943d5fe37a9c292a810bfae21e52771570f12e9ead32ad48800e0dd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 15 Feb 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR 2026 | 澳门大学&amp;英特灵达提出FSOD-VFM：无需训练，图扩散助力“小样本目标检测”性能飙升！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/P9MWBRYIGWibLxmV8iaXDFvUTKke5TXXBZicDGcvu0kkHjoCZIEfhlaoK3GD6h93gia1C89QiabHHT3nibl2A5jS2cA2kiaGwE47C9icz34caX71dWw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在目标检测领域，小样本目标检测（Few-Shot Object Detection, FSOD）一直是个“硬骨头”。传统的做法通常需要在大规模基类数据上预训练，再针对极少数的新类样本进行微调。但微调过</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655945&amp;idx=1&amp;sn=c78255c4bae87ebc27f159b52dc64edf&amp;chksm=97f4765e5ab81894b36d40e11a882c98787cb3d679468a798c166f52505119f9c999a9582fcd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 14 Feb 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[中南&amp;新国大等提出MIND：首个1080p闭环回访世界模型基准，直面“记忆一致性与动作控制”难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/P9MWBRYIGWibIiaLbicFbL0sAibqNdPLGj57NpDyiaVKWSHykMgKWpjZ2c2ADWuGKw19kfmZFpe6MfDfSyCgR6MjzN7ibnpic5PvbhOS8MgYqicCfQQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近一年，世界模型（World Models）的概念火得一塌糊涂。从 Sora 到各种具身智能的模拟器，大家都在追求让 AI 能够像人类一样理解、记忆并预测物理世界的动态。但说实话，现在的世界模型到底</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655930&amp;idx=1&amp;sn=6d3fe14d317dbbf92da048f676d030bc&amp;chksm=97509eec3e7fc8b37454f459e621942422655790eb8d6d88611434c900c7f4109a23c270cdcf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 13 Feb 2026 18:10:33 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[视频生成新进展，Adobe &amp; MIT 提出 SCD 架构：将因果推理与迭代去噪彻底解耦]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/P9MWBRYIGW8mCqI4AjNuTkFJoKatyicLSoelxsNsNtzQgNJRVLd8INtTWHThLt24lSg0icSChybgCibDD4WQmtUzxUsCbEZgHANDLRSiaSW9xqw/640?wxtype=jpeg&amp;wxfrom=0"/><p>标题：Causality in Video Diffusers is Separable from Denoising机构：美国麻省理工学院（MIT）、Adobe 研究院、Morpheus AI论文地</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655890&amp;idx=1&amp;sn=65210586b443c79a1615dbd73bd00fda&amp;chksm=9730860df7681dd7ed71ebf483e69cd5537e58c196a0ebe7af51f6b0c0dfc40898079d666d05&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 12 Feb 2026 23:58:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[组合创新也可以很甜！ViT-5：全面升级视觉骨干，ImageNet 86.0% 刷新纪录]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/P9MWBRYIGWibWjSbuKvXkicmV2ibXwDGicDkUmf06P5h5BRf3ibAJtkiajJIVTIJ2dMBZYou1qR0MD5q988XBWe2ppiaFgVjoiaLYwFTthiaibpACzOiaw/640?wxtype=jpeg&amp;wxfrom=0"/><p>自 2020 年底视觉 Transformer（Vision Transformer, ViT）问世以来，它几乎重塑了整个计算机视觉的编码范式。然而，一个有趣的现象是，虽然大语言模型（LLM）领域的架</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655865&amp;idx=1&amp;sn=6ee86e81ea8c5ccaacba813488bc9ee3&amp;chksm=97e293316adb5f9c7d61dac7c3280f03e41b5cb613542256447ab3f0bfe66aae15ae80ac2ca3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 11 Feb 2026 23:10:11 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR 2026 | 复旦上交大联合提出 Efficient-LVSM，开启 3D 视角合成线性时代]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/P9MWBRYIGW85ErLJzpdtuxlvllx0zhzr04Gf7iaKbC4uylUKgibc5YrNuCbKibAojUbABE0EolCbJTPuUjD8EcXib5qcLpZ5qdmsw3gFp7WRNhk/640?wxtype=jpeg&amp;wxfrom=0"/><p>在 3D 视觉领域，如何从几张随手拍的照片中合成出丝滑的任意视角图像，一直是研究者们追逐的“圣杯”。从早期的 NeRF 到后来的 3D 高斯溅射（3DGS），虽然效果惊艳，但往往需要针对每个场景进行繁</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655825&amp;idx=1&amp;sn=e8d80105f773ba4106ac203a4052ba97&amp;chksm=97d1a1d188c5f2046699712d88c6ea068c8251be4dae5656dfdabdf3d4655c69c2fc33d70634&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 09 Feb 2026 22:47:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[开源AlphaFace实时换脸：大模型语义加持，41 FPS，极端侧脸也不“崩”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/P9MWBRYIGW8w3k4veW0ftibDp3ddEjeQ1vHWIedQuh9KfshBjYnrzUz9gicCClEvEUchHxW3JDVshXcZIsia5Le2ia1k2WYsEt6KXo94SdqmrqI/640?wxtype=jpeg&amp;wxfrom=0"/><p>在人脸替换（Face Swapping）这个领域，虽然我们已经见过了不少让人惊艳的效果，但只要稍微把视角从“正脸”转向“侧脸”，很多算法就会原形毕露。当脸部转动角度超过 45 度，甚至达到 90 度时</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655786&amp;idx=1&amp;sn=c2367810f7598119f966cca30bc31e5b&amp;chksm=97d6577be6f6743cf127af61d1cf5b0ed85f37c3e01e7b0c89326889370b5f7d31b1f816c4a5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 08 Feb 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[英伟达开源商用视觉大模型 C-RADIOv4：蒸馏 DINOv3 与 SAM3，参数量仅 1/10 性能却不相上下]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/P9MWBRYIGW98C6uyNPLbiaQoibhCCTlPkyC7F2gjUvIfR2H18FlibQbpjDDF6JyEV4MRcBYXMz5Anzespw9PFuvnk8ZaxXALTBYt0LmhacAQdA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在视觉基础模型（Vision Foundation Models, VFM）的演进过程中，英伟达研究院（NVIDIA Research）的 RADIO 系列一直是一个独特的存在。它不追求从零开始训练一</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655774&amp;idx=1&amp;sn=cab0f0abff0b2ae2914eac0f4f2a554a&amp;chksm=9749f2ec7457a3b7fe56ecfa32e2aa839fd5b5d3035aab509344e0ff3385c059fe4d25c34038&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 07 Feb 2026 23:57:37 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[VGG &amp; Meta新作CoWTracker：告别代价体积，重映射机制刷新点追踪 SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/P9MWBRYIGW9MpOggUhkhibXDpj0ZRpXqkWPibfeyMcGm0ca1qrVVQ30IeSYHn0xLbycIn6JEMkkru3MQkBmBuT0bpfZYcianwRPemXNZtYm2Kk/640?wxtype=jpeg&amp;wxfrom=0"/><p>在计算机视觉的长河里，点追踪（Point Tracking）一直是个既基础又让人头疼的课题。想象一下，你要在一段杂乱的视频中，死死盯住足球运动员鞋带上的一个像素点，无论他如何快速奔跑、被队友遮挡，甚至</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655717&amp;idx=1&amp;sn=ef4e05eb64f5a6c459e2e53510c180e8&amp;chksm=97c9e31aa89baea6316457abd30a308d882aaca68837b37be28ac6e31f93558f1d838ae4031a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 06 Feb 2026 16:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[被导师放养，后果可能很严重。。。]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu9WLwpNnpoykr52Gddy7cvsWpgzlnzj2BJHhrSiar301giadKVcVz3FX21nzaWMUSQ4CFHvNkt9Z8A/640?wxtype=jpeg&amp;wxfrom=0"/><p>被导师放养，你以为是没有压力，自由自在。实际上很多人没有自制力，被放养后没有方向，根本设计不出实验方案。甚至临近毕业，连论文都不会写。这就是放养最严重的后果：写不出论文，没法正常毕业。后面找工作/升学</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655593&amp;idx=1&amp;sn=db22c2c4b707a3e20acfe5fe8ef54bf1&amp;chksm=97ef93a16432878ec6880eb30725024212d9a7e9c8da377701a4aaf12588cd9985631851bad4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 05 Feb 2026 12:33:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[告别 VAE 伪影！北京大学提出 PixelGen：引入感知流形，让像素级扩散模型性能首超潜空间模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsV0a72oodJBl9FCgZJgtxpUWYge1MgDsCoRSR2Gna8WBtqpvfwLiadMlGYv24uj3UNf6ibCRexTdNQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>说起当前的图像生成技术，扩散模型（Diffusion Models）无疑是绝对的主角。而在扩散模型的江湖里，一直存在着两条路径的“争端”：一条是目前如日中天的潜空间扩散（Latent Diffusio</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655585&amp;idx=1&amp;sn=bd3866ea4f2c50b882d9ab2568b30117&amp;chksm=975acba59493fa4e9ef41c1b60c051719ed3ea1ffcc163e07a2886fae88f444c2be1deb19cf1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 04 Feb 2026 14:30:32 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR 2026 | 港中文开源 EgoHandICL：首个 3D 手部重建上下文学习框架，性能暴涨 31%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsV0a72oodJBl9FCgZJgtxpcY4AeAn1MuTM2VnVLqBV5LDYGJAsM2ialZxbcOk4Km9bYCdzVmY2bdg/640?wxtype=jpeg&amp;wxfrom=0"/><p>不知道大家在玩 VR 游戏或者看第一人称视角视频时，有没有注意到一个细节：当我们的手去抓握物体，或者两只手交叉叠在一起时，电脑往往很难精准地还原出手的 3D 姿态。这在技术上被称为“第一人称 3D 手</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655520&amp;idx=1&amp;sn=a65f7941b03a49d314434d84a3519d8d&amp;chksm=971cd0af90b5eeaf5365e0cfae45f01fd01e58b06bb75a51a2d845781acc0cf522aeb457cc81&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 03 Feb 2026 12:51:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR 2026 | 给多模态大模型一个“拒绝权”：MA-PaPSP 如何让图像描述不再胡说八道]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuib5eI7d3g1lY8CfwAV97aw14Rn3FYsFQzECxYrsPKXyHibotCHwUnQkrbT9CcbUH9oglNniaiauxI3w/640?wxtype=jpeg&amp;wxfrom=0"/><p>在多模态大模型（VLM）飞速发展的今天，我们经常会被它们生成的生动描述所惊艳。但开发者们也深知，这些模型偶尔会陷入“幻觉”，对着一张模糊的图片“振振有词”实则完全错误的描述。在医疗、自动驾驶等对安全性</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655493&amp;idx=1&amp;sn=d3724f04bfefcfd887609640d0f32f1b&amp;chksm=97273502ee5b676d0b391d82de7b6d52836a792f84967c56d3c9d9d944cd41454657b9694cb3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 02 Feb 2026 23:33:57 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[何恺明团队新作：Pixel MeanFlow 挑战单步像素级生成，FID 2.22 刷新“单步+像素级”生成赛道纪录]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs71axPmlsEiaPEopN3IyEI7KyPp352SL5roVKQ60outVTq90ALOmbBMEmqpibJ0Ntp2eQMMIVViadFg/640?wxtype=jpeg&amp;wxfrom=0"/><p>在当今的 AI 图像生成领域，流匹配（Flow Matching, FM）几乎已经成为了“工业标准”。但如果你仔细观察这些模型，会发现它们大多依赖两个核心设计：一是需要经过几十甚至上百步的迭代采样，二</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655451&amp;idx=1&amp;sn=03632965c64c5cbe63e7545c1d73450b&amp;chksm=9799e62d130d5397a5ce820675d6c08b6809c11de2d39d4916fa19de4e5bdbcd2dff38d45ffb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 01 Feb 2026 22:44:56 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 杰出论文 | 同济&amp;微软等提出 LLM2CLIP：补足 CLIP 难以处理长文本的遗憾]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTt31wj7Wz3lTkiciahibeLEliaI8vhEvwGqZKJacuZ0icf6Lo53v8aafkbHG6dUzBpaa5euKFH39WKuQ1g/640?wxtype=jpeg&amp;wxfrom=0"/><p>在多模态大模型（MLLM）和生成式 AI 飞速发展的今天，CLIP 依然是连接视觉与语言的“基石”。然而，随着应用场景的深入，大家逐渐发现 CLIP 的文本编码器（Text Encoder）正成为整个</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655425&amp;idx=1&amp;sn=c7e532098a3cf52818a039e9e08d0678&amp;chksm=9792913a4eaf3eee6b64aa44c281c36ae0edaff0839157e1bf9e0a3aa56cd19af7a0bc1fb2ac&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 31 Jan 2026 23:52:39 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IEEE TPAMI 中科院等提出DPSD：无需原始数据，实现高精度隐私模型转录]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTt31wj7Wz3lTkiciahibeLEliaIPmJu6LT940YqYfDx1Knq1GLcaicibTFnlXwwacanFWz2nfPqGzS7D6sQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在深度学习大行其道的今天，我们习惯于将训练好的模型部署到各种终端。但你是否想过，这些看似“黑盒”的模型其实并不安全？攻击者可以通过模型反向攻击（Model Inversion Attacks）等手段，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655406&amp;idx=1&amp;sn=d9de088050f51c8d8c43e2f52691c34c&amp;chksm=971e428e0a533139d3a7fa106385d1c53cd9bcecac71fa70ea88436fd124a89a7ec1e5533b5f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 30 Jan 2026 15:01:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICLR 2026 | 北大&amp;腾讯开源 GenCompositor：视频素材“拖拽”即合成，生成式视频编辑迈入新阶段]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTspoibFcZLeGeybGBfw9wBFk9cdKWQO4hay3rEjH6yZCcdAYZNZOibmt6iclMhWibrtN6wJXJ8PwpdiaZQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在电影制作和短视频创作中，将一个视频里的动态元素（比如一只飞舞的蝴蝶或一场爆炸特效）无缝嵌入到另一个背景视频中，一直是一项“精细活儿”。这不仅需要剪辑师手动调整位置、缩放和运动轨迹，还要处理光影和谐化</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655382&amp;idx=1&amp;sn=1ef5c8a18091ea4e511fd5c875ace832&amp;chksm=97e7c32bbdc820a9882bb678be7d43e367849d02b767413526e19f28cd00926c3be5bbece71f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 29 Jan 2026 21:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AI4RWC@CVPR2026: 人工智能在真实世界中的挑战Workshop征稿中]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTspoibFcZLeGeybGBfw9wBFkbTDxukWxDEKPgKjep1ichWviaqpXtsprNRgh5L67EFnSqicicKnydOLIQA/300?wxtype=jpeg&amp;wxfrom=0"/><p>CVPR 2026 WorkshopAI4RWC: The 2nd International Workshop on Vision Intelligence for Real-world Chall</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655382&amp;idx=2&amp;sn=f1d5ed3230e0028139ad9c1a86a1c2fe&amp;chksm=97f73d1e3cc49ca2137d52fea7e2756c24118c0c2f4fac1efe491c48f96f50dd33869d9df47d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 29 Jan 2026 21:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 | 山东大学等提出 SAPA-Bench：首个大规模手机智能体隐私意识基准，主流模型普遍不及格]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuDMdIkicaegvfhFjwaen3DJHf6l9ho23EJfZy8OIh5WnHABXzIDB4faUf6RKibodoQhfm4s0uZBc4A/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着多模态大语言模型（Multimodal Large Language Models, MLLMs）的爆发，智能手机智能体（Smartphone Agents）正变得越来越“能干”。从简单的发消息到</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655301&amp;idx=1&amp;sn=10a641c4be2bce0747c4f29d9ac9140c&amp;chksm=977d85a2026401b51ce48fcf1e5fcb7b64cfc3648110984c6a3152ceb99f14b6b677bc9fe112&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 28 Jan 2026 13:27:45 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NTIRE 2026光场图像超分辨挑战赛正式开赛 | 验证平台已上线！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvvHE3zvianCbjrOxORYIoeqBeCgC7BWPllP2KgDLW82CSZatJZqbm2CqFIYhx38yuqQhjgxsT97sA/300?wxtype=jpeg&amp;wxfrom=0"/><p>光场图像超分辨挑战赛 （Light Field Image Super-Resolution Challenge）将作为NTIRE研讨会的一部分，与CVPR 2026一起举办。NTIRE全称New T</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655301&amp;idx=2&amp;sn=986476d7c4f394c80fdf519085b71018&amp;chksm=97a2d69e5f854900464e140f4dc423810c09656f4742046860949bb496df41fd05cbff46d5ae&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 28 Jan 2026 13:27:45 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[目标检测2026年好发论文的方向！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuJABB18ZECJsJ5V5MRC3tK2BHEYlwyB8ukSvLY6UDrn1TPvl6gn20ib6I1Jnrmib87MmqaBxGp0xfQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>YOLO实在卷不动了，不知道目标检测还有哪些baseline好用？不知道怎么选？实际上DETR系列都是好选择，也一直很火。包括RT-DETR系列、DINO系列、D-FINE系列等，近来更是出现了很多新</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655249&amp;idx=1&amp;sn=218afc083758716157623d61be3560ac&amp;chksm=97a907881d23ecdc833f95055bb020295b8a5238dd31678478f5a77bf9a27f39bf44b038f8c8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 27 Jan 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2026 NTIRE 第三届真实场景图像复原（RAIM）挑战赛震撼启动！三大硬核赛道等你来战！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuJABB18ZECJsJ5V5MRC3tKnNyEA8K1ECbxgbaGz1R88MPzZBBIRrYtxXLnMUviaMXibhRXr7ruzBIw/300?wxtype=jpeg&amp;wxfrom=0"/><p>CVPR 2026 NTIRE 第三届真实场景图像复原（RAIM）挑战赛正式拉开帷幕！当“多模态大模型”遇上“专业的画质评价”，当“计算摄影”挑战“动态复杂场景”，谁能定义下一代图像处理的标杆？今年，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655249&amp;idx=2&amp;sn=7b2702e94a4897f390700ac79626bcfa&amp;chksm=97390b7b5113896ab3f824923384319590b8542dc0edd22c44039dca118098af38384d96f930&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 27 Jan 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[首届 CVPR 2026 AAVM Workshop 征稿启动：Agentic AI 开启视觉媒体创作新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvvHE3zvianCbjrOxORYIoeqqmy6IduyTXuy6D2wZ26kq1pbVcNwKeCVQiahcdjU5NxZDITeY6WNmGQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着生成式 AI（Generative AI）步入深水区，视觉内容的生产已不再满足于“单点生成”。如何让 AI 从单纯的绘图工具进化为具备思考、规划与行动能力的智能体，已成为计算机视觉与多模态研究的最</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655205&amp;idx=1&amp;sn=632da447efadda60fa65356d781b4db3&amp;chksm=9735deae4cb18f408253832ced632b5ce3c19eb5557189d04ea0faa4a3faade0ddf159cea1a0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 26 Jan 2026 21:41:46 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 北邮提出PBC：告别高分辨率图像“乱码”，内容丰富度飙升！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv3ShrnTyVabuq2H0y19pb3pHkcsSeleKaNLHuHWUyMnqiaBY0tyWDlhadibOiaJFZgtAAzAVUia7p9Fw/640?wxtype=jpeg&amp;wxfrom=0"/><p>文生图大模型现在这么强大，那是否也可以用其做更高分辨的图像生成呢？是可以。但许多研究者发现结果常常边缘模糊不清，或者画面里出现重复的物体，甚至整个构图都“崩坏”了？这就像是给一张小图简单粗暴地放大，细</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655152&amp;idx=1&amp;sn=72eef83e5826d1f17ad6e28dac94a0ce&amp;chksm=97369d71010bd1a88affd60a027180ea966b5846c8cd2bfa553723f7743ac1a1fcb6ce4d9406&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 25 Jan 2026 07:03:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NTIRE 2026 第一届盲计算像差校正挑战赛正式开赛]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv3ShrnTyVabuq2H0y19pb32VMdbHyUk52YzofXHcZO9rZJYOq7IxNmlJoY6TWmjsawNiam4PPSgJw/300?wxtype=jpeg&amp;wxfrom=0"/><p>本次挑战赛作为 NTIRE 2026 研讨会的一部分，与 CVPR 2026 一同举办。NTIRE（New Trends in Image Restoration and Enhancement） 是</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655152&amp;idx=2&amp;sn=ae397cfe1ad38f734fd9d212c1deab74&amp;chksm=979aa628a55830993fc6070ca6da79264737044e95aaac510c6e57c2571d148181d0c7b43f0b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 25 Jan 2026 07:03:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 BuildingWorld：面向 World Models 的全球结构化 3D 建筑数据集]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv3ShrnTyVabuq2H0y19pb36O4ukk7MLr41ibVrpB8n3iajAiculCqFwa0qvcVmTZ7uMOAEo1lTYSR5g/640?wxtype=jpeg&amp;wxfrom=0"/><p>关键词：五百万LoD2建筑物，五大洲，Cyber City项目网站：https://szusic.github.io/BuildingWorld标题：BuildingWorld: A Structur</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655101&amp;idx=1&amp;sn=e14d502ef9456dcc7080e5f9a88ddf2a&amp;chksm=9712f032c92126e4f28379382f7a34f81f79046226babe408d5f80f4e78c8207eb259e6c8630&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 24 Jan 2026 23:03:34 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2025 | 北邮发布T2I可控生成全景综述：从条件到通用，一览模型创新与应用前沿]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvw09RdicP5Uy5Z9glHicgQTMgP1X2aWK8Eh5kbYsgBDYyEe4RFZyTn7x9rBWX1zxFiaM88RibGcI5Jcg/640?wxtype=jpeg&amp;wxfrom=0"/><p>扩散模型（Diffusion Models）的出现，无疑为视觉生成领域带来了革命性的突破。它们以惊人的文本引导生成能力，让“所想即所得”的愿景越来越近。然而，仅仅依靠文本来控制这些模型，在许多复杂和多</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655078&amp;idx=1&amp;sn=e0b876f6ad3e399f45392ff4ce4a9cfc&amp;chksm=97ce36e93e2bfb99071d9bac907880cc103b9793e142938c27521fd0eca2d87b3444f98e9014&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 Jan 2026 12:45:53 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[【3月热门EI会议盘点】多学科覆盖，论文快速发表指南！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvw09RdicP5Uy5Z9glHicgQTM2gCdAibTiciaW6y2xPdDSjQRthk4MsrKiaHl5vCz6I1QWAfXDicHia1ay70A/300?wxtype=jpeg&amp;wxfrom=0"/><p>2026年3月高含金量学术会议日程速览 2026年3月，国内还将迎来多场聚焦前沿科技的国际学术盛会，涵盖人工智能、网络通信、材料、计算机安全等热门领域， 欢迎各位学者下滑查看近期国际会议时间表，并选择</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655078&amp;idx=2&amp;sn=1ed1e0d2965d87bc3a6740567ef9e5b3&amp;chksm=97fa445dbe65a3a41d6cd4006d5b0135c3c2319e0abf41fbbec5835ff776cb17eff3867de030&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 23 Jan 2026 12:45:53 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[大连理工开源 Think3D：将 3D 重建接入思维链，VLM 空间推理性能显著提升]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvAiaria5x4coTPLhHe72fSfiaUDz1XEMtia4SNF4RlZibHSuNECuwmBVdRbRtzNWWNcw0bN9VIoze5XGg/640?wxtype=jpeg&amp;wxfrom=0"/><p>在视觉大模型（VLM）飞速发展的今天，我们似乎已经习惯了它们在图像识别、文档理解甚至艺术创作上的惊艳表现。然而，当你要求这些模型在 3D 空间里“转个弯”，或者判断几张不同角度照片里的物体相对位置时，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655007&amp;idx=1&amp;sn=2e0f55440b9d36edfe0b96d972da5cad&amp;chksm=97b66c77083880bc60c73ac1f0b86541e88c7b4f2f991f789c829de1a55f472ec3bf3fad875f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 Jan 2026 13:22:34 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[实习 | 启创InnoSpark大模型研发团队招聘实习生(上海)]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsctB59lNEDjBVc1AsGSv3E6KG4vTeH9uSBDs4t8DjlBVOyibZUqC9qX3O76MM4SianibicoVG50haJlg/300?wxtype=jpeg&amp;wxfrom=0"/><p>公司介绍启创InnoSpark大模型研发团队由华东师范大学上海智能教育研究院与上海创智学院领导，团队以服务国家及上海市“人工智能+教育”重大战略工程为使命，专注打造具有自主知识产权的中国版教育大模型。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247655007&amp;idx=2&amp;sn=3c5c57149dd4b1301484ee68cdb917f0&amp;chksm=977cadfb31ef5ae5f80f813fa055aa107f61ecc5e9dd15b5399b0fa2e72373604be1899a9c1e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 22 Jan 2026 13:22:34 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | 无标注微调！北邮让大规模扩散模型实现高质量域内生成与可控性]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTslMnibNts6LibW2YLAaaXA2SXyX6wpJ83qorztriaEj0facD4KHlJpialjlqxYr9Y7uNDdicTvJQor4vQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在人工智能生成内容（AIGC）领域，扩散模型（Diffusion Models）已经展现出惊人的创造力。然而，当我们希望这些强大的模型在特定领域（比如生成人脸、动物或特定风格的艺术品）内表现出色时，常</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654982&amp;idx=1&amp;sn=9cbe2e58e38f57ae0e57f32e63c03c06&amp;chksm=970c45368df3ca432181865a847b9fd3855657c3e3e5880f48400c7771f88e5b19b0b51f2e97&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 Jan 2026 21:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[YOLO26深度解析：彻底告别NMS，以三大“炼丹神器”定义下一代边缘计算视觉]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTslMnibNts6LibW2YLAaaXA2ScNoamcEicoFV1BLKVxtrnibxQ1tekZS80kVKjB1NxfKVRRcUYoRicXNBg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，YOLO 家族又有新动作Ultralytics YOLO26发布：专为下一代计算机视觉而生，边缘视觉AI的新起点。虽然官方尚未发布论文，但 arXiv 上出现了一篇来自 KIIT Univers</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654982&amp;idx=2&amp;sn=1dc59610f78d977042743055c3e65c0e&amp;chksm=97311ba1bdd1f35be8c3af9ab45c173059a1e99260427f49f4faf24707a891cc1936b037ed25&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 Jan 2026 21:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[天控智能招聘，应用数学、计算数学方向]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsctB59lNEDjBVc1AsGSv3EsUbonN1JKicm0XzJOT9791kwd0IgicrKv3KCdHjTheVYkfJaNQUKjr0A/300?wxtype=jpeg&amp;wxfrom=0"/><p>天控智能是国内最大的精密磨床厂家，拥有独立的国产CAD/CAM软件研发部门。现面向应用数学、计算数学方向的硕博学子发出诚挚邀请。（研究生需三年经验）欢迎感兴趣的朋友咨询、投简：[tnca@tncalt</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654982&amp;idx=3&amp;sn=c531b35ef851fa1dd00339bace5322b6&amp;chksm=97dbb1444858a999bb44b79d3deb2992525bc0da10eba702aab01e614c57d789d8114f70dcee&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 21 Jan 2026 21:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 哈工大提出ITKM：用图文知识统一多场景行人重识别]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuD2aHIiaCRLwia0vn3YfIWiauiarmoVfkWQvRrrFFvYfFNWD16y8MVicTALEmb5NNicTXz24ayy9Gq3FVA/640?wxtype=jpeg&amp;wxfrom=0"/><p>晚上光线不好、监控摄像头分辨率太低、目标换了一身衣服……在现实世界的行人重识别（Person Re-Identification, ReID）任务中，这些复杂多变的场景常常让模型犯了难。传统方法通常是</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654899&amp;idx=1&amp;sn=2782c4858cac43012768e0517963b631&amp;chksm=9740ec941b9c0bee8bd98ba86c302391a4e2c239cf2c0aed52f48424ce977ac634d91f3dfaec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 20 Jan 2026 12:37:51 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[计算机视觉方向的CCF-B会议，视觉顶会“地狱赛”！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvniaHAqcbxxztJJqD7nibok5xB4Z42oW3ibms6ibH8wQ4qicNZn55KrU4Vjq9thJcCfTsA8M258Ycsdvw/300?wxtype=jpeg&amp;wxfrom=0"/><p>计算机视觉人必盯的 CCF-B 会议来啦！ECCV 2026 投稿倒计时，这场“硬核赛场”等你来战～ECCV 2026（第 19 届欧洲计算机视觉国际会议）是计算机视觉领域公认的旗舰学术会议，与 CV</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654899&amp;idx=2&amp;sn=84491f7fda143512298ae9544b51ee5b&amp;chksm=974012dadcb1eb6c204138b16531be40f3ab8815e499f0c1285a1a2e583aca67fddfc1c053d0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 20 Jan 2026 12:37:51 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[WACV 2026 | 310万工业图像，444个类别！ICONIC-444数据集发布，专为真实世界OOD检测打造]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsctB59lNEDjBVc1AsGSv3EDEMj3OBu3bKlT51azWibzDqy4KyzyC5sEDS1uSS58otiay1AdDgHibxWg/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊聊一个在机器学习安全领域至关重要，却又时常让人头疼的话题——分布外（Out-of-Distribution, OOD）检测。简单来说，就是如何让我们的AI模型在遇到它“没见过”的</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654866&amp;idx=1&amp;sn=50c94a8c6e3f1670aa920cf5fef84dda&amp;chksm=97df8b314b5c194d0c06cf9e3ebc714a39aaa3876bd4d15dd00f2e304268c39a250ecec20720&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 19 Jan 2026 23:02:11 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[视频分割新SOTA！3AM融合3D感知，大视角场景上IoU大涨15.9点，完胜SAM2]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvml2GMlT9HmbGib584htzCHGw4ichoB1dOw36wjyOv60kDhH3ibG9UY9ZdNoiazN6ygrNTT9ys1GW1HA/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有趣的新工作，来自台湾阳明交通大学和英伟达研究院的研究者们联手打造的 3AM。如果你和我一样，一直在关注视频对象分割（VOS）领域，那你一定对 SAM（Segment A</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654803&amp;idx=1&amp;sn=b013f0250e865db93d9bcf58b1945cd5&amp;chksm=9780cd4e90aa024cb921804056d034fa7622a9c5b6c05886250e7cf564c8feb8f87c4d1d9949&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 18 Jan 2026 22:34:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[告别视觉瓶颈！蚂蚁集团、同济大学联手提出CLI，让大模型“看”得更准、推理更强]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuuiccwvsUrxbwJuqNJVTQrlVKntSH9HicC42zd0XfeRGDZ0yxgXkKWgBAdEr8AqHmibOZloSLJbqAAQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天想和大家聊聊一个最近在多模态大模型领域很有意思的进展。我们都知道，现在的大模型（VLMs）看图说话越来越溜了，但你有没有想过，它们“看”图的方式可能有点“死板”？目前，像LLaVA这样的主</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654771&amp;idx=1&amp;sn=00bba3cd629e0206fbc4c571be4be679&amp;chksm=97959f6ed04a36ba2052633ec6800d00c349f8cd9115567bee6bea312a1d48b18e64309510d3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 17 Jan 2026 21:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[2026年截稿的重要CCF会议DDL目录]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuuiccwvsUrxbwJuqNJVTQrlELq63Q1Bv0Na90MBYXVWE7uJfeVmMibpDlbll71n86X0zRVOyB0UbfA/640?wxtype=jpeg&amp;wxfrom=0"/><p>2026年新的一轮CCF刷榜开始了！精准把握计算机方向会议等级分类、截稿日期、rebuttal时间、征稿频次是顶会中稿非常重要的4打因素。时间把握的准确，一年可以4篇A会，把握时机精准转投。反之，一年</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654656&amp;idx=1&amp;sn=588f0382a08b7d21c378d96c6d789486&amp;chksm=9757a87bb7842753819fd4865a437f3bcced594af6049060cb4357f932607bc5a8f3eb3b6467&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 16 Jan 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Ultralytics YOLO26发布：专为下一代计算机视觉而生，边缘视觉AI的新起点]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvriaAodVGM5wWcgesQY9q4UzicMQVTX1kD6efHYEfobv7OUPqaCM2LxibfJSj9eJ8ybQ8xvAoa3HUxw/640?wxtype=jpeg&amp;wxfrom=0"/><p>近日，Ultralytics 正式发布 YOLO26，这是迄今为止最先进、同时也是最易于部署的 YOLO 模型。YOLO26 最早在 YOLO Vision 2025（YV25）大会上首次亮相，它标志</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654614&amp;idx=1&amp;sn=ac96d447cc8e7a0233f6877d10e93409&amp;chksm=973258a15ce592572ae1404b065c43d664f0b17a0d5db0a0a0bca7347c5f953331a124636160&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 15 Jan 2026 21:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦大学推出CME-CAD！异构多专家协同学习刷新CAD代码生成SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuXYQTEQjzWIGicejmH6FeN0LicJqxUEJq1ZHlHGUgyYcaIcMVLdTzogWSfDCwHEbwU5Lzyzr92aqCQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文为粉丝投稿。工业设计领域，计算机辅助设计（CAD）不可或缺，但传统CAD建模的复杂度及流程难以实现自动化生成高精度、可编辑模型的目标。现有技术（如从草图构建3D模型）常产生不可编辑的近似结果，无法</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654431&amp;idx=1&amp;sn=f2d9d0e32b1299121d59a75e735b39a8&amp;chksm=97b8c92bc33d741776163051bb5f5db2a6d345b38d246e225c4e04491cd482d5c62f60f86ccb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 14 Jan 2026 13:49:18 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[真实世界3D分割新范式！MVGGT：融合视觉、几何与语言，性能大幅超越传统方法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuXYQTEQjzWIGicejmH6FeN02PGCEFhzQ6xVCnIZgcrw7MyynxrsAhia5iaibfh6yIl0licr8X1wMv2bRw/640?wxtype=jpeg&amp;wxfrom=0"/><p>当我们在谈论让机器人或AR设备理解并与三维世界互动时，一个核心任务是“指代性分割”（Referring Expression Segmentation, RES）——即根据一句自然语言描述（如“左边那</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654416&amp;idx=1&amp;sn=2f9e9ec6075561c3d4c2d61ca0026736&amp;chksm=97093fefc2c95ecd13db2bd6fcac2d0c207d21b5edbe293e5babfea220f007b234de784d2719&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 13 Jan 2026 21:38:35 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[MLLM想得多也降智？Meta新作VideoAuto-R1：首创“二次作答”机制，推理提速3.3倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu6oNAfnlzLEhKj66UsQPnB8Tvcq8f5HbBGWqrs4oorvgPibdgEHhsJ3CTEtz770fl54loguw6dezQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，多模态大模型在视频理解领域可谓是高歌猛进，尤其是“链式思考”（Chain-of-thought, CoT）技术的引入，让模型能够像人一样，通过一步步的分析推理来解决复杂问题。但你有没有想过，我们</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654189&amp;idx=1&amp;sn=41f18de3f62799baa7fd6be54f6add08&amp;chksm=971bafe6725e120f45789f46212474c917f01de76c0904f70eaade105ba5b58c334d85fd0d95&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 12 Jan 2026 12:46:41 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[3D场景生成新突破！浙大、字节提出Gen3R：结构高手+美学专家，单图“脑补”完整三维世界]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsicSwwOfhcwCqHGKj45xUgf89ibBP4mx5VdOm0XWiah7NwHhLEPiaauzZgvB35ey7gib8icibGkn6MwyfjQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天想和大家聊一篇非常有趣的新工作——Gen3R，它巧妙地将两个看似独立的领域——强大的3D重建模型和前沿的视频扩散模型——捏合在了一起，实现了1+1>2的效果。过去，我们想要凭空生成一个三维场景，要</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654005&amp;idx=1&amp;sn=64d1e05bdbc1b7c2a569ecdf2d41bb95&amp;chksm=97874a4c2cb8210c5cb48bb559171fed3bf99aa03d2b9879398b04f43fd15c0000c4b920fa43&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 10 Jan 2026 12:34:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[2026年1月截稿！AI领域CCFA/B/C会截稿汇总]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvniaHAqcbxxztJJqD7nibok5nIiazzDMlToo6NDTkqxDCBOqkyyjq0k1FE4FmX0FadcwzoFJWZ58VdA/640?wxtype=jpeg&amp;wxfrom=0"/><p>2026年的学术征程已悄然开启！对于人工智能领域的科研人而言，2026年1月堪称关键截稿窗口期——多个CCF A/B/C级权威会议集中收官，既是检验研究成果的黄金赛场，更是链接全球学术资源的重要契机。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247653693&amp;idx=1&amp;sn=3347cb645520f3d9af1f1007927a4f2a&amp;chksm=972451db482904d114560a2cc071d724a83a2f226c09e5a6422473b7a0d9fe0e4aa013ede3a9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 09 Jan 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大、理想汽车联手推出InfiniDepth：用神经隐式场实现任意分辨率深度估计，效果惊艳！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtU3cHtAOp6biaDaQfmHPfzhibicSqXqVk4QVKxktRgaQdMbQc38ofw0lY6dqE7OsxiaKjwHKwW6cMWMQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>AI生成的深度图总是感觉有点“糊”，尤其在面对栏杆、树枝、人物头发丝等复杂场景时，细节总是表现不佳。这背后的一个核心瓶颈，在于长久以来深度学习模型都将深度图视为一个固定的、由像素格子组成的“棋盘”。这</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247653676&amp;idx=1&amp;sn=986f4fb4e8e85019900a6eaa8490a5f7&amp;chksm=974580b4682ae9f333b5730b848530ef6d66e7979fa69d3b13c1a4ebeea18739bb31b3f093bf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 08 Jan 2026 13:18:12 +0800</pubDate>
    </item>
  </channel>
</rss>