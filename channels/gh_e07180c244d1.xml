<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM6aYkwkiboia6lA9D7ANy49WBe9icxn5NQqJjvn4Pyntzvfw/132</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[从“Spider”到SAM 3：概念提示分割小考]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTutZfTz8odOVKG5JeDIvbddaRnLmdGRyHTaABfvLOMAtwa0mBImBGMZhia6P1UuTl7u9YmhSibc6wZg/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，SAM 3 以概念提示分割再次引起计算机视觉研究社区的注意。图像分割技术，作为理解视觉世界的基石，正从为特定任务（如车辆分割、息肉分割、伪装物体检测）训练的专用模型，迈向能够“分割万物”的通用大</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643251&amp;idx=1&amp;sn=2db2650f8af7554a5397c1c2c1421cba&amp;chksm=9769a31677d81771689c0c7505c6e1b9f57b890d55006523d6bf8becb3de69cec5d91b6d6493&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 25 Oct 2025 13:45:37 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[从「会画画」到「会思考」：快手可灵团队提出 T2I-CoReBench，最强模型也难逃推理瓶颈]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmgX4BiagIB2nM5OS3lv0RW37xOZvUDnfnmr36YPfOTYQoyIGBxaXUZALgFz4nY3RrPlK1c0zrE8w/300?wxtype=jpeg&amp;wxfrom=0"/><p>当前文本生成图像（T2I）技术早已不是画出来就行。从 Stable Diffusion 到最新的 Nano Banana，模型能轻松生成指令一致的简单画面，但要生成繁忙厨房中的 30 余种物品或绳索断</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643251&amp;idx=2&amp;sn=7999b166c93dbb875c2c8fd977f01ec9&amp;chksm=97a34ad5a83f2770dfd429aca15765f92327de7a30c12c90ee6cbb32dff951379eab0dc29810&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 25 Oct 2025 13:45:37 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Meta新研究Free Transformer：仅增加3%成本，让大模型生成更多样、更高能的回答]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmgX4BiagIB2nM5OS3lv0RW7elnEBaOLB56050OBGECjGibWlP6qeeWbrWXKvaLnEBPYy7rHx6llsg/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天和大家聊一篇来自Meta AI的有趣工作，论文标题为《The Free Transformer》。这篇论文给我们带来了一种新的思路，旨在解决当前大型语言模型（LLM）生成内容时略显“死板”的问题。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643050&amp;idx=1&amp;sn=064efb35b0b125312aa408ebf9f43fe2&amp;chksm=975762f6d24850e77179e61fef69b3122a7ed2cf685715222f412f720e23ac367680ca05fafe&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 24 Oct 2025 12:14:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 牛津大学等提出Memo：Transformer强化学习记忆效率提升10倍，具身智能体泛化能力更强！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtagea8ibEM2HC7joYu7jw6icMRmiaXzUNsvVJqG2K3gmrnIDce3V5PMGmwwQRFwKAAFxZu36UA84Miaw/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天CV君要给大家介绍一篇来自牛津大学、佐治亚理工学院和丰田欧洲的最新研究，这篇论文题为“Memo: Training Memory-Efficient Embodied Agents with Re</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643050&amp;idx=2&amp;sn=03a78b61fe73a527a5ccd4e0c2924823&amp;chksm=9744d6d96c243b56df5ab161907c33c752ed72569588824fc5893e1d1375ca9e23ea3cd1ae2d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 24 Oct 2025 12:14:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[港科大联合港中文、字节跳动推出DreamOmni2：不止修图，更能领会意境，让AI绘画大师拿捏氛围感！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmgX4BiagIB2nM5OS3lv0RWtS8JuKWpibP37dDhV8jzZ47J6o8Qqu0drL47fBPbrujey2PpN7OSDdQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>和大家聊一篇非常有趣的新工作，来自香港科技大学、香港中文大学和字节跳动的研究者们联手打造的 DreamOmni2。我们知道，现在的AI修图和绘画已经很强大了，但它们似乎总是“差点意思”。要么是只能听懂</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643050&amp;idx=3&amp;sn=4806852d8291ac328670b1caf40163d2&amp;chksm=97fd9281a6d18d2c84a0dfdc0d41c589a69ec885e47fbabe2fe2dd000991872e0f7ecf4b0e24&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 24 Oct 2025 12:14:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[OmniNWM：突破三维驾驶仿真极限的“全知”世界模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv0ibbT1cQ4rJrO5YH6O9bXBqSKuMpuGaDHkog3p64tq0ZELtIlNRIIEUaQIJ6flibUJ98dTOQzEUMQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: OmniNWM: Omniscient Driving Navigation World Models作者: Bohan Li, Zhuang Ma, Dalong Du, Baorui</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642957&amp;idx=1&amp;sn=b2c7b83a0a1cb877a2971365eac44c40&amp;chksm=9730f9874ae7d825ce2be337967a7787e0d252128f9606a6f3aaacdeb1cc7d883eaa0b750ef8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 23 Oct 2025 17:03:04 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[英伟达将开源 CuSfM：CUDA加速SfM，精度与速度超越COLMAP，离线重建新标杆！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvlA3fubIfltG7EzoNaqQibian4BwJ3ch2HXiahkljRZ53B5xW9OPnYkUQWz9G1hpiciadEKsiaLc3xicMHw/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天CV君想和大家聊一篇来自英伟达（NVIDIA）的最新研究，他们带来了一个名为 CuSfM 的系统。这个名字听起来有点技术范儿，其实 CuSfM 是“CUDA-Accelerated Structu</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642957&amp;idx=2&amp;sn=09af9334e21f2cc1afb9c0cea5ab0e87&amp;chksm=97a7837c46b9128844b805602abef784c9d04ef47007021f86c40b923fc24b482221bf108375&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 23 Oct 2025 17:03:04 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 最佳论文公布！卡内基梅隆大学提出BRICKGPT：让AI化身乐高大师，文本生成实体积木，还能保证搭得稳！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv0ibbT1cQ4rJrO5YH6O9bXBk4oibRfjxicKRmHGJ4GxS2JkpNKjyK7KksxlX2hnibWQ8Rd6vKdwusBxA/640?wxtype=jpeg&amp;wxfrom=0"/><p>刚刚ICCV 2025 大会公布了最佳论文和最佳学生论文，最佳论文由卡内基梅隆大学研究团队摘得，从11000多篇投稿论文中被选中，该论文有哪些值得关注的点，我们一起来看看。最近，生成式AI在3D内容创</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642633&amp;idx=1&amp;sn=86b75fbd2b146171c2e5761e55d7ee23&amp;chksm=97d3d35b2da68393bb0112b056f01bb66e76e25590a8faa35486cd5869329a62151f22f074ef&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 22 Oct 2025 10:58:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025最佳学生论文 | FlowEdit：告别反演，一种更直接的图像编辑范式，结构保持力SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv0ibbT1cQ4rJrO5YH6O9bXBhoBu9IHk759k73kuat5FdNefXmfvICoICMyL92nJ5VAPwSIZPehc3w/300?wxtype=jpeg&amp;wxfrom=0"/><p>刚刚ICCV 2025 大会公布了最佳论文和最佳学生论文，最佳论文由卡内基梅隆大学研究团队摘得，最佳学生论文由以色列理工学院获得，从11000多篇投稿论文中被选中，这些论文有哪些值得关注的点，我们一起</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642633&amp;idx=2&amp;sn=3d3b648ed7447e718cb2361af971933d&amp;chksm=979d3093a506faaa8ed4fa50f74c8aa8a17f15394f326c493d28e73c4dfb12d21afbd4c66455&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 22 Oct 2025 10:58:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 上下文学习新方法：RH-Partial2Global，一个面向可靠与全面视觉的提示选择框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvlA3fubIfltG7EzoNaqQibiaPFuB1NDM8ibTaSE3re1GMa2hjFShypKsmpuDGo5iaMlLmxC9t2PpqXYQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>华中科技大学、上海创智学院、伦敦大学学院、腾讯优图实验室等机构联合提出RH-Partial2Global框架，旨在解决视觉上下文学习（VICL）中的两个核心局限：对“相似性优先”假设的盲目依赖以及随机</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642392&amp;idx=1&amp;sn=35b64a21ea3a05cb760f04ff59a42585&amp;chksm=97cf519a5b26b27dc6c6bf3ed987f7800e5b5cd12fda41c42e167655b7a64d16a19a520bf6c5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 21 Oct 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工等提出Puffin：像摄影师一样思考，统一相机理解与生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskW6YdFLHGiaFJjFuKJfYc81GsWdkojzA74bpQkrEdOnHRgy9tgkkm2F9w/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，AI在空间感知和内容生成方面的能力又有了新突破。来自新加坡南洋理工大学、商汤科技、密西根大学和马普所的研究者们联手，带来了一个名为 Puffin 的统一多模态模型。它巧妙地将两个看似独立的任务—</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642392&amp;idx=2&amp;sn=7c0da7c6472fd019ddd0c9b631dd74e3&amp;chksm=979a5c7c794f7613501bafec7af49d3d831e3661049aebea9ce76d58f3091e715c8c5fd82983&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 21 Oct 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[YOLO Vision深圳站来啦！与我们一起见证视觉 AI 的下一篇章！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTss1VYKA9kGm6RFZPibHjEPJo6tt1gKnhYwmrrYyEiaWyibSxarFTg9nNYp7asq8l2gAdKO80XibOL9mQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>继伦敦 YOLO Vision 2025 圆满落幕之后，Ultralytics 即将再次启程—— 这一次，我们把视觉 AI 的盛会带到了 中国 · 深圳！今年，YOLO Vision 首次登陆亚洲，我</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642270&amp;idx=1&amp;sn=ef622aaa735b9524a45bf39109db5233&amp;chksm=971ab906db576dcd65fc183f2767607430ba5af28d53c92ecfbd75532b2432daff781763df9a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 20 Oct 2025 18:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI25 | AIGC发展的下半场：清华、上海AI Lab等发布重磅综述，系统拆解扩散模型“效率之道”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTss1VYKA9kGm6RFZPibHjEPJ2uVa4mr21C17yZ6D1aGK7H6Ribwg9OmfG7ibYyYGiceYnibCFa4dLcuQAg/300?wxtype=jpeg&amp;wxfrom=0"/><p>近年来，以扩散模型（Diffusion Models）为核心的AIGC技术取得了突破性进展，其发展历程大致可分为两个阶段。2021年之前，扩散模型理论基础在基本奠定。这一时期，研究者们主要致力于构建扩</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642270&amp;idx=2&amp;sn=30b5ad4882e8add946ed0e89db26158a&amp;chksm=973f40c8dd0e5586845bf1a19fd43a0a7b97069d98b241a2619e341c940609fcef19281a3e60&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 20 Oct 2025 18:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2025 OccScene:联合扩散框架，同时推进感知与生成进步，3D场景生成新突破！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs1dzEicyB6thTvfmj2Bswt4pBe6QiaPUPPODdKZegy9GHc5AIToBxQprl8pkiaWM8nfiben5TTLRQ7Fg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: OccScene: Semantic Occupancy-based Cross-task Mutual Learning for 3D Scene Generation作者: Bohan</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642206&amp;idx=1&amp;sn=4ef20cac6b88236a7a66cbac5bd98c43&amp;chksm=97905d628da2c8422c1f7b349a7898daca3967e05cce3683275cf8be80d9333d8935e8b4dd5d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 19 Oct 2025 13:11:47 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | KAUST与MetaAI提出Vgent：图增强RAG，长视频理解性能超越SOTA 8.6%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtNaxWwYdt7L2LJCIicpZicMtGLT9NCIo0leK7FlJgQSpITbCYYiaPqiar9WLCzXNGIibyjZUoxFLIaB9g/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是CV君。今天想和大家聊一篇非常有趣的新工作，它来自阿卜杜拉国王科技大学（KAUST）和Meta AI的研究团队，并被 NeurIPS 2025 接收为Spotlight论文。这项研究针对的</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642206&amp;idx=2&amp;sn=ca99a163c41e8a06736c957a533d9676&amp;chksm=978ec19c793a954adf23d6546074ca0174f53ee1ab8be47fe31382fe8cfd3261e71099bf5d1b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 19 Oct 2025 13:11:47 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[马普所&amp;谷歌等提出AnyUp：无需再训练的通用特征上采样，推理即用，效果SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs1le1vXqvbN9Ors1hmQF4yrfLhQdorpCTODL9qt0FTlSolaiaRShyicNiapA1raBJNLwJ5GypBWHh6A/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天和大家聊一篇非常有意思的新工作，来自马克斯·普朗克计算机科学研究所、谷歌、苏黎世联邦理工学院和慕尼黑工业大学的研究者们联手打造的 AnyUp。顾名思义，“AnyUp”就是“任意上采样”的意思。它的</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642172&amp;idx=1&amp;sn=838bf40beb2b5ddb94115e44a32b3c5b&amp;chksm=97bacacc3d1a5e03fcef6ffccadb2f9d5cf01f383a5891c5bb6c991b8e60f10b7be7f51febb2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 18 Oct 2025 07:02:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[北大等提出MaskDCPT：通用图像修复预训练新范式，PSNR提升3.77dB]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv0vybIz1iacEa1VUMStWHvpibCdslswF61vl6qnePoAIC8hKwMESRAUqwApiaEOyKIDIXz7KaeHD7vw/300?wxtype=jpeg&amp;wxfrom=0"/><p>朋友们，今天想跟大家聊一篇图像修复领域的有意思的新工作。如今，我们对图片质量的要求越来越高，但拍摄过程中总免不了各种意外，比如模糊、噪点、光线不足等等。传统的图像修复模型往往是“专科医生”，一个模型对</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642172&amp;idx=2&amp;sn=bfb588b0728479d2e5bfdd90b6f06887&amp;chksm=974f4eb0a33d022f0ecdcc0a919ff3b72202e0a51fb2c7b104c3b41f19c6e42c6d96b536774d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 18 Oct 2025 07:02:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Identity-GRPO：阿里开源多人物定制化视频生成的后训练优化算法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtNaxWwYdt7L2LJCIicpZicMtpyWtKCr9qpFn6YjNpPVzhMYvuibvlTszzvEWKib6LdujBLg4eMCj3G1Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇分享论文 Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642099&amp;idx=1&amp;sn=612ac4d3ef32ac086a5959bee407d50a&amp;chksm=97653382c8a0e795854c9ff186fd3ad63197428a9e84e009e3eedb87fad1da4467d65512b0ec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 17 Oct 2025 16:17:44 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Real-world Video Super-Resolution | VSR的十字路口]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtNaxWwYdt7L2LJCIicpZicMtSvUkMOwE6HukYiciaWBOjQeRrR6DPx6AuDROtMd1tFNJG7rtXeURSGTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>本文为粉丝投稿，原文链接：https://zhuanlan.zhihu.com/p/1959430260706744130。本文距离上一篇文章Real-world Super-Resolution |</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642099&amp;idx=2&amp;sn=c64dad72dfe4e13e850bf3f8ae3fcfe0&amp;chksm=9702a2e0bbf133f949ef64fba2fb9b94d4001844e43f3418082cf4cfdb75bf28798329ba7684&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 17 Oct 2025 16:17:44 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 为Sora视频加上“隐形身份证”：清华大学等提出Safe-Sora，时空频率感知水印新框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuwUbsFGXicqLs3cmiaVghtCYibMdQgbWYfFXAuKs4Rpn9LELUSdqAfJcO7bMq0WrLiaQZu9xVHNLU7ibw/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着Sora等视频生成模型的爆发式增长，如何为AI生成内容进行版权溯源和认证，成了一个亟待解决的问题。在图片生成领域，隐形水印已经是一种常见的技术，但在视频生成中，相关的探索还比较少。最近，来自清华大</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641670&amp;idx=1&amp;sn=f918f03aa34204a808782889c6a19205&amp;chksm=97b608a41142c83b71d6e5461c0ba2a8e906fef473da0f9d6c969e7fa0b02ba3c31717db1a7c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 16 Oct 2025 12:46:52 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[从DNN到MLLM的异常之旅：视频异常检测（VAD）范式大迁徙]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskWviazjdl9xxYbC1YaicYaICXzPGjPZbTsNsYRLlE9wHwM7UyycUBq8Tsw/300?wxtype=jpeg&amp;wxfrom=0"/><p>如果你还在纠结视频异常检测任务如何预测/重建，如何设计MIL框架，那你可能错过了一次正在发生的范式迁徙：研究从视觉空间的边界学习，悄然转向语义空间的理解与推理。从深度神经网络（DNN）到多模态/大语言</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641670&amp;idx=2&amp;sn=6c3ef3b1adca48fbbfc758b298d4e681&amp;chksm=970709789dc7ca6214bc1d64c3d5f0b02f5f477f3ed540d5d8d75fe2eefc9be0665bf9527399&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 16 Oct 2025 12:46:52 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IDEA提出Rex-Omni：将目标检测变为“下一个点预测”，零样本性能超越DINO]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs1le1vXqvbN9Ors1hmQF4yzQOnwzPLiaOy3MxzuKic5o41khbwNauhtQPZW1ibolR49NWBPCMFm8pHw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天和大家分享一篇来自IDEA 研究院的最新研究成果。这篇名为《Detect Anything via Next Point Prediction》的论文，介绍了一个名为 Rex-Omni 的3B参数</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641473&amp;idx=1&amp;sn=102d135e1c3ffa85196a2ef1292f9ffa&amp;chksm=978b08ab1072479e4da148cf91b77a6167bb5c4fc29eef669e836df1e42358c86c3d08cfcad2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 15 Oct 2025 15:44:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2025 | 华中科大与大疆等提出LLF-LUT++：4K照片增强仅需13ms，PSNR提升2.64dB！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskWk9o4kSo5kHyIhTaUE4ibicv88S1QxHXJ4zDCktxMwjpECdLTs0Vzu0zw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自华中科技大学、大疆和香港理工大学的研究者们，为我们带来了一项非常酷的工作。他们提出了一种名为 LLF-LUT++ 的新型金字塔网络，完美解决了高分辨率照片增强中“效果”与“效率”难以兼得的痛</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641473&amp;idx=2&amp;sn=56b84fa5047c0ec8877b2324e0cb681e&amp;chksm=975f137570e649e51e867604903859f8aedde9decb079bcac436168d9b81f1960a02a167a348&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 15 Oct 2025 15:44:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 让AIGC视频变为可探索场景：Instant4D实现单目动态场景的分钟级重建]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskWbsSqxndocKuCLicKQ0GkwNr1nSJCXnYia2WJDOzLUZTURiaXiaFMQibMEmg/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，视频生成生成模型例如 Sora, Veo3 得到了社区的关注。 这些模型能够生成具有视觉吸引力，高度逼真，天马行空的视频。 在这个工作中，我们希望能够重建任意视频，并且实现新视角渲染，把AIGC</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641003&amp;idx=1&amp;sn=ed5802038986826f42f9c7ee4eeb4bf1&amp;chksm=973b2edb48cdc800160470488cd6cc33e707dcc79d1fbd3d387adb288ce16df4ddaa81186d83&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 14 Oct 2025 12:36:36 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | PPFN：渐进式提示融合，让红外图像增强在复杂场景下性能提升8.76%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskWOygvr2E2ySxx7ObJ0LlQNsyL2u2W7kdrzV5sAu0Jfdzgwe4ZkI5cqg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自大连理工大学和大连海事大学的研究者们，为我们带来了一项关于热红外图像增强的新研究。这项工作已被机器学习顶会 NeurIPS 2025 接收。不同于我们常见的RGB图像，热红外图像的“视界”里</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641003&amp;idx=2&amp;sn=d8de02f86d5d5ecda3a0ff96b6f5d84f&amp;chksm=97cb39382571ad921506488b5eee2857b746724246dc328ebf2c684cd8f3f343880c482d64d9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 14 Oct 2025 12:36:36 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[SAM 3揭开面纱：不止分割万物，更能理解概念，交互式分割迎来新篇章！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuwUbsFGXicqLs3cmiaVghtCYvnLqUIzicibLUv1aicEqFelbHFFhkuicAD3IhvfJ8n93QGvEp6LXMfNicYA/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，Segment Anything Model (SAM) 系列迎来了第三代——SAM 3。如果说第一代 SAM 教会了模型“分割万物”，那么 SAM 3 则让模型更进了一步，开始“理解万物”。它</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640604&amp;idx=1&amp;sn=9640e887d893f35182914fdf11b5f359&amp;chksm=974eba97d851764a7e06788547b8af17340e13720c386db9a4423f55ab8359cc670f97fa3af3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 13 Oct 2025 12:32:58 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ACM MM2025 Oral | MoSEAR:为多模态情感推理补齐“冲突场景”的短板]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuwUbsFGXicqLs3cmiaVghtCYL8kjA0AA2HbrliacnudnCH8UlsNic0l9WZxB9ia5jiah1Wd1zxAwvhkmVw/300?wxtype=jpeg&amp;wxfrom=0"/><p>在电影“流浪地球2”中，尽管刘培强用冷静的语气掩盖内心的不安，但是人工智能MOSS还是通过他微表情识破了其隐藏的秘密。类似的，当一个人嘴上说“没事”，但表情却写满了失望，如今的多模态大模型能读懂这其中</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640604&amp;idx=2&amp;sn=69aa2a9ffc8231fa07249a99ff38f5ed&amp;chksm=97dabc6305743aa37094d2b3dbe55e142ca8c98b98ae062b0d94254e9fc0fbf5ec8caeb8bd35&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 13 Oct 2025 12:32:58 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | Latent Harmony：潜空间和谐共生，实现UHD图像修复新SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv9so0J5tAxMejTVmE0zXQ3kjIia5p9ib2dRl6ibncuM0cLSFClbEiaWxv4nDa5F5gSRKpLPd1xz0af3g/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天CV君想和大家聊一篇非常有意思的新工作，它来自中国科学技术大学和上海人工智能实验室，并被 NeurIPS 2025 接收。这项研究聚焦于超高清（UHD）图像修复，提出了一个名为 Laten</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640508&amp;idx=1&amp;sn=3dacde954621ad9d05bf44aa978ef92c&amp;chksm=976dbd7335fb8e8b6c95376b4b2632cdb82c561f4ebe44c584b4eb0790b6d20ae0ee50e501e9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 12 Oct 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | NTN-Diff：一石二鸟，利用空文本与频率感知破解图像修复难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvfxUxmBxdrd76bga45zN0PjSCpuibg1iabVCZo1TJicc1ic1YIvF9Y9XKpSRuE2HC295u8r351orVZ8A/300?wxtype=jpeg&amp;wxfrom=0"/><p>在文本引导的图像修复（Text-Guided Image Inpainting）领域，一个老大难问题始终困扰着研究者们：如何在根据文本描述填充缺失区域的同时，完美保留图像中未被遮挡的部分？很多时候，模</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640508&amp;idx=2&amp;sn=82febf7dd7a41eb6d45e09a1d49cebbe&amp;chksm=97945ce9883274478c098776ac6b2988491686bd1f17b89895875577207cd72ff1c478334954&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 12 Oct 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 Oral | RAG-6Dpose：三大创新模块，利用 CAD 作为知识库进行检索增强 6D 姿态估计]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv9so0J5tAxMejTVmE0zXQ3GZCJody1RAaHCmUKAQW8uOhM9VObicRkc8IDq6qqrKGc4k8rWnwia7xQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>准确的 6D 姿态估计对机器人操作至关重要，可实现像抓取这样任务中精确的物体定位。单目 6D 姿态估计旨在从一张 RGB 图像中准确预测物体的三维位置和朝向，这对机器人抓取与交互等任务非常关键。然而，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640313&amp;idx=1&amp;sn=c519aa8196af4d49206fa798d035556f&amp;chksm=97918e585542ab3945026d0af22c75e079f756c272c52db215ed5c0b5eb468dd7ec88c415b45&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 11 Oct 2025 13:07:17 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 | 速度飙升24倍！巴黎萨克雷大学等提出HARP-NeXt：实时3D激光雷达分割新标杆]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvBzrQOREVzKT4UfnMW9TcYRAYo3o3gCyqGviag1YKN6F0ShAm68DHWQX7kw1WqompahKb4Vc2Ik2g/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是CV君。今天想和大家聊聊3D激光雷达（LiDAR）语义分割这个领域。对于自动驾驶和移动机器人来说，能实时、准确地理解周围环境至关重要，而LiDAR语义分割就是实现这一目标的关键技术。然而，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640313&amp;idx=2&amp;sn=319b1068b23b5356ac62edd9351f29b2&amp;chksm=97c210fccb6b11764e5b2224370236a42767527120138dfe92cadc28f8a6693337f3550f64c1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 11 Oct 2025 13:07:17 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[U-Bench：U-Net十年“大乱斗”终结者，100个变体、28个数据集的终极对决]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvfxUxmBxdrd76bga45zN0PEgmXkx2ib7t1TKtE0kepoBNl2fzaHIdhzUtzFGqIkb7sickMJcjvaPGw/640?wxtype=jpeg&amp;wxfrom=0"/><p>自2015年诞生以来，U-Net无疑是医学图像分割领域的“王者”，其优雅的U形结构和出色的性能，催生了数以千计的“变体”模型。然而，这个繁荣的生态也带来了一个问题：新模型层出不穷，但我们真的知道哪个更</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247639919&amp;idx=1&amp;sn=fd4f39a9c066c69a8a6df5434ddf7441&amp;chksm=9717827550435ad86b4303aec44e5135361b1b2e9f164772c85bcaf78363f2f655e9a553d720&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 10 Oct 2025 14:50:45 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[东京大学、牛津大学等联合发布VLA万字综述：机器人迈向通用智能的全栈指南]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvBzrQOREVzKT4UfnMW9TcYWx9R79CiaFc5atUIdzk4RJx7hmghuOn18Q3Nxd7icVU5kEFetrTtjkzA/300?wxtype=jpeg&amp;wxfrom=0"/><p>当大语言模型（LLM）和视觉语言模型（VLM）的能力不断溢出到机器人领域，一个激动人心的新方向——视觉-语言-动作（Vision-Language-Action, VLA）模型，正成为通往通用机器人之</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247639919&amp;idx=2&amp;sn=df224c69de268ba102ff25427433177e&amp;chksm=977b2bb4d5371e0f0e9e70170d62cb719395a10560b439db4226fcec62f8361cfe70c657d63f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 10 Oct 2025 14:50:45 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Diffusion²来袭：威斯康星大学&amp;华盛顿大学等提出双扩散模型，“回溯历史-预测未来”，破解自动驾驶“鬼探头”难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmWEI9nicjxnJ8zanDyuJwHdibhsxNxXaxnB7MokNcibhIE0V1mcRo6RKlzXYkc5eAJCo0r17icnicx1A/640?wxtype=jpeg&amp;wxfrom=0"/><p>朋友们，今天我们来聊一篇非常有意思的论文，来自威斯康星大学麦迪逊分校、华盛顿大学和同济大学的研究者们，题为《Diffusion²: Dual Diffusion Model with Uncertai</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247639615&amp;idx=1&amp;sn=f3ee6bc7c6c291c7f628ecd6f62f1b00&amp;chksm=977eb6ea4aafd9b2ca14efac20f310480c24d7831987b8a4ef3dddfc4b5c152e833ce8dd820b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 09 Oct 2025 14:46:46 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[北大等提出TrackVLA++：赋予机器人推理与记忆，跟踪成功率飙升12%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvBzrQOREVzKT4UfnMW9TcYw2OW72DlER4zICD1QCpoBMv3VL0ceHb7ynQVDCIgdqe3GEBetq1Lug/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，具身智能领域又迎来一个非常有意思的工作。我们知道，让机器人像人一样在复杂的环境里持续跟住一个移动目标，其实非常困难，尤其是在目标被遮挡或者周围有长得很像的“路人甲”干扰时，机器人一不留神可能就“</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247639615&amp;idx=2&amp;sn=91b87ca4a7ae75111f0888ae1c9957c2&amp;chksm=97c1c081e74cfef40141f3155f5012517489ffeab79b3b7eac52b8bade073f62d3d19b4b3d06&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 09 Oct 2025 14:46:46 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 | Waymo与谷歌DeepMind联手提出Drive&amp;Gen：用生成视频评估自动驾驶，虚拟测试更逼真]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmWEI9nicjxnJ8zanDyuJwHDIPswGkuicpYYOiaNnlq1u1KAtIuwVfFwLNhEibPVeYDmlGTiapflXPNYw/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，自动驾驶领域迎来了两位“新玩家”：端到端（End-to-End, E2E）驾驶模型和视频生成模型。E2E模型试图用一个“大模型”直接从传感器输入预测驾驶操作，大大简化了传统复杂的模块化系统；而视</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247639304&amp;idx=1&amp;sn=550ba355d4167df876744480c285f2f9&amp;chksm=979d4aad72ea38a473d2873e06feb22b8f5b584134c8d30f652a82ec9dd551c49172298fbb42&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 08 Oct 2025 12:09:23 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[告别深度传感器！慕尼黑工业大学提出DropD-SLAM：仅用单目RGB即可实现RGB-D级的SLAM精度]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmWEI9nicjxnJ8zanDyuJwHgcKHOMPuNl54cWFLfc9e0IicWkatEwVqUTBzKhAhTicuhWXDPaB2Ocsg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天，我们来聊一篇非常有意思的SLAM领域新工作，来自慕尼黑工业大学和3Dwe.ai的研究者们。他们提出了一个名为DropD-SLAM的系统，这个名字很直白，意思就是“扔掉深度（Dropping th</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247639304&amp;idx=2&amp;sn=78ab7f916ec4433d758a363c4431eef2&amp;chksm=9735940e075bb713cb1faca2acbf0025537ee51c98453b4d8b55dcc8991aefa0c8c9f89c0f63&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 08 Oct 2025 12:09:23 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2025 | 电子科大等提出EEMFlow：从事件相机学习高效Meshflow与光流，速度提升30倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsTeSh4RLoXLA8MMC9VPRNzqiad4NpfYTp2KkicXcSAo2WicLUlu7hydx3FLwXuEnpuDnSiceRfoOtTDA/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有意思的新工作，来自电子科技大学、香港科技大学和西南交通大学的研究者们，他们关注的是一个越来越火的领域——事件相机（Event Camera）。这篇被 TPAMI 202</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247639120&amp;idx=1&amp;sn=75539d0808a7384c22a05b01ebcaf2bb&amp;chksm=975c5f1a84e6b583f424ade2ecac593d58b533a0619dbe10b63aa48e866c40543874bae5dea5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 07 Oct 2025 15:31:53 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[MICCAI 2025 | 莱斯大学提出MetaSeg：参数减少90%，元学习隐式网络重塑医学图像分割]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsTeSh4RLoXLA8MMC9VPRNzPgLv2qG6MCFvnIjQZibGfbeQ8njkzDK1tIS3za0xSszaoQicIV8IyVrQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自莱斯大学的研究者们在医学图像分割领域投下了一颗“重磅炸弹”。他们提出了一种名为 MetaSeg 的新框架，巧妙地将元学习（Meta-learning）和隐式神经表示（Implicit Neu</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247639120&amp;idx=2&amp;sn=893a4e1f9ab2fe7893e98bd324f0c598&amp;chksm=974b3b801bfcc4e3a2b65beaa90cb79305f56da2cfe19cd283755c10ff218c32488d849444c6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 07 Oct 2025 15:31:53 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 慕尼黑工业大学提出SIM(3)等变网络：让3D形状补全告别“姿态偏见”，实现跨域泛化]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsWzQRzR8ECqLgEf8Jbe1jOFpwy6j4OcCYQuoZcxSQApyqr36UsRgV0kyeXnNh4CicgBfM1wbkfSJw/640?wxtype=jpeg&amp;wxfrom=0"/><p>聊一篇关于3D形状补全的顶会论文。我们先简单聊聊为什么需要3D形状补全。在现实世界中，我们通过激光雷达（LiDAR）、深度相机等设备获取的3D数据几乎总是残缺不全的。这可能是因为物体被遮挡、传感器视角</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638848&amp;idx=1&amp;sn=8ed516b291a968b84634cc377d303bea&amp;chksm=97f750c087556e5fd03b962e8d621e5bbe37164a4be9fee4011e89b3d8d47c291e5432630920&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 06 Oct 2025 12:47:36 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 高通提出GCL：无需额外数据，通用多模态检索迎来“一统江湖”新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsWzQRzR8ECqLgEf8Jbe1jOOxEqdcrtgb23vZQD3LFt9E8DfAEhFfbibqPxnycvcPGvZEl0WNgvCEg/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是CV君。今天想和大家聊一篇非常实用的论文，它来自高通AI研究院，并已被NeurIPS 2025接收。这篇工作聚焦于一个很现实的问题：我们如何让机器在面对图、文、甚至图文混合的内容时，都能“</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638848&amp;idx=2&amp;sn=79950ef2e3a274622482b51856321171&amp;chksm=97356856157b0d08df03b8c9539b9a78af241bdb96f9b26ce93d8fa29b4c63e4a2a0666746fd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 06 Oct 2025 12:47:36 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[宾大提出F³：事件相机迎来“预测性”表征新范式，光流、分割、深度全SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtB0ib5icMePhR4iblOq43vhSj3XKH1ia8W8esTYPSqCgaC0x7zIKNQryRkDgeV0ChGRnlQ9wE1RLYqcg/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天想和大家聊聊一种非常酷的传感器——事件相机（Event Camera），以及一篇来自宾夕法尼亚大学的最新研究，它为处理这类独特数据提出了一种极具启发性的新方法。事件相机和我们手机、相机里常见的传统</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638707&amp;idx=1&amp;sn=a5b88345beb4a0643f1c73764e87fffe&amp;chksm=97fde72e866df8f2d8cf3e221de24091ba5c9588e36b45781e2c41dea803f5a4cf6a6775623d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 05 Oct 2025 22:41:59 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[南理工提出FMC-DETR：巧用“频率解耦”，航拍小目标检测精度飙升8.2% AP50]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtQJO9jUpDtyicfwaZshibtGvwshxJZFwyGB4vVFkLrGuE0eYkLEZ5FwUWXm2csiaqpccicjygeP4zyrw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在广阔的航拍图像中，要准确地找出那些只占了几个像素点的微小目标，比如远处的车辆、行人，无疑是一项极具挑战性的任务。这就像是在一幅巨大的画卷中“找茬”，不仅考验眼力，更考验对整个画面的理解能力。这项技术</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638660&amp;idx=1&amp;sn=ee5206c4ed4f83f1b28d3ed5a06b865d&amp;chksm=97bc5fedb9a4211c5ef60413bf695e3d3b8b2245513a5f910189b8f43a1655219b27d9d40fc9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 04 Oct 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | RAD：基于大规模3DGS孪生数字世界的端到端强化学习训练策略]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtQJO9jUpDtyicfwaZshibtGvSjygyicBPlJfAzIXYVBtWPxibcaMRFvVOElibeY8lJLS4S0bIEjXpiaPjw/640?wxtype=jpeg&amp;wxfrom=0"/><p>一、论文基本信息类别详情论文标题RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638621&amp;idx=1&amp;sn=53094d3e8f3baf73ea1d713c8f0c996d&amp;chksm=972e1021d09a24c30fb6c2410ea42525f189dec2ba4ef7df80b39931592d5cb5863c467af469&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 03 Oct 2025 12:41:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[武大新作MASt3R-Fusion：融合IMU与GNSS，为新一代视觉SLAM注入“多感官”智慧]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu6a5InlLvyzricJC1dNwgL8ouFWiaGSZBaLpmDzz0yQPEInQ2lcoiatwWr2ayLsS6qdiayiazrsfiaTuwA/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天CV君想和大家聊一个机器人和自动驾驶领域的核心技术——SLAM。简单来说，SLAM（即时定位与地图构建）就是要解决一个根本问题：一个机器人在未知环境中，如何知道“我在哪？”以及“周围长啥样</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638597&amp;idx=1&amp;sn=9d2668a10edb9ec779f97d7a0c992731&amp;chksm=97221ac64e4c07357ef3cd0db7207f32615c6b1fe38e08d87655c3e52ab131ba2eb0fb6b3104&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 02 Oct 2025 13:21:40 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[中科大、清华、快手等发布OpenGPT-4o-Image：为多模态AI打造的“超级燃料”，图像编辑性能提升18%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaJj8L54WLMsIRqx2X6zDBgRTg3KmUVicQLS2RiaTlfkxvkeSLBrNDRkbuJIN9dsh8dadban7TNYYg/300?wxtype=jpeg&amp;wxfrom=0"/><p>如今，我们都对GPT-4o这类强大的多模态模型的威力惊叹不已，它们能看、能听、能说，还能生成和编辑图片。但一个灵魂拷问随之而来：要让这些模型变得更强，下一个突破口在哪里？答案可能出乎很多人的意料——</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638597&amp;idx=2&amp;sn=74ca0b987665c0e63e2a0e4f6fa8b92d&amp;chksm=9796344e1ac9055dfd25878676b863bf6bf3e94c4c6aa7902ccc5b2b7a4579e70afac547ddc1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 02 Oct 2025 13:21:40 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工联合商汤提出Visual Jigsaw：像玩拼图一样，显著提升多模态大模型的视觉理解力]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaJj8L54WLMsIRqx2X6zDB9LvkFxyVM30CmqicXwODg5vh6zO6dQFpypTsdpLV5XtQg5ibHm5aYIDg/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，多模态大语言模型（MLLM）的发展日新月异，但大家有没有发现，很多模型似乎更偏爱处理文字，而在“看图说话”的“看”这个环节，总感觉还差那么点意思。它们或许能识别出图像里的物体，但对于更精细的视觉</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638569&amp;idx=1&amp;sn=b54044764004f56e5d49e5a90ab89ab6&amp;chksm=97332836b25f56abffe9fe8b50b9f347d407f2641c815bd4ce86dc33909a0545582a255451f2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 01 Oct 2025 14:13:04 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[天津大学联合腾讯提出Wan-Alpha：一键生成高质量透明视频，发丝级抠图不再是梦]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaJj8L54WLMsIRqx2X6zDBibaCsGibCJlhicfEYXkRgNNIOwjkfz0h1e5XVBViakhoAoGEdBC4yFsqLA/300?wxtype=jpeg&amp;wxfrom=0"/><p>对于视频创作者和设计师来说，获取带透明背景的视频素材（也就是RGBA视频）一直是个头疼的问题。无论是繁琐的手动抠图，还是效果不尽人意的自动工具，都耗费了大量时间和精力。随着AI视频生成技术的飞速发展，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638569&amp;idx=2&amp;sn=5d6ea97371bbd1a523b6fe308c425277&amp;chksm=97018df3d0a7403c6884598684fdf5d5fb637827bbbdf1b029cc16c0ef39048eaed5fb6bcf9b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 01 Oct 2025 14:13:04 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 清华大学与华为等提出全新正则化方法，破解稀疏视图3DGS“协同适应”难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaJj8L54WLMsIRqx2X6zDBufibxKqprqSfLsGepluFSQvYxibUgicT18hUNR14POgFfIQVAmdKC6TdQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>近年，3D高斯溅射（3D Gaussian Splatting, 3DGS）技术因其出色的渲染质量和实时性能，在三维重建领域掀起了一股热潮。然而，这项技术在密集视图下表现优异，一旦训练数据变得稀疏（即</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638453&amp;idx=1&amp;sn=526fd0a7a4c6fa9a41f77a9bca5f7d9e&amp;chksm=97227d6c01511fd5dc75ab8e807a6455d919599271151572c010d3ca5ad9ef379077f7c7a999&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 30 Sep 2025 16:04:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[DeFacto：用强化学习治愈AI幻觉，让多模态模型“有据可查”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTttDVCqQ9gFaBE2oxXtrPErItXvjagPrYXTJ6zG0dGZ9dOdzicOfAe8NMMc7FQQI4NplPRXZV5VSWw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reaso</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247638453&amp;idx=2&amp;sn=1d3171800ea06cf16cc83a1970a4cfb5&amp;chksm=978f2c79bd5a9543eb8cee832aa6eeffd0e65a381fc5b30d59b9572ee0b780194eb7f6062c4c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 30 Sep 2025 16:04:31 +0800</pubDate>
    </item>
  </channel>
</rss>