<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM6aYkwkiboia6lA9D7ANy49WBe9icxn5NQqJjvn4Pyntzvfw/132</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[港科大（广州）等联合发布多模态空间推理综述：为大模型画下空间理解的未来蓝图]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuys6qRxFTuDlhM93JP4mXyX1MXic5kVVmAjQGgnsPyTraWDqarH6JkjtJPbS7ooSdxM4EmwyQnJ5g/640?wxtype=jpeg&amp;wxfrom=0"/><p>我们生活在一个三维的世界里，理解空间关系是与生俱来的本能。但是，对于近年来飞速发展的大语言模型（LLM）而言，这似乎仍是一个不小的挑战。它们或许能对答如流、妙笔生花，但在被问及“桌子上的苹果左边是什么</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644088&amp;idx=1&amp;sn=8e7153b9415ea9804bc43d3e3619e1d6&amp;chksm=9789510698228bc3a5044c20c8fe44a87321d4879f29cd34c6449eab9c416e6154c06ffbdb9b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 31 Oct 2025 16:37:59 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ReDiff：突破并行生成瓶颈，多模态扩散模型生成质量与效率双提升]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuJlaJicNdBOUbQ5tvSg3aL6S6x2SFFC9d2aSbrBZFvId5wT9Ar5A8FJic0xcWkU08WZKptXlicpBEgA/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model作者团队：香港大</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644088&amp;idx=2&amp;sn=90e7703081a8d60f29092118c01d53fe&amp;chksm=971207d72a285cacea38ef1758ca10f9fe0fd0042581845ea6440f219932a952a68b31bba46f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 31 Oct 2025 16:37:59 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[谢菲尔德大学提出Region-CAM：mIoU提升13.6%，破解弱监督学习中的目标定位难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuys6qRxFTuDlhM93JP4mXylVjDRzS7jDJv5UJOuMibfb1oNxtFjOz2MkJOPiawcUaDbhWVIoChicd2w/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天想跟大家聊一篇非常有意思的文章，来自谢菲尔德大学的研究者们提出了一种新的激活图生成方法——Region-CAM。对于做弱监督学习的朋友们来说，类激活图（Class Activation Mappi</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644088&amp;idx=3&amp;sn=eb2ac1777b71dd90a5c2f9b19c03b036&amp;chksm=973adc8e532533a571b4b5f1fa29b85d41f9f2ece55d59d07b118c6806f55ca09f9c25cc1155&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 31 Oct 2025 16:37:59 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[英伟达开源ChronoEdit，让P图学会物理常识，时序推理颠覆图像编辑！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuJlaJicNdBOUbQ5tvSg3aL6LoHWddBSueS2icgex3lia89iaiccB2WjVHynq3gKhaNaPt0doXklgU7Tbw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，AI图像编辑技术层出不穷，但大家可能也发现了，很多模型生成的图片虽然乍一看很惊艳，但仔细一琢磨，总觉得哪里“不对劲”。比如让汽车掉个头，结果车直接瞬移了；或者让机器人手臂捡东西，动作却完全违反了</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644088&amp;idx=4&amp;sn=dc1c9d183ae76b327b6b589c1b47f9cb&amp;chksm=975b8749873f344ce9c6dca4f16e2131d71b3a0233b2685ee407d24dca1ad40e261460fc92f7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 31 Oct 2025 16:37:59 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[天津大学与快手联手提出GRAG：仅需4行代码，实现图像编辑的“丝滑”微调]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsd9QUPq9NZeUdBlOaqwSq0ne5W4ichzfgjucwHtMM3EfYVDAK5sLmYx69MCdwydbLLbLqpsKZyQgQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，基于Transformer的扩散模型（DiT）在图像编辑领域可以说是风生水起，但大家在享受AI带来便利的同时，可能也遇到过一个头疼的问题：生成的图像要么“改过头”，要么“没改到位”，很难精准控制</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643833&amp;idx=1&amp;sn=18b3e800deb6c7c4d4d83eddf46232bb&amp;chksm=970517b59314845823f3e58bd38e088c64bfdb3c68023ebcabf7b47751003351836bcdeaeae0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 30 Oct 2025 08:33:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[美团开源LongCat-Video：136亿参数长视频生成，分钟级推理720p]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtbDia2DWVzKyVTUN950LOy255ObIwYao0XhSEnEtLHichj7Kmuy8j0b7ac38zZs5rALmhgkjxSMCbw/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，视频生成领域又迎来了一位重磅玩家。美团的LongCat团队发布了名为 LongCat-Video 的技术报告，这是一个拥有 136亿参数 的基础视频生成模型。它的亮相，不仅在多个视频生成任务上表</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643735&amp;idx=1&amp;sn=2ee32b04436df37000e50efc3867ce18&amp;chksm=97ca089f36e81d6a2931cd3bc3781f81b8b392c997bfe0a1387e1e8ba55e4faa6a5e6063273f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 29 Oct 2025 13:08:16 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[中科院SNELLA：视觉模型微调新范式，性能超越SOTA，内存占用降低近40%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsd9QUPq9NZeUdBlOaqwSq0ia4hwR9pFLjhZRBoMpzPib1D5nFx3j0iauZRGficPbcGVOO7dT1hhKMtog/300?wxtype=jpeg&amp;wxfrom=0"/><p>当我们在谈论微调巨大的预训练视觉模型时，计算资源和内存总是绕不开的话题。为了让这些“大块头”能更轻巧地适应下游任务，参数高效微调（PEFT）技术应运而生。而在众多PEFT方法中，稀疏微调（只调整模型中</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643735&amp;idx=2&amp;sn=c1663a9cf93d63dd0e3280dbf319e0da&amp;chksm=97864ff3d76e4ad116ed70fb785085af359b808ee7f73c5ec07895b315533bc2e944fa6c1399&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 29 Oct 2025 13:08:16 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[普林斯顿大学联手谷歌DeepMind，BOB让少样本细粒度分类精度飙升7.4%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsd9QUPq9NZeUdBlOaqwSq0DGdepPpEYEkXpwGscxpVZ4XjVFYrGNkog6KWwBR1yIn961B15tiaUMw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自普林斯顿大学和谷歌DeepMind的研究团队联手，为我们带来了一项非常有趣的研究。他们提出了一种名为BOB（Beyond Objects）的新方法，旨在解决一个困扰业界已久的难题：如何利用文</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643735&amp;idx=3&amp;sn=18e30101a29ba5739e09724c80bde50d&amp;chksm=972d210fb16f8c5fc012fa862de858c00df30974183b4442da8fb28cb791a225004b44847cb1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 29 Oct 2025 13:08:16 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 | 大连理工等提出STG-Avatar：25分钟训练，单目视频实时生成高保真数字人]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtbDia2DWVzKyVTUN950LOy20QNs23cpRQecKpRydtKvkwfw5XDbHOeYZibicV7r2iblQGibxZAmAWIMWg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，数字人领域因为3D高斯溅射（3D Gaussian Splatting）技术的出现，又热闹了起来。这项技术大大加快了渲染速度，让实时高保真渲染成为了可能。不过，挑战依然存在，尤其是在处理衣服、快</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643735&amp;idx=4&amp;sn=951085432771dadc17e211eb376abda5&amp;chksm=97fe3cb6b28547952fb8de501ecd053892ecc37c1117048762e6e556817b8c85b18d206ca93c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 29 Oct 2025 13:08:16 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[重建超越RAE，还能做编辑！北大&amp;通义提出UniLIP: 自蒸馏训练助力CLIP大一统]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtbDia2DWVzKyVTUN950LOy2F8mkcOQDtlEQiazLWSxVqzS4xhDibQnBI9PR8LOGPfxavric3mzejQGNw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing论文地址：https://</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643566&amp;idx=1&amp;sn=f0896e8e567fcb91fdc05e9f24c4ef30&amp;chksm=9753d97b70fe846f42793fafa096e7b522e8076def58a687cb610b15338e404f0f8e1a912b3d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 28 Oct 2025 16:24:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[世界模型是否需要显式的 3D？UT Austin 新作 PE-Field 给出答案]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtbDia2DWVzKyVTUN950LOy2pyO6xAgr3JDWfKpy2su1R8RJxUkh303Nol8icLk9zuTH6lHKX6EsLjA/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：Positional Encoding FieldProject Page: https://yunpeng1998.github.io/PE-Field-HomePage/Paper: h</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643566&amp;idx=2&amp;sn=dcfc7d5a387221419699191696579af8&amp;chksm=97b55ad510f510e729734928bc4fa7f3d4a294ed89292b3121d0a2f7242811e63681b94ab6c1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 28 Oct 2025 16:24:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Feed-Forward 3D综述：三维视觉进入“一步到位”时代]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtbDia2DWVzKyVTUN950LOy27Ys9IqhG1uKUlmd4icwPYyT4ZBaYvuGEgmibamsZq3QCu6e2DGmEriaibg/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey论文链接：https://arxiv.org/a</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643566&amp;idx=3&amp;sn=50dd20e69861e696a3c3d004d926ef5f&amp;chksm=975c7c560e922c8b3b8745e8532b227a056a268327dc2473e7c58fd5dcb83a3950e9ba4e7f4d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 28 Oct 2025 16:24:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CUPID：单图秒速重建3D，港大提出姿态引导新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtbDia2DWVzKyVTUN950LOy2iamxS5EiapYpFZ3kSkXOkwLrVElwbZRnjnQZ9bqFmK8uKvRHePITvtiaA/300?wxtype=jpeg&amp;wxfrom=0"/><p>和聊一篇3D视觉领域的有趣工作，它关于如何从单张图片快速、精准地重建出三维模型。这项技术来自香港大学等机构的研究者们，他们提出的新方法名叫 CUPID，全称是 “Pose-Grounded Gener</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643566&amp;idx=4&amp;sn=a33ca0a6ce7e368515db5435a7eaf972&amp;chksm=97311542c14d468013cb9530d20f131b8eaf33fa3ccbb4bbacf7813c3c8b60d33d21f5b5214f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 28 Oct 2025 16:24:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[“压缩不减智”！EPIC让多模态大模型以更少 Token 跑得更快、更稳｜NeurIPS 2025]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuBq4iayS8xcC1N2u4AibX9MBia5Pg0vsRvabguPF1Grd5LNyCQvJBCM90rJgibt4fTS8TgCPjKTLM6Ew/640?wxtype=jpeg&amp;wxfrom=0"/><p>📌 导语摘要当视觉Token爆炸成为多模态大模型的最大负担，EPIC提出“渐进一致蒸馏”这一全新思路——不改模型结构、不加参数，却让模型在压缩中越学越强。这项来自上海交通大学 EPIC 实验室与上海人</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643332&amp;idx=1&amp;sn=119b586f563275ba61a55d065f3e21d9&amp;chksm=977869ca6ec1bf95be5a14c4507ae91d5b34349b1851a7b5954c692caf1f215bb61465cbc70b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 27 Oct 2025 14:48:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[川大等提出LG-CD：一句话精准锁定遥感影像变化，F1分数高达91.83%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuBq4iayS8xcC1N2u4AibX9MBAU2QZTxmiaCrRPgTFbUoIVrria7bwku7ehYrzrE3ria1v3DwzmNgCaP6Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天要和大家聊一篇非常有意思的新工作，它来自四川大学和新疆大学的研究者们。他们提出了一种名为 LG-CD 的新模型，全称是 Language-Guided Change Detection。简单来说，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643332&amp;idx=2&amp;sn=140491d468fb082fe9ce74cd6669a367&amp;chksm=9788ba5772396dabeba18b85584d7b1752cb121e60ba7b693fa90a869e94e1eb814de96bb2e0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 27 Oct 2025 14:48:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[首个面向大模型的形式化数学竞赛正式启动：推动AI数学推理迈向可验证新高度]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuBq4iayS8xcC1N2u4AibX9MBr9iaVeRUZA8rcq1VJz8nT95WpYwIp2Mc6sup9Fuwz6W4HHng5l1uvrw/300?wxtype=jpeg&amp;wxfrom=0"/><p>2025年10月20日，由中国计算机学会主办，蚂蚁数字科技和中国计算机学会大数据专家委员会联合组织，上海人工智能实验室、复旦大学、香港中文大学賽馬會「AI4SE」創科實驗室、新加坡科技局及Litex社</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643332&amp;idx=3&amp;sn=21a8c064b544989c53dc1fef09aca13a&amp;chksm=977852735a777b1a5cb9091bc65af4953a31adef223d938d2ceeba50e9c8a442084d8da9f1da&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 27 Oct 2025 14:48:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV'25｜开源AI3D数据集Objaverse++：更少的数据，却有更好的生成效果]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuBq4iayS8xcC1N2u4AibX9MBK9fXSLoEhCWnMAx8cKrpY2vmsyNZlia3jyrgicsRqD3bPGicNOQkHyqMA/300?wxtype=jpeg&amp;wxfrom=0"/><p>近日，甜菜欣欣团队在国际计算机视觉顶级会议 ICCV（International Conference on Computer Vision） 上展示了最新的研究成果——一项关于 AI 3D 数据集构</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643332&amp;idx=4&amp;sn=448e1d4a076cdfe5c22e68f80304fed9&amp;chksm=97f5445cf34a3c9f8cdc9e627592e70d398a18fb967adab5d2c0d581b740d450561f937d66ce&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 27 Oct 2025 14:48:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[上海交大与上海AI lab联手推出RAPO++：跨阶段提示优化，让文生视频模型“更懂你心”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmgX4BiagIB2nM5OS3lv0RWnkFABozbtYich8FaMWkMEv6EY0y7r45VGicPl3hSLbL2sNPIa75g7yLA/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自上海交通大学和上海人工智能实验室的研究者们，带来了一项名为 RAPO++ 的新技术，它巧妙地解决了文本到视频（T2V）生成中的一个核心痛点：我们普通人给出的简单指令（Prompt），往往难以</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643303&amp;idx=1&amp;sn=b645f694ee573a51756d1ebd88cf7f53&amp;chksm=97198335f4b1ae058a800b4d5cbd9a3a89a8d9d8beff18037356a5387b86fa85f65f14c40ec9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 26 Oct 2025 14:19:54 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 港中文等提出COS3D：协同语言与分割，革新开放词汇3D分割]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmgX4BiagIB2nM5OS3lv0RWM8U20OfR7RuWsQ33Tr0f2sd8XdHl2FzWboMrUyfyDhADkcuwDACFCA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是CV君。今天想和大家聊一篇非常有趣的新工作，来自香港中文大学、Autodesk、岭南大学和莫纳什大学的研究者们，他们联手打造了一个名为 COS3D 的新框架，给开放词汇3D分割（Open-</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643303&amp;idx=2&amp;sn=06f61cf5b8c867815b784cb695254286&amp;chksm=9782059ae7b207fad0f8575f4edd8f4764274ce95db82c391fa6862aa1735723609508c9a965&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 26 Oct 2025 14:19:54 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[从“Spider”到SAM 3：概念提示分割小考]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTutZfTz8odOVKG5JeDIvbddaRnLmdGRyHTaABfvLOMAtwa0mBImBGMZhia6P1UuTl7u9YmhSibc6wZg/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，SAM 3 以概念提示分割再次引起计算机视觉研究社区的注意。图像分割技术，作为理解视觉世界的基石，正从为特定任务（如车辆分割、息肉分割、伪装物体检测）训练的专用模型，迈向能够“分割万物”的通用大</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643251&amp;idx=1&amp;sn=2db2650f8af7554a5397c1c2c1421cba&amp;chksm=9769a31677d81771689c0c7505c6e1b9f57b890d55006523d6bf8becb3de69cec5d91b6d6493&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 25 Oct 2025 13:45:37 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[从「会画画」到「会思考」：快手可灵团队提出 T2I-CoReBench，最强模型也难逃推理瓶颈]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmgX4BiagIB2nM5OS3lv0RW37xOZvUDnfnmr36YPfOTYQoyIGBxaXUZALgFz4nY3RrPlK1c0zrE8w/300?wxtype=jpeg&amp;wxfrom=0"/><p>当前文本生成图像（T2I）技术早已不是画出来就行。从 Stable Diffusion 到最新的 Nano Banana，模型能轻松生成指令一致的简单画面，但要生成繁忙厨房中的 30 余种物品或绳索断</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643251&amp;idx=2&amp;sn=7999b166c93dbb875c2c8fd977f01ec9&amp;chksm=97a34ad5a83f2770dfd429aca15765f92327de7a30c12c90ee6cbb32dff951379eab0dc29810&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 25 Oct 2025 13:45:37 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Meta新研究Free Transformer：仅增加3%成本，让大模型生成更多样、更高能的回答]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmgX4BiagIB2nM5OS3lv0RW7elnEBaOLB56050OBGECjGibWlP6qeeWbrWXKvaLnEBPYy7rHx6llsg/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天和大家聊一篇来自Meta AI的有趣工作，论文标题为《The Free Transformer》。这篇论文给我们带来了一种新的思路，旨在解决当前大型语言模型（LLM）生成内容时略显“死板”的问题。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643050&amp;idx=1&amp;sn=064efb35b0b125312aa408ebf9f43fe2&amp;chksm=975762f6d24850e77179e61fef69b3122a7ed2cf685715222f412f720e23ac367680ca05fafe&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 24 Oct 2025 12:14:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 牛津大学等提出Memo：Transformer强化学习记忆效率提升10倍，具身智能体泛化能力更强！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtagea8ibEM2HC7joYu7jw6icMRmiaXzUNsvVJqG2K3gmrnIDce3V5PMGmwwQRFwKAAFxZu36UA84Miaw/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天CV君要给大家介绍一篇来自牛津大学、佐治亚理工学院和丰田欧洲的最新研究，这篇论文题为“Memo: Training Memory-Efficient Embodied Agents with Re</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643050&amp;idx=2&amp;sn=03a78b61fe73a527a5ccd4e0c2924823&amp;chksm=9744d6d96c243b56df5ab161907c33c752ed72569588824fc5893e1d1375ca9e23ea3cd1ae2d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 24 Oct 2025 12:14:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[港科大联合港中文、字节跳动推出DreamOmni2：不止修图，更能领会意境，让AI绘画大师拿捏氛围感！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtmgX4BiagIB2nM5OS3lv0RWtS8JuKWpibP37dDhV8jzZ47J6o8Qqu0drL47fBPbrujey2PpN7OSDdQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>和大家聊一篇非常有趣的新工作，来自香港科技大学、香港中文大学和字节跳动的研究者们联手打造的 DreamOmni2。我们知道，现在的AI修图和绘画已经很强大了，但它们似乎总是“差点意思”。要么是只能听懂</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247643050&amp;idx=3&amp;sn=4806852d8291ac328670b1caf40163d2&amp;chksm=97fd9281a6d18d2c84a0dfdc0d41c589a69ec885e47fbabe2fe2dd000991872e0f7ecf4b0e24&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 24 Oct 2025 12:14:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[OmniNWM：突破三维驾驶仿真极限的“全知”世界模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv0ibbT1cQ4rJrO5YH6O9bXBqSKuMpuGaDHkog3p64tq0ZELtIlNRIIEUaQIJ6flibUJ98dTOQzEUMQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: OmniNWM: Omniscient Driving Navigation World Models作者: Bohan Li, Zhuang Ma, Dalong Du, Baorui</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642957&amp;idx=1&amp;sn=b2c7b83a0a1cb877a2971365eac44c40&amp;chksm=9730f9874ae7d825ce2be337967a7787e0d252128f9606a6f3aaacdeb1cc7d883eaa0b750ef8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 23 Oct 2025 17:03:04 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[英伟达将开源 CuSfM：CUDA加速SfM，精度与速度超越COLMAP，离线重建新标杆！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvlA3fubIfltG7EzoNaqQibian4BwJ3ch2HXiahkljRZ53B5xW9OPnYkUQWz9G1hpiciadEKsiaLc3xicMHw/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天CV君想和大家聊一篇来自英伟达（NVIDIA）的最新研究，他们带来了一个名为 CuSfM 的系统。这个名字听起来有点技术范儿，其实 CuSfM 是“CUDA-Accelerated Structu</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642957&amp;idx=2&amp;sn=09af9334e21f2cc1afb9c0cea5ab0e87&amp;chksm=97a7837c46b9128844b805602abef784c9d04ef47007021f86c40b923fc24b482221bf108375&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 23 Oct 2025 17:03:04 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 最佳论文公布！卡内基梅隆大学提出BRICKGPT：让AI化身乐高大师，文本生成实体积木，还能保证搭得稳！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv0ibbT1cQ4rJrO5YH6O9bXBk4oibRfjxicKRmHGJ4GxS2JkpNKjyK7KksxlX2hnibWQ8Rd6vKdwusBxA/640?wxtype=jpeg&amp;wxfrom=0"/><p>刚刚ICCV 2025 大会公布了最佳论文和最佳学生论文，最佳论文由卡内基梅隆大学研究团队摘得，从11000多篇投稿论文中被选中，该论文有哪些值得关注的点，我们一起来看看。最近，生成式AI在3D内容创</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642633&amp;idx=1&amp;sn=86b75fbd2b146171c2e5761e55d7ee23&amp;chksm=97d3d35b2da68393bb0112b056f01bb66e76e25590a8faa35486cd5869329a62151f22f074ef&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 22 Oct 2025 10:58:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025最佳学生论文 | FlowEdit：告别反演，一种更直接的图像编辑范式，结构保持力SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv0ibbT1cQ4rJrO5YH6O9bXBhoBu9IHk759k73kuat5FdNefXmfvICoICMyL92nJ5VAPwSIZPehc3w/300?wxtype=jpeg&amp;wxfrom=0"/><p>刚刚ICCV 2025 大会公布了最佳论文和最佳学生论文，最佳论文由卡内基梅隆大学研究团队摘得，最佳学生论文由以色列理工学院获得，从11000多篇投稿论文中被选中，这些论文有哪些值得关注的点，我们一起</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642633&amp;idx=2&amp;sn=3d3b648ed7447e718cb2361af971933d&amp;chksm=979d3093a506faaa8ed4fa50f74c8aa8a17f15394f326c493d28e73c4dfb12d21afbd4c66455&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 22 Oct 2025 10:58:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 上下文学习新方法：RH-Partial2Global，一个面向可靠与全面视觉的提示选择框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvlA3fubIfltG7EzoNaqQibiaPFuB1NDM8ibTaSE3re1GMa2hjFShypKsmpuDGo5iaMlLmxC9t2PpqXYQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>华中科技大学、上海创智学院、伦敦大学学院、腾讯优图实验室等机构联合提出RH-Partial2Global框架，旨在解决视觉上下文学习（VICL）中的两个核心局限：对“相似性优先”假设的盲目依赖以及随机</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642392&amp;idx=1&amp;sn=35b64a21ea3a05cb760f04ff59a42585&amp;chksm=97cf519a5b26b27dc6c6bf3ed987f7800e5b5cd12fda41c42e167655b7a64d16a19a520bf6c5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 21 Oct 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工等提出Puffin：像摄影师一样思考，统一相机理解与生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskW6YdFLHGiaFJjFuKJfYc81GsWdkojzA74bpQkrEdOnHRgy9tgkkm2F9w/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，AI在空间感知和内容生成方面的能力又有了新突破。来自新加坡南洋理工大学、商汤科技、密西根大学和马普所的研究者们联手，带来了一个名为 Puffin 的统一多模态模型。它巧妙地将两个看似独立的任务—</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642392&amp;idx=2&amp;sn=7c0da7c6472fd019ddd0c9b631dd74e3&amp;chksm=979a5c7c794f7613501bafec7af49d3d831e3661049aebea9ce76d58f3091e715c8c5fd82983&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 21 Oct 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[YOLO Vision深圳站来啦！与我们一起见证视觉 AI 的下一篇章！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTss1VYKA9kGm6RFZPibHjEPJo6tt1gKnhYwmrrYyEiaWyibSxarFTg9nNYp7asq8l2gAdKO80XibOL9mQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>继伦敦 YOLO Vision 2025 圆满落幕之后，Ultralytics 即将再次启程—— 这一次，我们把视觉 AI 的盛会带到了 中国 · 深圳！今年，YOLO Vision 首次登陆亚洲，我</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642270&amp;idx=1&amp;sn=ef622aaa735b9524a45bf39109db5233&amp;chksm=971ab906db576dcd65fc183f2767607430ba5af28d53c92ecfbd75532b2432daff781763df9a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 20 Oct 2025 18:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI25 | AIGC发展的下半场：清华、上海AI Lab等发布重磅综述，系统拆解扩散模型“效率之道”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTss1VYKA9kGm6RFZPibHjEPJ2uVa4mr21C17yZ6D1aGK7H6Ribwg9OmfG7ibYyYGiceYnibCFa4dLcuQAg/300?wxtype=jpeg&amp;wxfrom=0"/><p>近年来，以扩散模型（Diffusion Models）为核心的AIGC技术取得了突破性进展，其发展历程大致可分为两个阶段。2021年之前，扩散模型理论基础在基本奠定。这一时期，研究者们主要致力于构建扩</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642270&amp;idx=2&amp;sn=30b5ad4882e8add946ed0e89db26158a&amp;chksm=973f40c8dd0e5586845bf1a19fd43a0a7b97069d98b241a2619e341c940609fcef19281a3e60&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 20 Oct 2025 18:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2025 OccScene:联合扩散框架，同时推进感知与生成进步，3D场景生成新突破！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs1dzEicyB6thTvfmj2Bswt4pBe6QiaPUPPODdKZegy9GHc5AIToBxQprl8pkiaWM8nfiben5TTLRQ7Fg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: OccScene: Semantic Occupancy-based Cross-task Mutual Learning for 3D Scene Generation作者: Bohan</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642206&amp;idx=1&amp;sn=4ef20cac6b88236a7a66cbac5bd98c43&amp;chksm=97905d628da2c8422c1f7b349a7898daca3967e05cce3683275cf8be80d9333d8935e8b4dd5d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 19 Oct 2025 13:11:47 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | KAUST与MetaAI提出Vgent：图增强RAG，长视频理解性能超越SOTA 8.6%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtNaxWwYdt7L2LJCIicpZicMtGLT9NCIo0leK7FlJgQSpITbCYYiaPqiar9WLCzXNGIibyjZUoxFLIaB9g/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是CV君。今天想和大家聊一篇非常有趣的新工作，它来自阿卜杜拉国王科技大学（KAUST）和Meta AI的研究团队，并被 NeurIPS 2025 接收为Spotlight论文。这项研究针对的</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642206&amp;idx=2&amp;sn=ca99a163c41e8a06736c957a533d9676&amp;chksm=978ec19c793a954adf23d6546074ca0174f53ee1ab8be47fe31382fe8cfd3261e71099bf5d1b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 19 Oct 2025 13:11:47 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[马普所&amp;谷歌等提出AnyUp：无需再训练的通用特征上采样，推理即用，效果SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs1le1vXqvbN9Ors1hmQF4yrfLhQdorpCTODL9qt0FTlSolaiaRShyicNiapA1raBJNLwJ5GypBWHh6A/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天和大家聊一篇非常有意思的新工作，来自马克斯·普朗克计算机科学研究所、谷歌、苏黎世联邦理工学院和慕尼黑工业大学的研究者们联手打造的 AnyUp。顾名思义，“AnyUp”就是“任意上采样”的意思。它的</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642172&amp;idx=1&amp;sn=838bf40beb2b5ddb94115e44a32b3c5b&amp;chksm=97bacacc3d1a5e03fcef6ffccadb2f9d5cf01f383a5891c5bb6c991b8e60f10b7be7f51febb2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 18 Oct 2025 07:02:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[北大等提出MaskDCPT：通用图像修复预训练新范式，PSNR提升3.77dB]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv0vybIz1iacEa1VUMStWHvpibCdslswF61vl6qnePoAIC8hKwMESRAUqwApiaEOyKIDIXz7KaeHD7vw/300?wxtype=jpeg&amp;wxfrom=0"/><p>朋友们，今天想跟大家聊一篇图像修复领域的有意思的新工作。如今，我们对图片质量的要求越来越高，但拍摄过程中总免不了各种意外，比如模糊、噪点、光线不足等等。传统的图像修复模型往往是“专科医生”，一个模型对</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642172&amp;idx=2&amp;sn=bfb588b0728479d2e5bfdd90b6f06887&amp;chksm=974f4eb0a33d022f0ecdcc0a919ff3b72202e0a51fb2c7b104c3b41f19c6e42c6d96b536774d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 18 Oct 2025 07:02:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Identity-GRPO：阿里开源多人物定制化视频生成的后训练优化算法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtNaxWwYdt7L2LJCIicpZicMtpyWtKCr9qpFn6YjNpPVzhMYvuibvlTszzvEWKib6LdujBLg4eMCj3G1Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇分享论文 Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642099&amp;idx=1&amp;sn=612ac4d3ef32ac086a5959bee407d50a&amp;chksm=97653382c8a0e795854c9ff186fd3ad63197428a9e84e009e3eedb87fad1da4467d65512b0ec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 17 Oct 2025 16:17:44 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Real-world Video Super-Resolution | VSR的十字路口]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtNaxWwYdt7L2LJCIicpZicMtSvUkMOwE6HukYiciaWBOjQeRrR6DPx6AuDROtMd1tFNJG7rtXeURSGTQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>本文为粉丝投稿，原文链接：https://zhuanlan.zhihu.com/p/1959430260706744130。本文距离上一篇文章Real-world Super-Resolution |</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247642099&amp;idx=2&amp;sn=c64dad72dfe4e13e850bf3f8ae3fcfe0&amp;chksm=9702a2e0bbf133f949ef64fba2fb9b94d4001844e43f3418082cf4cfdb75bf28798329ba7684&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 17 Oct 2025 16:17:44 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 为Sora视频加上“隐形身份证”：清华大学等提出Safe-Sora，时空频率感知水印新框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuwUbsFGXicqLs3cmiaVghtCYibMdQgbWYfFXAuKs4Rpn9LELUSdqAfJcO7bMq0WrLiaQZu9xVHNLU7ibw/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着Sora等视频生成模型的爆发式增长，如何为AI生成内容进行版权溯源和认证，成了一个亟待解决的问题。在图片生成领域，隐形水印已经是一种常见的技术，但在视频生成中，相关的探索还比较少。最近，来自清华大</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641670&amp;idx=1&amp;sn=f918f03aa34204a808782889c6a19205&amp;chksm=97b608a41142c83b71d6e5461c0ba2a8e906fef473da0f9d6c969e7fa0b02ba3c31717db1a7c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 16 Oct 2025 12:46:52 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[从DNN到MLLM的异常之旅：视频异常检测（VAD）范式大迁徙]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskWviazjdl9xxYbC1YaicYaICXzPGjPZbTsNsYRLlE9wHwM7UyycUBq8Tsw/300?wxtype=jpeg&amp;wxfrom=0"/><p>如果你还在纠结视频异常检测任务如何预测/重建，如何设计MIL框架，那你可能错过了一次正在发生的范式迁徙：研究从视觉空间的边界学习，悄然转向语义空间的理解与推理。从深度神经网络（DNN）到多模态/大语言</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641670&amp;idx=2&amp;sn=6c3ef3b1adca48fbbfc758b298d4e681&amp;chksm=970709789dc7ca6214bc1d64c3d5f0b02f5f477f3ed540d5d8d75fe2eefc9be0665bf9527399&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 16 Oct 2025 12:46:52 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IDEA提出Rex-Omni：将目标检测变为“下一个点预测”，零样本性能超越DINO]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs1le1vXqvbN9Ors1hmQF4yzQOnwzPLiaOy3MxzuKic5o41khbwNauhtQPZW1ibolR49NWBPCMFm8pHw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天和大家分享一篇来自IDEA 研究院的最新研究成果。这篇名为《Detect Anything via Next Point Prediction》的论文，介绍了一个名为 Rex-Omni 的3B参数</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641473&amp;idx=1&amp;sn=102d135e1c3ffa85196a2ef1292f9ffa&amp;chksm=978b08ab1072479e4da148cf91b77a6167bb5c4fc29eef669e836df1e42358c86c3d08cfcad2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 15 Oct 2025 15:44:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2025 | 华中科大与大疆等提出LLF-LUT++：4K照片增强仅需13ms，PSNR提升2.64dB！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskWk9o4kSo5kHyIhTaUE4ibicv88S1QxHXJ4zDCktxMwjpECdLTs0Vzu0zw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自华中科技大学、大疆和香港理工大学的研究者们，为我们带来了一项非常酷的工作。他们提出了一种名为 LLF-LUT++ 的新型金字塔网络，完美解决了高分辨率照片增强中“效果”与“效率”难以兼得的痛</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641473&amp;idx=2&amp;sn=56b84fa5047c0ec8877b2324e0cb681e&amp;chksm=975f137570e649e51e867604903859f8aedde9decb079bcac436168d9b81f1960a02a167a348&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 15 Oct 2025 15:44:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 让AIGC视频变为可探索场景：Instant4D实现单目动态场景的分钟级重建]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskWbsSqxndocKuCLicKQ0GkwNr1nSJCXnYia2WJDOzLUZTURiaXiaFMQibMEmg/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，视频生成生成模型例如 Sora, Veo3 得到了社区的关注。 这些模型能够生成具有视觉吸引力，高度逼真，天马行空的视频。 在这个工作中，我们希望能够重建任意视频，并且实现新视角渲染，把AIGC</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641003&amp;idx=1&amp;sn=ed5802038986826f42f9c7ee4eeb4bf1&amp;chksm=973b2edb48cdc800160470488cd6cc33e707dcc79d1fbd3d387adb288ce16df4ddaa81186d83&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 14 Oct 2025 12:36:36 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | PPFN：渐进式提示融合，让红外图像增强在复杂场景下性能提升8.76%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSN8GjDGQ0icCYibOAaVHskWOygvr2E2ySxx7ObJ0LlQNsyL2u2W7kdrzV5sAu0Jfdzgwe4ZkI5cqg/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自大连理工大学和大连海事大学的研究者们，为我们带来了一项关于热红外图像增强的新研究。这项工作已被机器学习顶会 NeurIPS 2025 接收。不同于我们常见的RGB图像，热红外图像的“视界”里</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247641003&amp;idx=2&amp;sn=d8de02f86d5d5ecda3a0ff96b6f5d84f&amp;chksm=97cb39382571ad921506488b5eee2857b746724246dc328ebf2c684cd8f3f343880c482d64d9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 14 Oct 2025 12:36:36 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[SAM 3揭开面纱：不止分割万物，更能理解概念，交互式分割迎来新篇章！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuwUbsFGXicqLs3cmiaVghtCYvnLqUIzicibLUv1aicEqFelbHFFhkuicAD3IhvfJ8n93QGvEp6LXMfNicYA/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，Segment Anything Model (SAM) 系列迎来了第三代——SAM 3。如果说第一代 SAM 教会了模型“分割万物”，那么 SAM 3 则让模型更进了一步，开始“理解万物”。它</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640604&amp;idx=1&amp;sn=9640e887d893f35182914fdf11b5f359&amp;chksm=974eba97d851764a7e06788547b8af17340e13720c386db9a4423f55ab8359cc670f97fa3af3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 13 Oct 2025 12:32:58 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ACM MM2025 Oral | MoSEAR:为多模态情感推理补齐“冲突场景”的短板]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuwUbsFGXicqLs3cmiaVghtCYL8kjA0AA2HbrliacnudnCH8UlsNic0l9WZxB9ia5jiah1Wd1zxAwvhkmVw/300?wxtype=jpeg&amp;wxfrom=0"/><p>在电影“流浪地球2”中，尽管刘培强用冷静的语气掩盖内心的不安，但是人工智能MOSS还是通过他微表情识破了其隐藏的秘密。类似的，当一个人嘴上说“没事”，但表情却写满了失望，如今的多模态大模型能读懂这其中</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640604&amp;idx=2&amp;sn=69aa2a9ffc8231fa07249a99ff38f5ed&amp;chksm=97dabc6305743aa37094d2b3dbe55e142ca8c98b98ae062b0d94254e9fc0fbf5ec8caeb8bd35&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 13 Oct 2025 12:32:58 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | Latent Harmony：潜空间和谐共生，实现UHD图像修复新SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv9so0J5tAxMejTVmE0zXQ3kjIia5p9ib2dRl6ibncuM0cLSFClbEiaWxv4nDa5F5gSRKpLPd1xz0af3g/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天CV君想和大家聊一篇非常有意思的新工作，它来自中国科学技术大学和上海人工智能实验室，并被 NeurIPS 2025 接收。这项研究聚焦于超高清（UHD）图像修复，提出了一个名为 Laten</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640508&amp;idx=1&amp;sn=3dacde954621ad9d05bf44aa978ef92c&amp;chksm=976dbd7335fb8e8b6c95376b4b2632cdb82c561f4ebe44c584b4eb0790b6d20ae0ee50e501e9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 12 Oct 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | NTN-Diff：一石二鸟，利用空文本与频率感知破解图像修复难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvfxUxmBxdrd76bga45zN0PjSCpuibg1iabVCZo1TJicc1ic1YIvF9Y9XKpSRuE2HC295u8r351orVZ8A/300?wxtype=jpeg&amp;wxfrom=0"/><p>在文本引导的图像修复（Text-Guided Image Inpainting）领域，一个老大难问题始终困扰着研究者们：如何在根据文本描述填充缺失区域的同时，完美保留图像中未被遮挡的部分？很多时候，模</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640508&amp;idx=2&amp;sn=82febf7dd7a41eb6d45e09a1d49cebbe&amp;chksm=97945ce9883274478c098776ac6b2988491686bd1f17b89895875577207cd72ff1c478334954&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 12 Oct 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 Oral | RAG-6Dpose：三大创新模块，利用 CAD 作为知识库进行检索增强 6D 姿态估计]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv9so0J5tAxMejTVmE0zXQ3GZCJody1RAaHCmUKAQW8uOhM9VObicRkc8IDq6qqrKGc4k8rWnwia7xQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>准确的 6D 姿态估计对机器人操作至关重要，可实现像抓取这样任务中精确的物体定位。单目 6D 姿态估计旨在从一张 RGB 图像中准确预测物体的三维位置和朝向，这对机器人抓取与交互等任务非常关键。然而，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640313&amp;idx=1&amp;sn=c519aa8196af4d49206fa798d035556f&amp;chksm=97918e585542ab3945026d0af22c75e079f756c272c52db215ed5c0b5eb468dd7ec88c415b45&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 11 Oct 2025 13:07:17 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 | 速度飙升24倍！巴黎萨克雷大学等提出HARP-NeXt：实时3D激光雷达分割新标杆]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvBzrQOREVzKT4UfnMW9TcYRAYo3o3gCyqGviag1YKN6F0ShAm68DHWQX7kw1WqompahKb4Vc2Ik2g/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是CV君。今天想和大家聊聊3D激光雷达（LiDAR）语义分割这个领域。对于自动驾驶和移动机器人来说，能实时、准确地理解周围环境至关重要，而LiDAR语义分割就是实现这一目标的关键技术。然而，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247640313&amp;idx=2&amp;sn=319b1068b23b5356ac62edd9351f29b2&amp;chksm=97c210fccb6b11764e5b2224370236a42767527120138dfe92cadc28f8a6693337f3550f64c1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 11 Oct 2025 13:07:17 +0800</pubDate>
    </item>
  </channel>
</rss>