<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_e07180c244d1.jpg</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[无人机多目标跟踪AMOT：让无人机在复杂动态中“看”得更稳、跟得更准]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcL93fVp04RxdanDFz7ficWzoy7A2VoXuFwf6ly9AWibZlTibsTAKUvLgicQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇由中国科学院、阿德莱德大学、麦考瑞大学、加利福尼亚大学默塞德分校等机构的研究者们共同完成的最新成果，论文标题为《Tracking the Unstable: Appearance-Guid</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631902&amp;idx=1&amp;sn=b076b3a41062db259806261219de01f8&amp;chksm=97873689dd0bbc19c4657052d52752a708b62fb2234131e163c4c26a915079bbef1eec5a8bed&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 06 Aug 2025 23:33:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 马里兰大学和Meta提出Trokens：语义感知的关系轨迹令牌，革新少样本动作识别]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcs3roXBtQJrknRTIPs2zERzMXibPmB2eteicgm5M3iajbNvH8KetzB9MaQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇来自马里兰大学和Meta的研究论文《Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Rec</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631859&amp;idx=1&amp;sn=baaa5cc8b4cf132c3cc93bd34e762594&amp;chksm=973eff8bb669ba63ff5137d1256401fb3d89b7c3c285d45c89e0afa01298212141a595d7a9b3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 06 Aug 2025 15:19:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南大&amp;复旦&amp;南洋理工等提出LongVie：突破一分钟界限，迈向可控的超长视频生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcWuiaOctIC09yvQQLNk7AvOH7HehPkdfCLIqNFDqtdGeeQ4vyJOqlm5A/640?wxtype=jpeg&amp;wxfrom=0"/><p>从Sora的惊艳亮相到Kling的开源，视频生成技术在2024年迎来了爆发。然而，无论是学术界还是工业界，生成几十秒的短片已是极限，想要生成 一分钟以上、且内容、动态、风格都高度可控的 超长视频，仍然</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631843&amp;idx=1&amp;sn=de32e1e11daccdc142ec960177a944eb&amp;chksm=972fd4dd122d585f64ab9b81a3f504fba46e2582c1a4843d70c9419fbe6cbc689ea092c2bfe6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 06 Aug 2025 13:44:14 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 浙大等提出H3R：融合显式几何与隐式注意力，通用3D重建性能SOTA，收敛速度提升一倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcia8hR034uh9oL506a4xmYWzvwSNAV5ia1wntO0ybJa6cNLicxl0u5UPTA/300?wxtype=jpeg&amp;wxfrom=0"/><p>从几张任意视角的照片，就能“凭空”生成一个完整、逼真的3D模型——这是“通用三维重建”（Generalizable 3D Reconstruction）的终极目标。近年来，随着3D高斯泼溅（3D Ga</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631843&amp;idx=2&amp;sn=677cfff13e8113580d40b3dd3920990e&amp;chksm=97f502e1fd9e6a55f0fb4845ba5269bc7bd0d4e7f19725ecb69373a456a30c34d8d0016f5764&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 06 Aug 2025 13:44:14 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 Highlight | 彻底告别相机姿态！帝国理工提出SPFSplat：稀疏视图自监督3D高斯溅射新SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsrciamnRdQm2dib8DwzbWvAaotNos4tS084QOiafHs9ib2y23kBfsd9IhIGCMvTiaGfHZjDXVAXSJg9Kg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇来自帝国理工学院的最新研究成果，论文标题为《No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Spa</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631788&amp;idx=1&amp;sn=9799a89d9c6c539ad678a0f7c825ce1e&amp;chksm=975c209ab252e4f0aadb18eaa8ef9108c0cfdb778e809188211e9280d67b3cfb540601f1baac&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 05 Aug 2025 23:33:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南开提出GlimpsePrune动态剪枝，砍掉92%视觉Token性能反超10%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsrciamnRdQm2dib8DwzbWvAaHBVHmY60m48njdqMItjHRPIHW0wic6icYIplvQxqDNMDM1yL73noww1A/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇由南开大学、Shanghai Innovation Institute、天津大学及vivo AI团队联合发表的最新研究，论文标题为《A Glimpse to Compress: Dynam</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631764&amp;idx=1&amp;sn=db8da1df44df0b8d80dfbaeda1974038&amp;chksm=97c51fa32b99bdd26f2fc879d49d5a9ae31e4bba0ee190f330a3a4f8512c4b44d460f978f29e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 05 Aug 2025 14:54:41 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里发布Qwen-Image：不止于图，“文图”并茂的AIGC新篇章！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsrciamnRdQm2dib8DwzbWvAaVicfspeCJ9cKu7mViakVoDZuNBMW9wPicH1vwLicTXPS3tiaaWwEX90t12Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>近日，Qwen Team团队发布了其Qwen系列在图像生成领域的最新力作——Qwen-Image。这份技术报告详细介绍了一款在复杂文本渲染和精准图像编辑方面取得显著突破的基础模型。Qwen-Image</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631764&amp;idx=2&amp;sn=edebb70fc95668a53472f521cbd13642&amp;chksm=97126a4e295a023823267474d047cefecc5d7026f9b290d6ab6e56bc802a6b182e9956efccf8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 05 Aug 2025 14:54:41 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 高丽大学等提出“表示偏移”，统一Token压缩与FlashAttention]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvENeTlr1zWSCU042QpEsN59yDe03QibrmWBc06SefJEFibibYIOZpl2oqNJ4qj8m7icf4nfP7v15l5HQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>Transformer模型已成为AI领域的绝对主力，但其核心的自注意力（self-attention）机制存在一个“弱点”：计算和内存开销会随着输入序列（token数量）的增长而呈二次方爆炸。为了给T</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631706&amp;idx=1&amp;sn=ffec15e3ef8b68e74019c52855d94125&amp;chksm=978a763582de17b787cf791becafd408f28def97602478c13491e2f2201c0a04b2bd684f4518&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 04 Aug 2025 23:38:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[西安交大、OPPO等提出 D3，用二阶物理特征“揪出”Sora类AI视频，无需训练！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvENeTlr1zWSCU042QpEsN5Rf6YcVTO65zAic5Gb4wOGO0F2F8nWxEGBkicbgohibktzdeqbCiaL9z5Wg/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着Sora等视频生成技术的飞速发展，人类正进入一个真假难辨的时代。高保真AI视频的泛滥引发了公众对虚假信息传播的深切担忧。然而，现有的检测方法往往难以捕捉到AI视频在时间维度上的微小破绽。为了应对这</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631692&amp;idx=1&amp;sn=904a4fc2b06d41a713ec08051c0d2c09&amp;chksm=97d76a07f567c23dc35f45c450fcd8368b598c325ca212c113ff351932e9a72c3007d5b5c90b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 04 Aug 2025 12:06:12 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[万物皆可指代分割：多模态指代分割技术大综述]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvENeTlr1zWSCU042QpEsN5JjR2j1xYKMkJhArAa9s8Kozg8PCdiaONTlRRDnmibxykO0XyRE2LIwWA/300?wxtype=jpeg&amp;wxfrom=0"/><p>本文将介绍一篇来自复旦大学、上海财经大学和字节跳动的最新综述论文《Multimodal Referring Segmentation: A Survey》。该研究全面系统地梳理了多模态指代分割（Mul</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631692&amp;idx=2&amp;sn=dbfdf069078fcd992357b25580e1e61f&amp;chksm=971ecc99a6095e6606dbab92cc2376441c9d1eba0760257126497ba99d2b93fe64b23488b4d7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 04 Aug 2025 12:06:12 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[2025下半年了，来一篇"通用目标跟踪"最新综述]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs7EdoFVULZQZK40W8X7Lic2Ij7M7ib9QUFT2tfJOz018Sc4YhKkjic6zRGq3FbIpPPOXqEL03Z97wJg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文分享一篇关于通用目标跟踪（Generic Object Tracking, GOT）的最新综述性论文。该论文系统性地梳理了从经典到前沿的跟踪算法，并创新性地提出了一个统一的分类法，将主流跟踪器划分</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631642&amp;idx=1&amp;sn=c390a273e171b422dcaad168d8403bd3&amp;chksm=9724644f88dafbf1c13814a32f61f38cec6754c9122eae7b0f50f31f83e2dd8d386c02c15278&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 03 Aug 2025 23:38:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | CMU 提出MonoFusion：四个相机就够了！稀疏视角下的高品质4D动态场景重建]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs7EdoFVULZQZK40W8X7Lic2UD0PDUSVBo6YNjulur90ibicDP4aWx188PicfTjtJtXuibe7nicS9UrqGbw/640?wxtype=jpeg&amp;wxfrom=0"/><p>如何用最少的设备捕捉和重建一个动态的三维世界？这一直是计算机视觉领域追求的目标。传统上，要实现高质量的动态场景重建（即4D重建），往往需要像电影特效工作室那样，部署由数百个精心校准的相机组成的“相机阵</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631622&amp;idx=1&amp;sn=2f8768c7d2f75092563a0397e1206c66&amp;chksm=975a82f4bc6757149cc47ec855c79afd317c9b1cd4c893dea811ccd8213f3949555a73c0d7a2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 03 Aug 2025 15:07:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[Moiré Zero：迈向“零”摩尔纹，一种高效、高性能的图像去摩尔纹新架构]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs7EdoFVULZQZK40W8X7Lic2ibhCrRkGA1iaLDNSJulK6XeZflrFmJtqytNa86c4BS6e1xl98jOj10xg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文解读一篇来自成均馆大学、延世大学及三星显示的研究者们带来的最新成果——《Moiré Zero: An Efficient and High-Performance Neural Architect</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631601&amp;idx=1&amp;sn=7252f077b76f04402f8d5cd7e97a23a0&amp;chksm=97db8581093a76b308e4abffe2369208f830e0bee8c188976fbc8da0b8f954d916e3247b290e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 03 Aug 2025 14:33:45 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025｜南大南开新工作DiscretizedSDF：高效鲁棒的可重光照高斯模型，代码开源！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvY1q9XV7weS9RH6iblEibpVbALeCKRicg9phbeQF1bdG9wGE5SydVBusExR5dfn22Vib18fWYMC7SQcg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文分享一篇南开大学和南京大学在ICCV 2025上发表的最新研究成果《Gaussian Splatting with Discretized SDF for Relightable Assets》。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631585&amp;idx=1&amp;sn=16594ea770c590d9ab155068827cb14f&amp;chksm=97814528f4a47b1f83196b23516b9f31aa6e7b1961ac1f3c92e278003681ed628034b02d1405&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 02 Aug 2025 14:04:21 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | CMU提出SmartCLIP：模块化对齐，解锁更智能的视觉-语言表示]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs39CMOs3YPpB8LrxLPeficYyYibqCh9CKUfdUuR2CGziab9ZSVvlZiaSw7GrjG1ZRkhAdO1GF86jK0pA/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇来自卡内基梅隆大学、穆罕默德·本·扎耶德人工智能大学和悉尼大学的学者们共同发表于 CVPR 2025 的最新研究成果。这篇名为《SmartCLIP: Modular Vision-lang</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631570&amp;idx=1&amp;sn=ff4a6d423b66f2f11c9faa7206fc236b&amp;chksm=972a3b8f586283b3ba5ce4a6106ee582ce0ae61fefcbd417c9607c2da63ad66ce3ffee38daed&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 01 Aug 2025 04:31:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[6大学术会议汇总，提交EI数据库！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsQ2UPicNOEg1AvU3fpia1B4zicB4tia2AXTBDic4SQ5UpgiaRIPCXBy35V6W26lOn8crLpmv9V9JG5rSnw/300?wxtype=jpeg&amp;wxfrom=0"/><p>组委会尊敬的学术界同仁：AC学术平台谨此发布2025年下半年重要学术会议信息公告。经审核，在AC学术平台发布的所有会议均将邀请全球知名学者莅临现场，分享最新学术研究成果，且会议论文均将提交EI数据库。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631570&amp;idx=2&amp;sn=1f89aaa1f09eedf57e54f1d31de75fb1&amp;chksm=97edb0a1c81adf0b1c941e88e0c8b564944a205a3d68023791c3fa4687c3e1201b9390ec8370&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 01 Aug 2025 04:31:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[别再“一视同仁”！川大提出SMFNet，选择性融合运动与深度，解锁视频显著性检测新高度]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsQ2UPicNOEg1AvU3fpia1B4zCwwBfvMWJe76WkS0eURIbeUCZdAuKic3ZIR9fhKpueGKk2Jh37OMIeQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>在视频中准确地找出最吸引人眼球的物体（即显著目标），是计算机视觉的一项核心任务。近年来，随着带有深度信息的RGB-D摄像头的普及，利用额外的深度（D）和运动（光流）信息来辅助传统的RGB图像，已成为提</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631555&amp;idx=1&amp;sn=609dcfbc1ad45fd8c9375111e6a38a5c&amp;chksm=97c62d47df0d7867bb374dac4b71f27e2ebd001fadac64d215032ec4407939f12f50cc906984&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 31 Jul 2025 10:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[SAM/SAM2赋能视频万物分割与追踪：一篇“过去、现在、未来”的全面综述]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsQ2UPicNOEg1AvU3fpia1B4zWm7mzmXAQMCS3saNnOWg6jUhvPFzCYW34EicicymbsWUTZj2AW01eJHw/300?wxtype=jpeg&amp;wxfrom=0"/><p>视频目标分割与跟踪（Video Object Segmentation and Tracking, VOST）是计算机视觉领域一项复杂且至关重要的挑战，它要求在动态变化的视频帧中，将目标分割与目标跟踪</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631555&amp;idx=2&amp;sn=e5a8db9572fd5fb15f324b5238674f82&amp;chksm=97b5636ab2f03c08e26b2058c762d1a5c020785ab9357b7af2e6558dde5bce3bee645b03cfc2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 31 Jul 2025 10:10:00 +0000</pubDate>
    </item>
  </channel>
</rss>