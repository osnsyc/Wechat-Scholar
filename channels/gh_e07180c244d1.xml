<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM6aYkwkiboia6lA9D7ANy49WBe9icxn5NQqJjvn4Pyntzvfw/132</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[腾讯 ARC Lab 开源 IC-Custom ：一个强大且灵活的图像定制化工具！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtrgmDwRqjuAJ99YVQtTtz8S1wUIicDCj1ibQiaxOEJ3qJQyPCIbQnEgwa3owkhlOBCgg7klW6Fqb4ng/640?wxtype=jpeg&amp;wxfrom=0"/><p>如何让AI根据我们提供的一张（或几张）图片，生成具有相同主体、但场景和风格各异的新图片？这就是“图像定制”技术，它在虚拟试衣、产品展示、IP创作等领域拥有巨大潜力。然而，现有方法通常需要为每个新主体进</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247634002&amp;idx=1&amp;sn=c26d3a3f4f424aac851ef51046350f5f&amp;chksm=97b1b4726175bdd9115803faca1fd61ec5b3ae0c414678cf32b314388616143f3ebab59b651d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 04 Sep 2025 13:29:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[USO：鱼与熊掌亦可兼得，字节跳动提出统一框架，完美融合主体与风格生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs85VAeryJxCcyrib6LNAMEr2VHua9PQRU4u500wqGBwuCSmibOT5Uh3GFIHWzBIJe5V1EIDzna7uzw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在AIGC图像生成领域，有两个非常主流且看似“对立”的需求：主体驱动生成（Subject-driven）和风格驱动生成（Style-driven）。主体驱动：类似于制作“数字分身”，追求的是让特定的人</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633916&amp;idx=1&amp;sn=9210a6a9acc38dd3a4c518830b833955&amp;chksm=976d1003f9a620b2bc826d0c35e2a41585da28a50b35a5778fcfbfe5f2785b7db3b4af1be645&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 03 Sep 2025 12:29:04 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | InterVLA：聚焦第一视角感知决策，大规模通用人-物-人交互数据集与评测基准]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTulRzNUialC9ibhCibFVZKHpo0KTcSRr5MIL1eadz9iaU4tRqhz7hR9icAQ0C9gjo9QU823ibibztql7qfwg/640?wxtype=jpeg&amp;wxfrom=0"/><p>近日，上海交通大学联合宁波东方理工大学、南京航空航天大学以及联想的一篇关于第一视角人-物-人交互的研究工作被计算机视觉顶级会议ICCV 2025录用，论文、代码、数据均将开源。论文标题：Perceiv</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633894&amp;idx=1&amp;sn=f88acedca2541a4ce4fb16a902946b4a&amp;chksm=973a51487824826a2e51a9eb879c7e03f54eace0f37750bb808067bf72bf48b1ccde02389291&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 02 Sep 2025 12:56:47 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[字节跳动提出OneReward：一个奖励模型统一多任务图像生成，效果全面超越PS！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTt2PiaYXouHjtyW6XDmZibGGGzqPojEfR49iaffKTC1QdD5fxN4EQmibetqjRfwvSWAqencb6CBSNkZpA/640?wxtype=jpeg&amp;wxfrom=0"/><p>图像修复、内容扩写、无痕移除、文字生成……这些强大的图像编辑功能，我们通常需要在不同的软件或模型中切换使用。有没有可能用一个统一的模型，同时精通所有这些任务，并且在各项指标上都做到顶尖水平？来自字节跳</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633779&amp;idx=1&amp;sn=3054505fa255a575def5cf950e11c767&amp;chksm=97b4a1c6a25cd22f38e839b2febc015303b431160e2b26171b1242768a54f97b363d31e2edaa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 01 Sep 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[POSE：100倍加速视频生成，腾讯混元提出单步对抗平衡蒸馏框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTt2PiaYXouHjtyW6XDmZibGGGFkD8juPmWXib5fJunWD1c1SXL0lpjTmSeE32X09R52S0WWOeicAIiaogA/640?wxtype=jpeg&amp;wxfrom=0"/><p>近日，视频生成领域迎来一项重要突破。来自腾讯混元与加州大学洛杉矶分校的研究团队，共同发布了一种名为 POSE (Phased One-Step Adversarial Equilibrium) 的创新</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633735&amp;idx=1&amp;sn=f61f6f6dc7e7b4ce5bd56b101848759b&amp;chksm=97bec8cd2190e0505e9c95811cee056219c0933e6ded98e16c53ec9c287871ee205776e6c6eb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 31 Aug 2025 22:14:15 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 AnimateAnyMesh：文本驱动通用网格动画新范式，实现高效高质量4D内容生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtf4XRHF89ZKb7IaWicpIgnlpr3cFDwPt7mxicuSy0odJvuSPzzjAAxScMqDcw7gqVt9rgKjGCf895w/640?wxtype=jpeg&amp;wxfrom=0"/><p>引言4D 内容生成，即包含时间维度信息的 3D 内容创建，在 VR/AR、游戏等领域具有广阔的应用前景。然而，由于时空建模的复杂性和高质量 4D 训练数据的稀缺性，创建高质量的动画 3D 模型仍然充满</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633723&amp;idx=1&amp;sn=e9eed6bd3d71d18fc2f4ef4271397346&amp;chksm=974c0ab5778df91a535dd00aabcc759509a17a4ae78d453f44d943aa5b687a9bf27548b3b600&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 30 Aug 2025 19:31:27 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[苹果发布MobileCLIP2：最强移动端CLIP，开源数据生成代码！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsV3X5r4e4W1QyOVr4sdCwhlp2tUmWk4JA2LP3aTpqb1clJGzEfznTo6KYialQoeXvic1ODDxoT1XFA/640?wxtype=jpeg&amp;wxfrom=0"/><p>苹果公司的研究人员最近推出了 MobileCLIP2，这是其高效端侧多模态模型家族的最新成员。作为MobileCLIP的继任者，MobileCLIP2通过改进多模态增强训练方法，在低延迟、轻量级的模型</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633708&amp;idx=1&amp;sn=81e077d6e670ebc110ae66b8b37ef534&amp;chksm=9701a82ebefd6be64032fc51850a3ba3f3b315b6adac1743ebcf0db0ece7547386dd2ad84774&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 29 Aug 2025 11:04:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 3D-MOOD：让单目3D检测走向开放世界]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu2PK9BkDZJyWAXPibRSfGVL8QV18ZKZr4nZIvYWk9MrTtYR6O1ia4axvg8aicedUgeWibIeA5eAxfkcA/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文解读一篇在3D视觉领域具有开创性意义的论文：“3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection”。这篇论文首次系统</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633597&amp;idx=1&amp;sn=f722dd8026a1e274102c9542e68a6c78&amp;chksm=97d42f254f1b7a58aabd0086d8a267a8f43355d8a6964a3baa388fd96f41657b34f44343e23b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 28 Aug 2025 12:15:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[FoCa：将特征缓存视为常微分方程求解，实现DiT模型高达6.45倍无损加速]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsial8c09EMEdWDjvMmIbl70vmoLT3mDribISPLOVKwDmqyrI4vopkgCS6cmgvb78N4vdbYD1JEib6ZA/640?wxtype=jpeg&amp;wxfrom=0"/><p>扩散变换器（Diffusion Transformers, DiT）在图像和视频生成方面取得了卓越的成就，但其巨大的计算成本限制了其在实际应用中的部署。为了解决这一问题，研究者们提出了特征缓存（fea</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633517&amp;idx=1&amp;sn=377375b08e6e58196737eaac1e530a4b&amp;chksm=9799ed89ddfde7360a9962680def7003a53d76267d5df0ea29462b5a3917bdb940f4506af3ab&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 27 Aug 2025 12:13:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[量子压缩思想启发，Squeezed Diffusion Models让噪声更有“方向感”，FID提升15%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuUVdDBvsQZLaicF1MPTSE6gzNbpCfVHUyUPFycftxwpnYZmwZZuewGRKtM3mI50nElVtxicezkQZkQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍的论文是来自斯坦福大学的研究者们发表的《Squeezed Diffusion Models》。这篇论文受到量子物理中“压缩态（Squeezed States）”概念的启发，为扩散模型提出了一种</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633425&amp;idx=1&amp;sn=e5c92ad44f3a44aa033c362394cc0a3d&amp;chksm=97fbe5da70ea5ce4c09b82e5dbb0faf3b54d1a78b8446897a9915797374276976afb20ba2461&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 26 Aug 2025 13:52:37 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[站在巨人肩上：索尼AI提出“模型驱动”新范式，“继承知识”铸就强大视觉基础模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsN8tSl7eUibu5QwKXibyspU390qS8SpdsNMmkiaQ1eJeicvmFlPdT6o17nEXgLgWicfuXEzsgib0vg1SOA/640?wxtype=jpeg&amp;wxfrom=0"/><p>近日，来自 Sony AI 的研究者们发表了一篇题为《Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Visi</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633349&amp;idx=1&amp;sn=3ca52dac1178d9d59b98421523d64ec5&amp;chksm=97d780b2e8428113e1993656c641155ac40cfece1524ef7f8b8c4292dc762a4b27ad230d103b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 25 Aug 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[挑战扩散模型基石：T-space解耦，实现4-6倍速分布式训练]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsN8tSl7eUibu5QwKXibyspU3L3vvnNYRGxBSbUshaib6ictkVHm3ibbViaqC721G79riaEXMsaAPLWpEnVA/640?wxtype=jpeg&amp;wxfrom=0"/><p>来自亚马逊的研究团队近日发表了一篇引人深思的论文，题为《Disentanglement in T-space for Faster and Distributed Training of Diffus</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633309&amp;idx=1&amp;sn=f1f5019e447e0b2749661eef00cfba32&amp;chksm=97d9dc534aa45103d1fb4004e12e03a6a3a21b58e02be35e512b363a93d75e77d1f1fc25ee97&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 24 Aug 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[UniUGG：首个统一3D理解与生成框架，让AI同时看懂、问答、还能创造3D世界]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSFpmHojpWNgflep1DniccbXfqoZE3Q4ibqCP9cy1CM5GgVGNvmfBBjuamMicUKbetKPXdmCVW71SSQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文要探讨一篇来自复旦大学和华为诺亚方舟实验室的最新研究 《UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633289&amp;idx=1&amp;sn=0ce3c60822428be321b7a32404fe0116&amp;chksm=97eef93475f2a81e9f188b28eef224ea9ff2becb2066f053e062ba495528805753d1daa14bf5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 23 Aug 2025 12:12:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工&amp;Netflix提出CineScale：解锁8K图像和4K视频的电影级高清生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuoONxWl1tle4ebDvwbicKRepCamic1XnhssCZao3oKfeRicup4n2nRX0qtxBOR5NjXiagOZqribFgAm4w/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇由南洋理工大学S-Lab和Netflix Eyeline Studios的研究者们共同完成的重磅新作，论文标题为《CineScale: Free Lunch in High-Resolut</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633287&amp;idx=1&amp;sn=320b4ee326ae86063548c3d22efc0cf6&amp;chksm=978e613f960c89a69c4575a6936804643cee3c230e95bc7dc645a4e53508efd6f6e1cfca7bb2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 22 Aug 2025 14:07:20 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[GSFix3D：当高斯溅射遇上扩散模型，解锁新视角修复神技]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSFpmHojpWNgflep1Dniccbxiaxh8bBB8DRmrbOrmhWcLxrlKXKH6IuyNEjML7mCRzzTvcgCibWQkGQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>3D高斯溅射（3D Gaussian Splatting, 3DGS）技术以其闪电般的渲染速度和逼真的画质，在三维重建领域掀起了一场革命。然而，这项技术并非完美无瑕。当面对训练时未曾见过的新奇视角，或</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633159&amp;idx=1&amp;sn=1b884c07209011f8db476d1b3763b4e8&amp;chksm=9789414707c158c17d877585f0e070915339585b42296a966678c56569fb92ac6ade3dd77c5c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 21 Aug 2025 14:43:47 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里提出Vivid-VR：概念蒸馏，教T2V大模型学会视频修复]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSFpmHojpWNgflep1DniccbZnMhorYvsKXej7g8dxBPrqX0eFKUfzJjiaViacq0gjH6avEKfRo9xsRA/300?wxtype=jpeg&amp;wxfrom=0"/><p>近年来，以Sora、Latte为代表的文生视频（T2V）大模型，凭借其惊人的生成能力，展示了AI在理解和创造动态世界方面的巨大潜力。一个自然而然的想法是：能否利用这些强大的预训练模型来“修复”那些画质</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633159&amp;idx=2&amp;sn=3676697c38e073426ae55b2274d47041&amp;chksm=974e00db976be1f04a32d0e127dd83809f58cdba86a56a53c29c8aa93d972f4c2bbd0ceda912&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 21 Aug 2025 14:43:47 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2025 | 骨架动作理解大一统：东南大学等提出USDRL，一个面向密集表征学习的基础模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu4O9kFJBDQUZekEBeHHzlEQfLYCNqI3hpY0Rz1iaKXH0gibM0mM9dnXoBFDRoqNc0D6MKLymZdRUoA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天，介绍一篇已被TPAMI接收的论文，该研究由东南大学、西北工业大学、南京大学、南京理工大学以及中国科学院的学者共同完成。论文针对当前基于骨架的人体动作理解领域缺乏一个能够处理多样化任务、具备良好扩</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633107&amp;idx=1&amp;sn=2255e5bd2c7dadc9f478a011f25161ae&amp;chksm=979bde120f9312bbd9238cfe6cbd7966932120986a0fbd06bf89e00d82895e65ceac34d41d3c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 20 Aug 2025 18:14:06 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[为长视频生成减负！浙大与华为提出Compact Attention，挖掘结构化稀疏加速2.5倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu4O9kFJBDQUZekEBeHHzlEMHhUVFGWicXqKslialKzNicpM8FFxDk35uSfsdLkWMQAibsEbzFaAgbNkw/300?wxtype=jpeg&amp;wxfrom=0"/><p>随着Sora、可灵等模型的涌现，AI视频生成技术正以前所未有的速度发展。然而，在通往更高清、更长时视频的道路上，一个巨大的计算瓶颈始终存在——自注意力机制（Self-Attention）。对于基于Tr</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247633107&amp;idx=2&amp;sn=51b70ffa00a7eec0bdf1a6389276477d&amp;chksm=971fe76111dd9ea77c92c3f0b65190cf7a30ad61c32ffab68c93f0be7373dabf725090054b94&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 20 Aug 2025 18:14:06 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | DeCLIP：解耦CLIP注意力，哈工大（深圳）、港大提出通用开放词汇密集感知新框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvK01rRpuicx4giaYhB58UysUDD2oMLyrW8rvxSKN5D8u8wyicxwqnj66dIFXGpuveyficDJTcY6No7dg/640?wxtype=jpeg&amp;wxfrom=0"/><p>当前，目标检测、实例分割等密集视觉感知任务，大多仍受限于一个“预定义”的封闭类别集，这极大地限制了它们在视觉概念无界的真实世界中的应用。尽管像CLIP这样的视觉语言模型（VLM）在开放词汇（Open-</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632981&amp;idx=1&amp;sn=3350d3ae0803418e30755ed646536f0f&amp;chksm=9765fb1fa8bdaee007b7d3a1ab18f44975ca170007e13662ac541ac3a47957d0a03322703926&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 19 Aug 2025 21:43:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[英伟达ViPE：任意视频一键转为3D几何数据，开源引擎与亿级帧数据集重磅发布！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvK01rRpuicx4giaYhB58UysUsqFRomLZpDuzXqhZjV6N8MC1DGnd8RR8BViaone4CkkM8iatOlLND6iag/300?wxtype=jpeg&amp;wxfrom=0"/><p>精确的三维几何感知是机器人、VR/AR、自动驾驶等众多空间AI系统的基石。然而，当前最先进的方法大多依赖于大规模、高质量的标注数据，但从真实世界的视频中获取一致且精确的3D标注（如相机位姿、深度图）却</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632981&amp;idx=2&amp;sn=a9f6060acf1661657e9a2967acaa3c90&amp;chksm=971ec81d2a42aeb159d15371aeaf54c3b4da47961c0bcc5d133a241ecc5ddf6c34709224f9f8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 19 Aug 2025 21:43:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | MobileViCLIP：快55倍！南大等提出首个高效“视频-文本模型，让多模态AI在手机可运行！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuubFXJFggHLr698gpo7hFHN4ibg6d6bQJjt0ibWxqZZRweW0GFqKpYWB5ENC08AzQ0yNeclP13daQg/640?wxtype=jpeg&amp;wxfrom=0"/><p>关注公众号，发现CV技术之美视频-文本预训练模型（如Video-CLIP）在视频搜索、分类和理解等任务上取得了巨大成功，但这些强大的模型几乎无一例外地基于庞大而高延迟的Vision Transform</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632945&amp;idx=1&amp;sn=0bb02ebde8548f2314c63e178d5a3aef&amp;chksm=97978837025935c4034cc33753ca307909aa5eac88afbc68e43bd7b45b2a02b51e72846377d9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 18 Aug 2025 13:17:06 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[突破长视频生成瓶颈：南大 × TeleAI 联合推出全新 AI 生成范式 MMPL，让创意“一镜到底”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuubFXJFggHLr698gpo7hFHdEVQ0qdk5hibc8XGcTyd2b6h2PjdaWFvELicDtePjSfPFCMQr3CNszXQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>关注公众号，发现CV技术之美你是否曾被 AI 生成视频的惊艳开场所吸引，却在几秒后失望于色彩漂移、画面模糊、节奏断裂？ 当前 AI 长视频生成普遍面临“高开低走”的困境：前几秒惊艳夺目，之后却质量骤</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632945&amp;idx=2&amp;sn=a4727747de5d9d50d620a769fa4f1fac&amp;chksm=973b92c988d48c71a13f0b13c4375dcd7978908e7bcb727ebf4a4d3fcbf6072124f31a05b992&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 18 Aug 2025 13:17:06 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[给DiT装上“迷你”控制舵：NanoControl实现高效精准控制，参数量仅增0.024%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTszaqfhcRYL2ZfWys1FqWvQvtMmiaGicQflopOAoHe6TmLdE2MIQcTibLX1HgVBc7fYtJYtxPAqYlHmA/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着以Transformer为核心的DiT架构（如Sora、Flux.1）在文生图领域展现出超越U-Net的强大实力，一个新挑战也随之而来：如何像ControlNet控制U-Net一样，高效、精准地控</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632850&amp;idx=1&amp;sn=f0a69f6faf877405d9691c1c2ad70f14&amp;chksm=97e8ba0aab5d705642aad9d99b7149fd5ff91c4dc652936b7e1ebd8f7a747f62aadbbf7a7cea&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 17 Aug 2025 18:05:22 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 告别“尬舞”，InterSyn交错式学习生成逼真多人交互动作]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs2C0CD9wGRIbrNFmSfxD1iaqjro5lKWfdNrFHtpibughXHtlkzXY7ibfgkpoQMThfaJbnDC3Ypg3zcw/640?wxtype=jpeg&amp;wxfrom=0"/><p>在虚拟现实、游戏开发和电影动画等领域，根据文本描述生成逼真的人体动作（Text-to-Motion）是一项关键技术。尽管现有的AI模型在生成单人动作（如行走、跳跃）方面已取得显著进展，但在模拟多人交互</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632795&amp;idx=1&amp;sn=6270db16d1039e4347ee1de15d68670e&amp;chksm=973b8e4dfda63fd5db753e449fa862045bf6d8e69a0ec9354eaf1a429497855da48b8f1db747&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 16 Aug 2025 12:36:23 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[DINOv3震撼发布：Meta AI的视觉巨兽，重新定义自监督学习]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsl0vhhj5ficFMpVn8D6JTy9FZFrDoHAbze3BqVpwULORCBAmb8lkN9wCVNgUmXlWlzMKhURavT9zw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天要介绍的论文是刚刚发布的 DINOv3 ，这不仅是DINO系列（DINO, DINOv2）的最新力作，更是一份宣告自监督学习（Self-supervised learning, SSL）迈向新高度</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632741&amp;idx=1&amp;sn=ea40c86462c9d67a3850357473c81fe7&amp;chksm=971d3cb209207b53543561441741bb7a8a9905bb5d5e7ea0adf9c7e7133d9950a053f8db7be0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 15 Aug 2025 11:46:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 (Oral) | DPoser-X：基于扩散模型的鲁棒3D全身人体姿态先验，树立领域新标杆]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsl0vhhj5ficFMpVn8D6JTy9AgpF67cibak8sPKmplXQEYoKSlUA5aHLs9qVC5LwlZd0EUI054rg2Fg/300?wxtype=jpeg&amp;wxfrom=0"/><p>本篇介绍的论文是《DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior》。这项研究提出了一种名为DPoser-X的创新方</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632741&amp;idx=2&amp;sn=5c96a20f6772200ad29d5eb07d3238fd&amp;chksm=97d93214eed006dafa0397c06644ab1c058d8d84a0f602b269abd6693976d8459f3a6bba0bf0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 15 Aug 2025 11:46:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦&amp;微软提出StableAvatar: 首个端到端“无限时长”音频驱动的人类视频生成新框架!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuyBWNJ3Hib5ric0ibfxszUh9ldwZDvpgYibIHv0ZicuFbIjSc55bn8xAhwQFicI4Nkhb0uavcXhU8ibqHsA/640?wxtype=jpeg&amp;wxfrom=0"/><p>关注公众号，发现CV技术之美扩散模型的兴起极大地推动了语音驱动人类视频生成的研究。具体而言，语音驱动人类视频生成旨在基于参考图像与音频，合成面部表情与身体动作与音频高度同步的自然人像视频，在电影制作、</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632542&amp;idx=1&amp;sn=247c4fa9678b70bea7543ea559cb76b7&amp;chksm=97c8327ab20490e924a27ea0b1f5d4e0e3ded48ee97fb85406fb866b18302245f23d05c18c4e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 14 Aug 2025 13:30:53 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | HVPL：分层视觉提示学习，让“视频实例分割”模型告别灾难性遗忘]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuyBWNJ3Hib5ric0ibfxszUh9lPRtQGMQCnXjlCR50xQ6XNaPSmnR7RGN1StrrsjVcjicibKdWeicpoKbog/300?wxtype=jpeg&amp;wxfrom=0"/><p>关注公众号，发现CV技术之美视频实例分割（Video Instance Segmentation, VIS）是一项强大的技术，它不仅要分割出视频中每个物体的轮廓，还要在不同帧之间持续跟踪同一个物体实例</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632542&amp;idx=2&amp;sn=736ecf2114538243bfcc3a1db50cbd65&amp;chksm=97e05de4b02c52cf3f07956cd3bf548df37564b288970335a7f1136d3ab31eda88a3faec2db4&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 14 Aug 2025 13:30:53 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[耶鲁&amp;大连理工&amp;南洋理工等提出MDCNeXt：X射线下的“动力电池缺陷精准检测”新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuEiamNv0zMpU2M0xEbp1dUNLezfiaMt2x6DtvonpfedZUTJN3wiamD2OIzwAemqGjhJsfib0GXP62ibsw/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文探讨一篇聚焦于电动汽车核心部件——动力电池质量检测的最新研究论文：《Power Battery Detection》。这篇论文对一项名为“动力电池检测”（Power Battery Detecti</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632392&amp;idx=1&amp;sn=1ccd06da3ec6ed943d21827ba51b587d&amp;chksm=97669bbe6bb0e52b8a3f2c5f05681fd1bacc9a30909c55ec02be882243f4ceec77ee4233a24f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 13 Aug 2025 12:43:37 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IEEE TPAMI 南洋理工&amp;哈工大提出 MARCONet++ 攻克中文文本图像超分难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuEiamNv0zMpU2M0xEbp1dUNWUpeH1NtmO4XyBBxNZDeeCw3Vic2sPblyKPCGA4h4karnb0flPZTOng/300?wxtype=jpeg&amp;wxfrom=0"/><p>文本图像超分辨率（SR）旨在从低分辨率图像中恢复清晰的文字，但这项任务在处理结构复杂、样式多变的中文文本时尤其具有挑战性。现有方法大多为英文设计，或依赖于字符识别先验，在处理严重退化或不规则布局的中文</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632392&amp;idx=2&amp;sn=d146994564d628b5e790d98861c8505a&amp;chksm=977d10dcefd4b0825629072fd0fba5b23b53ddd8238e213450dbbb9241f46a0337a0663e5549&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 13 Aug 2025 12:43:37 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TCSVT 2025 | 跨模态学习助力复杂工业过程异常检测：FmFormer框架与基准测试新突破]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv1doYgP4xDzpiaWJBwYQR4M2KooszubibXFnool9ZqobM8Ius83adAhdwbVxl6Aia0IBfulEZmmaOyw/640?wxtype=jpeg&amp;wxfrom=0"/><p>关注公众号，发现CV技术之美本篇分享 TCSVT 2025 论文Cross-Modal Learning for Anomaly Detection in Complex Industrial Pro</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632355&amp;idx=1&amp;sn=fd6c9e83a9e617c65854eda894516f04&amp;chksm=97b5cccd9da5e8702437641c7d2156bb0f23b5048ba4511c0bc67b8e506d353dff154eb8869d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 12 Aug 2025 12:23:24 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | LightSwitch：CMU提出材质引导的扩散模型，2分钟实现高质量三维场景重打光]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv1doYgP4xDzpiaWJBwYQR4MUJkXRTUlozlfkXA77vnlg78vByl3KykwPUCk9W2XibL3gmRz6T5SQ6g/300?wxtype=jpeg&amp;wxfrom=0"/><p>在3D内容创作和虚拟现实中，能够随心所欲地改变场景的光照（即“重打光”，Relighting）是一项至关重要的技术。近年来，利用2D生成先验（特别是扩散模型）进行3D重打光的方法展现了巨大潜力，但它们</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632355&amp;idx=2&amp;sn=0fa6d86803a35ab07caca0a9ed0a115d&amp;chksm=9730bfc6075136a29f33bbfdfcc7cb0828b606353556271633e62874c9697f21bce173fe6696&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 12 Aug 2025 12:23:24 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[KnapFormer：为DiT训练提速2-3倍，Adobe提出在线负载均衡器新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGoEiaFBcE5ZUkvmzOFsR06GluJGxwpYJdic0Jd9u7GZshjeqIhJiaTMI7w/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇解读来自Adobe的研究论文《KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training》。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632321&amp;idx=1&amp;sn=03c415eca63c08565f25d1f344f950ab&amp;chksm=973671c0e2cb16b34f850bcadc32fedd69a07819b48bf1148218b6c571085ea3b4514eb87298&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 23:35:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ACMMM 2025 | TaAM-CPT：南理工提出“文本即万物”，仅用文本数据实现多模态零样本分类]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGhv3OFvxMV86jRJeldYVmKxQ4rLWCgI0Coz2daQUARicI2Ep3icJOhXTw/640?wxtype=jpeg&amp;wxfrom=0"/><p>多模态学习的目标是让AI能够像人一样，同时理解图像、视频、音频、文本等多种信息。然而，当前的主流方法（如CLIP）都建立在一个昂贵的基础上：需要海量的、成对的“模态-文本”标注数据（例如，亿级的“图像</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632295&amp;idx=1&amp;sn=4c1f3a45f7ffcb120a1cb115a6687da0&amp;chksm=978a1e88dff8b188b7711d8a4e7d3472cb9143ec580dc0ff5d0dcd37518e1fe09366feae76c1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 10:01:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | DPC：用于微调视觉-语言模型的双提示协作]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGM95dRLAjZlic7cZ5Tr3Eo4I9TBhvfvaTE4ib85dmYZIXpOhDNuibLPBAw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文题目：DPC: Dual-Prompt Collaboration for Tuning Vision-Language Modelsarxiv 链接：https://arxiv.org/abs/</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632279&amp;idx=1&amp;sn=bdcc8756baa5f52c96da2dd611a17601&amp;chksm=97727db8934c3482baafd2a738a27f732d09ccbae53cdbc372ed388bc50a4eefa7f8b5060c52&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 04:45:25 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工等提出SSTGNN: 当Deepfake检测遇上图神经网络，一个参数量减少42倍的轻量级统一框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGMyZn0O9FV4663M5HtzQ45FngLb5pWZNgQz7rBh1cH6jwxsg6B5iaOuQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>SSTGNN: 当Deepfake检测遇上图神经网络，一个参数量减少42倍的轻量级统一框架从Sora的惊艳亮相到各类AI视频生成工具的普及，我们进入了一个“眼见不一定为实”的时代。Deepfake（深</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632260&amp;idx=1&amp;sn=e9eba68220be2c7a152636c92ba54cd1&amp;chksm=973dbe16d06b4c125848195de2c3e1cf957f52f8fcb7146919064bc9a8b59ed17add0fa089b7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 02:21:36 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IJCB 2025 | FaceAnonyMixer：在潜在空间中混合身份，打造可撤销、可保护隐私的人脸]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGjqwWBOCqib3SUyP1fL2kK3uMbAT0Q7M53iaiaLva9IIDdjOSibhsNictMqw/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着人脸识别（FR）技术在身份验证、移动支付、公共安全等领域的广泛应用，个人生物信息的隐私泄露风险也日益凸显。如何在享受技术便利的同时，有效保护我们的“脸”？传统的面部匿名化技术往往以牺牲识别性能为代</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632244&amp;idx=1&amp;sn=9df8ecacfee1730b6c59e10a17deaa6f&amp;chksm=976f644e00bf9a3bee06d2e7e0609c77f500840225baa242a268b7fd2a40731431f23912d843&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 01:27:59 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[天大&amp;深圳理工提出：ODOV，迈向“开放域、开放词汇”的目标检测新纪元]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77rb9dK1NO2O6L1xuGCyicWuuUKBlTyc8fkKJYcGTudAkY02IlzQQvp7Lw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天要介绍的论文是 《ODOV: Towards Open-Domain Open-Vocabulary Object Detection》 ，由天津大学和中国科学院深圳理工大学的研究者共同完成。这项</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632231&amp;idx=1&amp;sn=53940289b831ee623e0bea83f4c2ab82&amp;chksm=97d4d8243ade5388b48eee450b65346c44f96efca7fe132179c54ecaf394b219c66c40bed22c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 10 Aug 2025 10:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2025 | DAPT：先解耦再对齐，破解视觉语言模型“信息不对称”难题，显著提升“提示调优”性能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77rDKlUDAbNlbkyNqz1fovkDQKQDcg6H6WKTkMrLIHLIZz7NCW7hH0iaEA/640?wxtype=jpeg&amp;wxfrom=0"/><p>近日，一篇发表于顶级期刊TPAMI的论文提出了一种名为 DAPT（Decouple before Align Prompt Tuning） 的新型提示调优（Prompt Tuning, PT）框架，旨</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632185&amp;idx=1&amp;sn=390d283cdfc37d246336fd231c1e929c&amp;chksm=97d6b83bc0729c2f4308b487dd62c94865caa5003b8e7cf4769bf0a1ab3bfda6282bcc4f6a3b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 10 Aug 2025 04:18:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[厦大、Meta AI等提出OmniEvent：首个统一“事件表征学习”框架，性能最高提升68.2%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77rMbARk06msAawfpz1sndPHqMWXUbyIVfRME90iavMuhicclpA7wSeyoHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>事件相机（Event Camera），作为一种模仿生物视网膜工作原理的新型传感器，正凭借其超高的动态范围、微秒级的时间分辨率和极低的功耗，在计算机视觉领域掀起一场新的革命。然而，这种“未来之眼”也给研</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632220&amp;idx=1&amp;sn=bcf48becc31717d8c857de685839a78f&amp;chksm=97167d9d7f1f36ef216ecfdee495c0da60ec7b124c5395bb099d58d2885c9acba2a6cf9f8c64&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 09 Aug 2025 23:35:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[TIP 2025 | 大连理工等提出PSD：巧借“深度基础模型”，终结分布外（OOD）深度补全难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77ribgkcecFK0xbibicTIMOZzpT1GEibsJn1I9la4mz1ic75JCqYRyPWw6qcbw/640?wxtype=jpeg&amp;wxfrom=0"/><p>深度补全（Depth Completion）是计算机视觉中的一项关键技术，旨在从稀疏的深度测量点（通常由LiDAR等传感器提供）和配对的RGB图像中，恢复出完整的稠密深度图。然而，现有的深度学习模型严</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632198&amp;idx=1&amp;sn=931b69e4b6a48c1a91a5ef654ee3c0c4&amp;chksm=97a6435fa25feb8838ecf672888ce625d3aad7cb4190e42d90f30c4d8e44a6e70fec51544e25&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 09 Aug 2025 13:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[MOSEv2震撼发布：专为真实复杂场景打造的视频对象分割新基准，顶尖模型性能骤降!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77rM9IdeHd2XBJRbMaoRqSQ4JlypFRwQP4Qo5lJHfiamX9C5SCK9DO7Hxw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天，深入解读一篇在视频理解领域具有里程碑意义的工作——《MOSEv2: A More Challenging Dataset for Video Object Segmentation in Com</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632173&amp;idx=1&amp;sn=f631a3d74b08a57524137261d4045679&amp;chksm=97dacca3d375a72ee893a0425db54e8505a569aad600efaf0a09dce1cbc738630f3018ecff49&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 09 Aug 2025 08:09:54 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 | 北大提出SDGPA：仅凭文本描述，实现零样本域适应分割SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvESE3ZCvibzZ54vQZeKSkM3HHGQEaqPYxbxVGjPXtxejuiaVpctqdqTsmrg9nZ3Cg68skh6mdScWaw/640?wxtype=jpeg&amp;wxfrom=0"/><p>想象一下，你训练了一个能在晴天自动驾驶的AI，但它一到下雪天就“失明”了。这就是AI领域的经典难题——域偏移（Domain Shift）。如何让模型在从未见过的新环境（目标域）中也能正常工作？最极致的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632160&amp;idx=1&amp;sn=8cd5c8b56b8dbc3443378905cad222c0&amp;chksm=97790db7ebbfbada85fadb13f82604245090970d3b098bd78a52c4a512ab5c7e2ffbbbbbe6ec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 09 Aug 2025 02:13:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 清华提出 GAP：从“无色”到“多彩”，首个文本引导的点云“高斯化”新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvESE3ZCvibzZ54vQZeKSkM3d2EX9yic86lEFn8viaI63ZPop5xaCQFEJCOwJUu143hV3CnLFEgttyTQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>3D高斯溅射（3D Gaussian Splatting, 3DGS）技术以其快速、高质量的渲染能力，正迅速成为3D内容创作领域的新宠。与此同时，点云作为一种基础且易于获取的3D表示形式，其应用无处不</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632144&amp;idx=1&amp;sn=da28c74e38a824cae7eeae7c58bd48a1&amp;chksm=97b500ff0deed7b388002fea9f4006ac5d55d557ed4519aa522732b7f78e626367ad49868e3c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 08 Aug 2025 23:35:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IEEE TPAMI | 超越“模态鸿沟”：电子科技大学等提出统一模态分离框架，UDA性能与效率双提升]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvESE3ZCvibzZ54vQZeKSkM3E3q02a0FEGvtX6Epwhtr9ibsicKq4fog7AjOmiaQH0SMmOubUlu80YficQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文解读一篇已被顶级期刊 IEEE TPAMI 接收的论文《Unified modality separation: A vision-language framework for unsupervi</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632128&amp;idx=1&amp;sn=1da6fcee97b3efc0224f77db0ee761c8&amp;chksm=97e90bf5ac1f597d0f44e7c9dc7a882b638bc5acbd849f0edf7479176dcfd12a657dec037758&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 08 Aug 2025 12:35:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IEEE TPAMI | 旋转等变性拯救任意尺度超分：西安交大等提出旋转等变ASISR框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvESE3ZCvibzZ54vQZeKSkM3d3pBDDick6Wct6fjh7xKECiaHLwiajJyWk0ic9b27LjMEFuFYVPuo71Tqg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇介绍的论文是《Rotation Equivariant Arbitrary-scale Image Super-Resolution》，它被计算机视觉顶级期刊 IEEE TPAMI 接收。该研究由</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632096&amp;idx=1&amp;sn=37d3c307a38f58796cd1f3554a020771&amp;chksm=97ed585b1be586930f2d239e86557a898f17e88cf55571d68e4378c2bba989979f387ec02ba9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 08 Aug 2025 10:21:12 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[RPCANet++: 让分割网络像数学模型一样可解释，告别“黑箱”！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs2ibat8s7dEkCicoaNtu60awAE1M8kofamQXaJRva011o1LwZGhdTPfQzbnKloU4GgGkPWjibnXUb2g/640?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习模型虽性能强大，但其“黑箱”特性一直备受诟病——往往只知其然，不知其所以然。如果一个模型既有深度学习的强大性能，又有传统算法的清晰可解释性，会是怎样一种体验？本篇介绍的论文是《RPCANet+</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632073&amp;idx=1&amp;sn=884cfd31d04d1b7e91faaf406f866ecc&amp;chksm=971bdd13ed18ae08ed1090655b07c797f97bfb84f3297e53255481042438e372428716b04ee3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 07 Aug 2025 23:34:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | AFOG：当注意力机制成“内鬼”，巧妙瓦解目标检测Transformer]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs2ibat8s7dEkCicoaNtu60aw53icOUKfTmaNlPFnD41ibGv6q64dMEdAs1fCaHtWfbzZHwnrLdibFSxmg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇介绍的论文是《Adversarial Attention Perturbations for Large Object Detection Transformers》，这项研究来自佐治亚理工学院等</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632019&amp;idx=1&amp;sn=bda5626e3be8fa73d496cd39d1b7cc91&amp;chksm=970ecc4e4b7e37d5c650135ee1bb25607aaddab6682d30f5072532e25dd61c9bd452776f8f0b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 07 Aug 2025 08:28:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[小红书 hi lab 开源最强多模态大模型 dots.vlm1，性能对标闭源 Gemini 2.5 Pro和Seed-VL1.5]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs2ibat8s7dEkCicoaNtu60awGHvO8kYTJomKiaBY2tVon3iar10PKicTWutDud4ib3UlrDLOliazibS0A2hA/640?wxtype=jpeg&amp;wxfrom=0"/><p>关注公众号，发现CV技术之美2025 年8月，小红书 hi lab 发布并开源了首个自研多模态大模型 dots.vlm1。该模型基于 12 亿参数的 NaViT 视觉编码器和 DeepSeek V3 </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631954&amp;idx=1&amp;sn=695fa833af8800ad996f25057d15d467&amp;chksm=975a4d5b9bad2ff553f49cb9c814b0a41d5c54852e670130a22dd3f243f95089eee73ce69adf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 07 Aug 2025 06:15:31 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 清华等提出YOLO-Count：让AI“心中有数”，可微分“对象计数”精准控制图像生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcPFTP0eUAGbV4Uu5cfFmqicgxLHOg8gSJ1GcyGDRXGEIeGrW6JO9JiaQw/640?wxtype=jpeg&amp;wxfrom=0"/><p>你是否曾让AI画“三只猫”，结果它却给你画了五只，或者干脆糊成一团？当前强大的文生图（T2I）模型虽然在艺术风格和真实感上表现惊人，但在精确控制生成对象的“数量”上却常常“数不清”。为了解决这个业界难</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631916&amp;idx=1&amp;sn=03f6aac8708471a21138af86e9bd385f&amp;chksm=97a98eeb918379ee70494af3a28cf41a79db93f723e34adddf8a314f3eb116ed53f80573f1d0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 07 Aug 2025 01:10:00 +0000</pubDate>
    </item>
  </channel>
</rss>