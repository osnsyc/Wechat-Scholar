<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM6aYkwkiboia6lA9D7ANy49WBe9icxn5NQqJjvn4Pyntzvfw/132</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[三篇论文看多模态LLMs“视觉文本压缩”，清华、DeepSeek、AI2“不约而同”的深度探索]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvtUBBIAia0hr2ribSpibOgLZeZB6aCdDX8026TW4N6RtvRDEj3W2kGnSKR9kvDI09Wahfew1f4xtn7A/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天我们来聊一个最近在AI圈子里激起不小浪花的新方向。大家都知道，让大语言模型（LLM）处理越来越长的文本，是通往更强通用人工智能路上的关键一环。无论是分析一本小说、一份财报，还是进行多轮对话</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648265&amp;idx=1&amp;sn=0ec214efd0c030504c78ae45dac8f31a&amp;chksm=97cf065fd0d21153386b97f8621adfd43e5263d5ba49f6f92b5c50144ad78b94691974fc383f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 02 Dec 2025 12:54:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[华科&amp;清华&amp;快手推出VGT：20倍加速，唤醒任意视觉语言模型的“隐藏绘画技能”！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvC3L5x1XvGia2SiaG6qsBy5Hk6XYjTuvDEKGEKaDaP8a0WpsXVCVL8qrNMoYKPo2kqYtvj5otVsYuA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自华中科技大学、清华大学和快手等机构的研究者们，带来了一篇非常有趣的工作，提出了一种名为VGT (Visual Generation Tuning)的新范式。简单来说，这是一种“视觉生成调优”</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648265&amp;idx=2&amp;sn=cca2d33133c5559d445f803c7b6e4c14&amp;chksm=97b7ca6e4964c1093c65244e3c91acf769e499be65c5ef7b90ee2f14e645303add2e0ef39b75&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 02 Dec 2025 12:54:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[【EI检索|计算机领域】2025年12月-2026年3月 IEEE 国际会议推荐]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsMTQLdCZg1u8A7Qv2W2SOZN82oNcpiaLkISuA6xAicQWxRunJws8FXnBOcJDDEx9PueKBNPh298vsg/640?wxtype=jpeg&amp;wxfrom=0"/><p>加★标 点关注不迷路IEEE出版| 江苏大学主办1、人工智能驱动图像处理与计算机视觉技术国际学术研讨会 (AIPCVT 2025)会议官网：www.ipcvt.com召开时间：2025年12月12-1</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647975&amp;idx=1&amp;sn=05b4fa2b9710d99f7615b0c4895ff093&amp;chksm=97a707f28309b6ca697dcdefc40923fbcb4a5c23e7a5dc722652474e773ce6a9f1903e6c81db&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 01 Dec 2025 08:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Mamba杀入3D追踪！新框架MambaTrack3D破解高时延难题，精度提升9.5%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTtNY34WOnAOibQmD2ezbY776bXLu9Wov3IhbL0JrfdVwlt1jBnjlE4zQSTwdRCOk8FiaFiau1yRKnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，状态空间模型（State Space Model, SSM），特别是Mamba架构，在序列数据处理领域掀起了一股热潮，大有与Transformer分庭抗礼之势。这股风也吹到了3D视觉领域。今天，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647975&amp;idx=2&amp;sn=a9754b0564aee3565a445ddeefbd78df&amp;chksm=97e0645dc7e2a262a2fa1638e141fe48f83d4aa22c5fb19c07a458ee953208e1d1fcc64e94e7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 01 Dec 2025 08:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[视频运动编辑新进展：谷歌MotionV2V发布，实现对任意物体、任意时间的精准运动控制]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8JNml6HqoVUcFuLIHicWNAj58MjKnmiaRfhibA7UxsEkyZzEtGISXOqtag/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: MotionV2V: Editing Motion in a Video作者: Ryan Burgert, Charles Herrmann, Forrester Cole, Michae</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647902&amp;idx=1&amp;sn=9139f307fc04cc6218fa04fce05574dd&amp;chksm=973e7900c2314ced763eab1fe3515e49850b86681fc855ed4ea0324395910062ba42f798c8d0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 30 Nov 2025 08:00:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Seeing without Pixels！DeepMind新研究CamFormer，相机“轨迹线索”解锁视频理解更多可能！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTOOlJM72Alu3F8PYnTAgMl87kU8jX2yBzT1NSeNQ5v8dylclxdzt5mSo2dLQFZrfe2CFZnnO4mA/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Seeing without Pixels: Perception from Camera Trajectories论文作者: Zihui Xue, Kristen Grauman, Di</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647865&amp;idx=1&amp;sn=0ddde4ee3cd32967dc483c7da24ef91b&amp;chksm=97fa629bfbf81a950fcc8cc1049fe90fae5a06f715c63c4d26ed0d504423214e8652bff8e621&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 29 Nov 2025 17:07:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | ByteDance推出MERIT：首个支持交错多条件查询的多语言语义检索数据集]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTOOlJM72Alu3F8PYnTAgMN5E0MCpntVvlSvnVIf1m7w8ecSjdCZiavAehEGY7jz9YcWDmpHvncxQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>主页：https://merit-2025.github.io/论文：https://arxiv.org/abs/2506.03144代码：https://github.com/weichow23/m</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647865&amp;idx=2&amp;sn=b05aa9f60cc417e17a0ff7eeaefe5591&amp;chksm=9728445c40b6b546a0d89e619d6272f0f9fc9026b53f7a37be86fabb9aa0ced0f188732a56cb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 29 Nov 2025 17:07:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[北大、南大&amp;华为提出DeCo：训练提速10倍，端到端像素扩散模型首次追平潜在扩散！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8WhyVdHGqibKy1jDEQl6egae3JH33jVaYDvdiaRUia0m7oyVsuiawMuxsRQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation论文作者: Zehong Ma, Long</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647763&amp;idx=1&amp;sn=8fd3fb388a0286cc8540bb708f3ca3e4&amp;chksm=97bf3e4fa8f322a6698912f81894ccbf50cd5998f80bda5caeecde0d0edac32d7b4fb081a757&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 28 Nov 2025 11:43:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ACL：DDL40天！查缺补漏篇含时间线!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsMTQLdCZg1u8A7Qv2W2SOZtDkxouscfrPkmRPKclS8LpSHlO3Hjkej2mY4oreAII3jwNkGQHyGkg/300?wxtype=jpeg&amp;wxfrom=0"/><p>了解ACL 2026的关键时间节点对于你规划投稿至关重要。根据目前的搜索结果，ACL 2026采用了与其他顶会不同的ACL Rolling Review (ARR) 系统，其投稿和评审流程分为两个阶段</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647763&amp;idx=2&amp;sn=2520d17918619150e586a9ccb349046f&amp;chksm=97ba1751aed93cb758670cad56914b3a2fee7bb84828e3a7c572700fbee86e1223abfa76294b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 28 Nov 2025 11:43:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[iMontage：南洋理工联合StepFun新作，将视频模型“魔改”为万能多图生成器]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8y2XpBk09xYXlVniayyFzrOZGrqVjFsdxuhLnxxLFn5JrpiaAVq9thcVQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天早前分享了一篇百度用视频生成模型进行图像编辑的文章（把图像编辑看作视频生成，百度提出Video4Edit用1%数据追平SOTA），下面分享的这篇也很有意思，用视频生成模型进行多图生成和编辑。论文标</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647717&amp;idx=1&amp;sn=f9c59d264b60bdda09ff9ae1759430fd&amp;chksm=9721ffaa6eb8ff5d4c50e82ef684de477b1d6e396c9a6737e9ade82de4171521318642ab6dbc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 27 Nov 2025 13:20:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[下一代多模态智能的基石：浙江大学万字长文综述，构建从诊断到治理的LVLM数据生态]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY86f2KnUyM4ibXxS3wNaFd5Wgcc1riaYf87yXtP2tluL0JkibT8s1cVibFmA/300?wxtype=jpeg&amp;wxfrom=0"/><p>下一代多模态智能的基石：浙江大学万字长文综述，构建从诊断到治理的LVLM数据生态—— ARC 框架：划出多模态智能发展的「数据弧度」论文标题: Data Quality Management for</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647717&amp;idx=2&amp;sn=afd06f2bffca9d8df0ec251893503362&amp;chksm=974e8b5e5a142c16d47eb0f8d2b2af87f6ff4363c65b43dcb2f4366be4ec36e474ca3a80c896&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 27 Nov 2025 13:20:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[告别“感知-决策”分离式内耗，Percept-WAM以统一模型实现58.9 mAP的BEV检测，引领自动驾驶新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8CzcaFibhcxbOwVNYetTzc7FgSfDNWAEoqfOvpqOHhCmlW414Y8kVPUg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647479&amp;idx=1&amp;sn=46b4f6ab2c042bbf485892aa89a6bebe&amp;chksm=97a4290eb9384a094d28af59f27b45d3e4be1a67f04abbe2173090f34813a4cca2e29424e1d2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 26 Nov 2025 16:09:27 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CS/AI博士发论文：找到“有价值的问题”就是最好的idea]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsibCBrncbfyiaybeVStZYvqjxOQDxicETGkMpAMOgGmNO9IY9DXoGXMsia0qf6ibicmWSxPk4JXcqpiaPsw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近发现，其他方向博士大多都只需要2篇顶会或高区期刊，而CS博士要3篇起。而且尤其是人工智能、体系结构、网络、软件工程等子领域发表数量更多，博士延毕的焦虑感更重。昨天看到一位博士大佬发了7篇顶会，1篇</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647479&amp;idx=2&amp;sn=23ddb2463cf3e74c55a20081d2e86403&amp;chksm=97261df21dfa935adb5803d4d013bd7b213ab1f492ef9adcf156b6c2c4e8575b44bf743c5ffb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 26 Nov 2025 16:09:27 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦腾讯联手攻克4D检测难题，DetAny4D实现端到端时序一致性目标感知]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8ffFL1pic4sicgflR9SeZn1oDtiaKcWy2c9Zjawv0QOp5emj8mKBFMzwpg/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video作者：Jiawei Hou, Shenghao Zhang,</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647479&amp;idx=3&amp;sn=244fd32a38469afa799f2474d28c84cc&amp;chksm=97a3afbd2e975a78bac77a1e1737a869f8720af9e33c1b0bec8c6e42b07eccd8b12c8de89e26&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 26 Nov 2025 16:09:27 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 教机器人时间管理：华科大&amp;小米提出GRANT，教智能体并行执行任务，效率提升超30%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsibCBrncbfyiaybeVStZYvqjE96RSC1lXQCEu2Licicbol6ibXaa72Rob4G88GrK0lV0sLLbqOibamUdgw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution作者: Dingkang Lian</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647335&amp;idx=1&amp;sn=98095d818e1995657a7ea62f445d9f02&amp;chksm=9749cdf645a0af274b9aa7c09cb41dfe3ebebe448bb67af4afe0328a9af33061d06c86bb5ae1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 25 Nov 2025 20:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[笔墨未动，草图先行！UNC提出SketchVerify，让视频生成告别“物理翻车现场”，规划效率提升超15倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsibCBrncbfyiaybeVStZYvqjHkIdCqiaEN3SEzLdbCYcNELn8kN1JML8KoxWzVQ4VutNJBMnHfSXWnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，视频生成领域真是越来越热闹了，但不知道大家有没有发现，AI生成的视频虽然越来越惊艳，却常常在一些基本问题上“翻车”。比如，物体运动轨迹飘忽不定，或者干脆无视牛顿定律，上演“反重力”奇观。这背后其</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647335&amp;idx=2&amp;sn=cc7619cbff3fc0cbbbb7b33891677a3e&amp;chksm=972be9bf073d814dea6479c03ab4dfd321d0c34d3242f852ca072043a3a5a7c53522ce454112&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 25 Nov 2025 20:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 浙大&amp;上海AI实验室发布RacketVision：首个跨多种运动的球拍分析基准]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTtNY34WOnAOibQmD2ezbY7owG4k7xyibQpTUWzvc0mku1P3zicIex8v4Xtxr78dzbzyNpXRe5aSyPg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis作者: Linfe</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647200&amp;idx=1&amp;sn=e947ebc47021ba55b9aafb97b0c959c0&amp;chksm=97b9e0d2170b36d5ee8231e638ac19476fd98a6c2be457f34bbea6474b18292b58be2511e9b3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 24 Nov 2025 18:25:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[图像去模糊新突破：同济、港浸大联合提出四元数卷积，精准建模通道耦合关系]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTtNY34WOnAOibQmD2ezbY7U5HPTmESTiaIncPmtCgAdggSpbpiaCaSEIlGv35s8ZfmM9viasTSUwHtA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇关于图像去模糊很有意思的新工作。我们都知道，照片拍糊了是个很头疼的问题，而“盲去卷积”技术就是为了在不知道模糊具体成因（比如怎么抖的、怎么失焦的）的情况下，把模糊照片恢复清晰</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647200&amp;idx=2&amp;sn=a585f8ec3f7a91707b95c94e6397fd28&amp;chksm=975e4fb0b812b05810c705145ec206f8c40e4507c947c55467fe1970aad5b8ad9331c055170e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 24 Nov 2025 18:25:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[基础架构的新探索：清华提出Step by Step Network]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTueI12bhdACsFO0uzaMHENEEmt5hmVeTBklpny5CNAGoHe83NdwH6rL4ddYQrVDfq1XP4Ba2jyuJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天我们来聊一篇非常有意思的新工作，来自清华大学和华为诺亚方舟实验室的研究者们提出了一个名为Step by Step Network (StepsNet)的通用网络架构。它的核心思想非常直观，就是让神</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647018&amp;idx=1&amp;sn=68f01a9ad012afde26db76cbaaff7ead&amp;chksm=97f8f4aa4ab29d02f4e572db4741efcf2a5095929666be64c112569019bc0567dc4e69bde76c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 23 Nov 2025 11:38:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | MIT新研究：数据集蒸馏迎来“线性时代”，一张图顶半个ImageNet？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvricRZfrtrTuIU5XiaOa5y1nJ16JGZroAZraL01eLTRFHiaJGfIDZ9bVEUsm52IdiatuichqJibdQBwn1w/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Dataset Distillation for Pre-Trained Self-Supervised Vision Models作者: George Cazenavette, Anto</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646966&amp;idx=1&amp;sn=88dac4b3c22e561dd422c109f18a1bfa&amp;chksm=97d75786643f827369fa08a13a1390cce16fca25861276e62c4acfb697736b73899bdbff0e75&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 22 Nov 2025 12:37:16 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[遥感变化检测，ChangeDINO来了：DINOv3驱动，IoU、F1指标全面SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvricRZfrtrTuIU5XiaOa5y1nfYe0HJ2OicIPIs0fsCcNDXcWm42t7W94IYPbHxGudLuCibKMFIVK6yZQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery作者: Ching</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646911&amp;idx=1&amp;sn=7a79c513509f614e9b7126c9c75ed38e&amp;chksm=976a8376dffa53e2276c07c98163a9572e1dfc2120f0c7351d53b43cc55a65f8eddc31c863c2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 21 Nov 2025 17:38:11 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[解耦骨骼与体型，实现前所未有的动画真实感！Meta SAM 3D核心技术：开源人体参数化模型MHR]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvricRZfrtrTuIU5XiaOa5y1n7Qs2fKZCdzws88icoZZqzoRYj5h6T0s2zvlnWzEGhiaYiccZ8MNDjNQXQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天想和大家聊一篇非常有趣的新论文，它来自Meta，之昨天发布的SAM 3D的关键技术，题为《MHR: Momentum Human Rig》。这篇论文介绍了一种全新的参数化人体模型——MHR</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646911&amp;idx=2&amp;sn=1d9ef3256c8519b1337480753c3ca650&amp;chksm=9714c7b76b940088e5ed66c390ba16c397c4ba35d64adf1fe5e1cef5a06f5188a3a561aed3b5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 21 Nov 2025 17:38:11 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[发布即产品！SAM 3D横空出世：Meta再次颠覆3D视觉，单图即可实现高精度三维重建]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtroia98h5rf12wVfAsP1m3WnVQVamGKYG8WjTyjsUtxtW6JYGAf4fjbGkjIa4nzVVianGiaa2zJj5fw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天，Meta AI 推出了其 Segment Anything Model (SAM) 系列的最新力作——SAM 3D，一个致力于理解和重建物理世界三维形态的开创性模型。无论你是探索 AR/VR 前</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646694&amp;idx=1&amp;sn=b812cbbd33278ddb551d1bface962b15&amp;chksm=97f4ba5b7a5beb7a9e6ac57304f5a9b3a5b2eb97cd8627d2c7babf49bfa7fbce97bf16856570&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 20 Nov 2025 15:11:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[很强很惊艳！Meta重磅开源SAM 3：可概念提示，统一检测、分割与追踪，性能提升2倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtroia98h5rf12wVfAsP1m3WTh0icdZpPRtFAubJwR3ljTNsp2sBqpcCa9ibibzxVMmiaGU2vmz9V03adA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天Meta AI终于公开发布了他们“分割一切”系列的最新力作：SAM 3（之前在匿名投稿阶段）。这不仅仅是一次常规升级，更像是一次“王炸”级别的进化。简单来说，SAM 3现在能够在一个统一的</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646694&amp;idx=2&amp;sn=e451f05131a76f3d52450a0add0822ff&amp;chksm=97fc2e2703ef5146eb253d9b7ada13f1e37b6c3b5de75bfab6b5a239c5fa0536af3d0f02f3aa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 20 Nov 2025 15:11:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[何恺明团队再出手！将ARC视为视觉问题，ViT从零训练60.4%准确率，达到人类平均水平]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsgQHqgLkzkHJEyvk4cicj0n68BUCzoIpMOUhluPUsf9EibqicZ2UlaSvXr9wNzxBciadk6MTBTibqkiaTw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: ARC Is a Vision Problem!作者: Keya Hu, Ali Cy, Linlu Qiu, Xiaoman Delores Ding, Runqian Wang, Ye</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646512&amp;idx=1&amp;sn=8c6aa75138f40097522c1fe5378aff69&amp;chksm=97ec23f9a299dcdc02e80e4de01edf86ff17a8c538f367312b8ca30be07d2d0763516a1651a3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 19 Nov 2025 15:57:10 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CMU新作Co-Me：无需重训，VGGT长序列迎11.3倍加速！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsgQHqgLkzkHJEyvk4cicj0nZwZCzxlb3h3fWK8s6KT2XRRicdPGknHWeTsCcRyUg0QRC2H69bwIFibw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers作者: Yutian Chen, Yuhen</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646512&amp;idx=2&amp;sn=8441816f904d834fbff257f53a367549&amp;chksm=97a37b5d060525f115a2f983b90e82704b3d560013849563aaf1a07881bb0b28a21ebc598cb8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 19 Nov 2025 15:57:10 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[MIT何恺明团队新作：让扩散模型回归“去噪”本质，简单Transformer即可实现SOTA性能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbfkzY7GbB6TyibzDBq4OFRvCGpbDr889LoAyrCgQnhsWVfVkMhGjQMct5OMuf9KjdnYVMsNv6W5A/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天来自麻省理工学院（MIT）的何恺明团队发表了一篇引人深思的技术报告，对当前主流的扩散生成模型提出了一个根本性的拷问：我们真的需要让模型去预测“噪声”吗？论文标题: Back to Basics:</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646237&amp;idx=1&amp;sn=88640d1ae12bfb2b12f2c594280a0ada&amp;chksm=97bf3da5a7372749ffeb673f0c25ee42f4be233ba90dda8639c12b5cacf3923ef86345e74386&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 18 Nov 2025 15:20:14 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Yann LeCun团队新作LeJEPA：仅一个超参数、50行代码，实现可证明、可扩展的自监督学习]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbfkzY7GbB6TyibzDBq4OFRibnBdibM0IhA9JyCZZeWXBoyGmMEGoZmyaibA5biaFJiaU2TS4jubiaKeEcA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有趣的新工作，来自图灵奖得主Yann LeCun和他在Meta-FAIR及布朗大学的同事Randall Balestriero。这篇名为《LeJEPA》的论文，可以说是给</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646237&amp;idx=2&amp;sn=df009aba33b412c14c1cd9c2e8c0758f&amp;chksm=97ddab3eb5e90b8396b209d9e96ba692bc12d15a8a22a44c0f7805aee82389e5751cc6c57202&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 18 Nov 2025 15:20:14 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral MiniShift+Simple3D：面向高分辨率3D异常检测的可扩展数据集与实时检测新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv4nqqn4BfjKjxporPdLG1nqQib1o4MicL54OpsAru0FgMXaR5lia5UbD2hE53A9tmSb0ApuIeCzNINA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在工业质检场景中，细微凸起、微划痕等缺陷可能引发严重安全隐患，例如航发动叶片的微小裂纹可能导致高空故障，精冲齿轮的细微划痕会造成啮合失效，而现有3D异常检测技术面临着效率和精度的双重挑战。来自华中科技</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645986&amp;idx=1&amp;sn=be0744a6e6cfec51bdf475e67ac3eb32&amp;chksm=970e02a2f82899562550c322a937570c7ef5e4f76867a4ceefeb62557b70d51a43e605906e13&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 17 Nov 2025 12:35:54 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 山大提出DiveSeg：为DINO注入“水下感知力”，实例分割性能提升显著]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv4nqqn4BfjKjxporPdLG1nJy4LIQY3jRjLLwFUXfvSn3Tv1Av0Leics08cFh1EORe4eHrtdJBAG9Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天我们来聊一篇来自山东大学团队发表在AAAI 2026上的新工作。他们首次将强大的自监督视觉基础模型DINOv2引入了水下实例分割（Underwater Instance Segmentation,</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645986&amp;idx=2&amp;sn=59400c3fe66754ddc0484efceaeffd14&amp;chksm=97d6277a0fa92e8ddcb2fb344d4b390d43bc00e03151125f947763389a079600eb29fe67e366&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 17 Nov 2025 12:35:54 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 清华大学等提出SpatialActor：解耦空间感知，重度噪声下机器人操作鲁棒性暴涨19.4%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsWAJSALYJdHPJAWIZbibY6tbtPyePuLgOibiaGjxB9zRiaSdrqbY4m3wXt0m7vlUZBexibfvjeP9xmjjg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation作者</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645905&amp;idx=1&amp;sn=3c7289e60c48bd124015cbc3b498e965&amp;chksm=972017ddc5e6096f5956d5308cc155b725727bd38cab6885dff2c998bb5030fc29f13e6a79ec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 16 Nov 2025 07:02:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 中科院联合港大提出ARRA：外部视觉表征对齐重塑全局一致性，让自回归大模型充分释放图像生成潜能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu89QTKP8qGnCS5hcw8XXiaW1qdD5OEiaiaPmzbOrgMeLs0zvmE9VbkLl2E2AhKPyBlQL9APFEomFZJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregr</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645870&amp;idx=1&amp;sn=bcaccdf418731a885668263d3505d55a&amp;chksm=9733f68b9a2c044c8a4cd8352ea53779990a088234826e2d21e0c3e08d2400a95adaa5725ed6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 15 Nov 2025 11:31:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[“全模态”3D视觉基础模型OmniVGGT出炉！即插即用任意几何模态，刷新3D视觉任务SOTA，赋能VLA模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuMLJSWapND0kWBs66j0X7pHrPibsHN4m7ezYs3E9KSvJGMspoRQibBKCxvvsXp5h2ZJicles6Gehiajw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer作者: Haosong Peng, Hao Li, Y</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645843&amp;idx=1&amp;sn=0edb5c298f336157e2292730ef66b717&amp;chksm=9721417cce14fcef72fa073fd3e7aedad77c6763650475b011af3b84a1df8900e1ab419e4a4e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 14 Nov 2025 22:13:58 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[WACV 2026 | PALMS+：融合深度基础模型，手机室内导航的技术再突破]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuMLJSWapND0kWBs66j0X7p6l9NxKlpeKTBbemMjvYFYpniasbYZaicviaexUOHicB6kMwBZH9mdNtSEA/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model作者: Yunqi</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645843&amp;idx=2&amp;sn=73b50f9ee7ae99d3ee07a92f54a36d4e&amp;chksm=97308fcd0133763533aaf77d684b27ec19faf1931cca1bca9dfc54c0495086ef775ec9586303&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 14 Nov 2025 22:13:58 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral 中科大联合西工大提出RSKT-Seg：专为遥感打造的高效开放词汇分割框架，推理提速2倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsWAJSALYJdHPJAWIZbibY6tlWicp1HcNSr7OgibaCH6wp0OWeDrP16Kv2rsfBLL8K5sUTQutfj5DFsA/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing作者: Bingyu Li, Haocheng</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645477&amp;idx=1&amp;sn=0200dcedfbe62bb27116467ce0a70e41&amp;chksm=97dcf42db5a6726ebf367b745c284fb38ebe645b43f6b2e0c753d5ca18dcd760ada1d45d33ec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 13 Nov 2025 14:07:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Roboflow&amp;CMU论文披露RF-DETR细节：首个COCO数据集突破60 AP实时目标检测，速度飙升20倍！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsWAJSALYJdHPJAWIZbibY6tStVff1xFmHLWCAe3ickteqLJwiavJWL8xSMftxxbC5Vhr1jicmZs0Gcyw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: RF-DETR: Neural Architecture Search for Real-Time Detection Transformers作者: Isaac Robinson, Pe</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645477&amp;idx=2&amp;sn=7b7fe5113c80c45571e8857a19035512&amp;chksm=976dbefabde56fe9be70fb254e2c18294893630ab66bb04ea03d3ffcfa171043b6a1747064a3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 13 Nov 2025 14:07:21 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ConsistEdit：重新定义AI视觉编辑，港科大、清华等机构提出MM-DiT注意力控制新方法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvfujXVz1s1hQzXiaTTtUuyBFiaS5QgBDR1EicibqvgSG73ZatRickSmO9l4AVZNEA41whdDq3Uz9ibn8tw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: ConsistEdit: Highly Consistent and Precise Training-free Visual Editing作者: Zixin Yin (香港科技大学),</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645297&amp;idx=1&amp;sn=98310e0c2d9a8e212fad343cba1d786c&amp;chksm=9781220e8bac1deea486d173530bf2fd54e5af5bbe44dbf25ae792a6832a39be9dffd82e1705&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 12 Nov 2025 19:49:17 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 中科院携手快手发布LiveStar：首个“会说话、懂沉默”的直播AI，推理速度提升1.53倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra69vVQrm5ibKGfLxls9CGQyGcSVGROkOSRbeK1wWkJuzau4sqj3dDjiazQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: LiveStar: Live Streaming Assistant for Real-World Online Video Understanding作者: Zhenyu Yang, K</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645297&amp;idx=2&amp;sn=4d6f1fcdae54ff354ef1b0eb42760208&amp;chksm=97c77c26ad061b07f5e8d888218fa15cbade6ede49bde605005cf69430368e5c7d1927c3330d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 12 Nov 2025 19:49:17 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[直到毕业我才知道，原来博士延期是常态~]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuO6szg3WXCv2hOCo8SAevdqdkYq4G1YNX6WqJ2YX07Y0vGnFhdKJVedQtqZX6ulFUCiaQKxiblyIiaA/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近经常收到读者的留言 : 抱怨科研真是太难了，竞争压力大，导师不给指导、不开组会，一年见不到导师几次，对于论文初稿、毕业毫无建议! 其实他不是个例，大家也会有这样的烦恼：前沿顶会、期刊论文、综述文献</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645016&amp;idx=1&amp;sn=1a2f184f79b33e82007d5bed73c27043&amp;chksm=973bd2e8a7d2d389bcc1b52b165466700ec31f6600258930a5211c80d2fc94e8c9ebc3702052&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 11 Nov 2025 12:17:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 上交大、南农大提出ADPretrain：为工业异常检测量身打造的预训练“超能力”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra6HMzozlQqPIuibwM6ausRpnAxduwEQib3wOyKHsFLIIO91rfCeBmXFMeA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有趣的新工作，来自上海交通大学和南京农业大学的研究者们，他们提出了一个名为 ADPretrain 的新框架。简单来说，这是一个专门为工业异常检测（Industrial A</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645016&amp;idx=2&amp;sn=660a0c3459f4f8f9744de4d98300cbee&amp;chksm=9751b3bc5b0f89f58c2bce511ea77314d3e69665110ed80a772481b72df747124c7a2b215800&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 11 Nov 2025 12:17:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[3DV 2026 | 特伦托大学等提出DEMO：让AI看懂复杂人体动作，实现密集描述新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra6jR7UYewmOGZMHTX7Fh60O5txOJ5z7HxsEoD8px1yS2odqSAVkicpOOw/300?wxtype=jpeg&amp;wxfrom=0"/><p>3DV 2026 | 特伦托大学等提出DEMO：让AI看懂复杂人体动作，实现密集描述新范式3DV 2026 | 打破动作理解次元壁！DEMO模型携CompMo数据集开启3D密集运动描述新纪元特伦托大学</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247645016&amp;idx=3&amp;sn=5fe7bd6d9cf772bf818add4b5d0a18c8&amp;chksm=97096d9a6a61d4d98b339f87b960c198411eeaf750a7a6b69a49e8cf9dc2e3cb5227554cb3d7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 11 Nov 2025 12:17:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AI“世界模型”离真实手术还有多远？首个外科视频生成基准SurgVeo揭示“合理性差距”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra6KlenJ0ZIBCtZlCnwtoba0LkQLLePrWHmj3MUiapHibiaUtkXk4ibOxqS2Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，视频生成领域的基石模型正展现出作为潜在“世界模型”模拟物理世界的惊人能力。然而，当这些技术被应用于像外科手术这样高风险、需要深度专业因果知识而非普适物理规则的领域时，其表现如何？这是一个至关重要</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644891&amp;idx=1&amp;sn=9cd8b1694c6911be8a1015249b3a4602&amp;chksm=9712ac245417211234e79740db7d9a5accdc685b95a7d09f82ad1b9929afc484fc623a203d3c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 10 Nov 2025 13:00:57 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ACM MM 25 当MLLM遇上行人重识别：是“降维打击”还是“水土不服”？深度评测来了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtFOYCHl202frRxfdjmicra6x3iaqc1POmukJGcBibHcKibYwcgZn3pD6KFLJoYiaVvpiaodSvoMC2GSUlQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，大型多模态模型（MLLM）的能力边界又一次被拓宽了。当红的 GPT-4o 不仅能说会道、看图作文，现在，来自武汉大学的研究者们想知道：如果让它来做一件非常专业的计算机视觉任务——行人重识别（Pe</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644891&amp;idx=2&amp;sn=c73fce6d4568a3f68cd4239176c9c281&amp;chksm=975fffa48918c4f9452d25a705e4a08dfb43417617edebeecf90b05a691f7608436980e0ddda&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 10 Nov 2025 13:00:57 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS25 | 清华&amp;北大提出LinearDiff-ViT：让Transformer学会“找不同”，实打实提升模型性能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTux5HIyhHZa9cXErvk8Dq97zoichKw2gYOZmC2Uul6TQqxdqicicnEFmZibVj1CejzJdqdTZV3qkFEWdQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，我是CV君。今天想和大家聊一篇来自清华大学和北京大学的最新研究，它给热门的Vision Transformer（ViT）带来了一次相当漂亮的“线性提速”。这篇被 NeurIPS 2025 录用</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644847&amp;idx=1&amp;sn=86679fbc907f3342ce714b35d8895019&amp;chksm=97bfcf7333d6a2959c31f7275d458517fe3abbcfb1ee2218d3cc382526fecafee4f2572c4c27&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 09 Nov 2025 11:32:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TRO'25开源|机器人建图的终局？一个框架搞定光学、几何与语义！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTux5HIyhHZa9cXErvk8Dq97xdnQZfr0Yb0ibSRHUq0Hf23K5vxY8bfe2hvgnFdWq5FmRKCvmlDlicQw/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常扎实的工作，来自北京理工大学团队，并已被机器人顶刊 IEEE Transactions on Robotics (TRO) 接收。这篇名为 OmniMap 的论文，提出了</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644814&amp;idx=1&amp;sn=d11cb2f4ff8460653807d90beb1619d7&amp;chksm=97a2c62d83d9bb2a0cc40657a455f7ba0be0b5b012c0246f73c663c00e98e05f1e6e95656922&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 08 Nov 2025 20:32:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 | 北理工提出BoRe-Depth：仅8.7M参数，在嵌入式设备实现50.7 FPS高清深度估计]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTux5HIyhHZa9cXErvk8Dq97s7Dk4nqovOsRM7LHGQOAcecibWxR7ic296tAoCR1h66jIdRtt3Ae4KGw/640?wxtype=jpeg&amp;wxfrom=0"/><p>单目深度估计是无人系统实现3D感知的关键，成本低廉但效果常常不尽人意，尤其是在算力有限的嵌入式设备上，生成的深度图往往模糊不清，物体边缘细节丢失严重。来自北京理工大学的研究团队针对这一痛点，提出了一种</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644765&amp;idx=1&amp;sn=7559e56f0d7ff4b0de2f09b530b10b63&amp;chksm=9731feefafb82a5358e59983abadc053f069f30a7bd1cf9b625b48cac59ca3e7c94aa389c3eb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 07 Nov 2025 21:32:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TCSVT 25 | 宁波诺丁汉大学等提出De-LightSAM：仅需SAM-H 2%参数，实现通用化医疗影像自动分割]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTux5HIyhHZa9cXErvk8Dq97iaicOuFSVibkh25k47fOVSOEiaOPyqiaUibzdZ9WLXDr3Qcib4ibdfTkIP8ZVA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，分割一切模型（Segment Anything Model, SAM）在计算机视觉领域掀起了一股浪潮，它强大的零样本分割能力让人印象深刻。然而，当这位“通才”进入严肃的医疗影像领域时，却显得有些</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644765&amp;idx=2&amp;sn=5f3c424de43191606872ccef8dc34d29&amp;chksm=97bf49e478f171f5211265cb71acb87f9bc2eb0a744d62fc34362a8666d0d9a3eb353e5b4183&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 07 Nov 2025 21:32:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦大学&amp;StepFun提出WithAnyone：告别“复制粘贴脸”，实现可控、高保真的多人ID一致性生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsyba4iavErNFtUTDh5donxrhibWSpOFpJs7icmpSNwbx896qzWekds4snKcPq4l37hdMF04gNicQ1mDw/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，AI图像生成领域最头疼的问题之一，莫过于如何让生成的虚拟人物不仅长得像，还能在不同场景、姿势和表情下保持身份的一致性。很多模型生成的“写真”，仔细一看，总感觉像是把同一张脸生硬地“复制粘贴”到不</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644594&amp;idx=1&amp;sn=09e73999afe70cb10e14994c2dcac033&amp;chksm=971e7579bf8ce4229abfa867f23c5408b28efefc1bb2264613e7a6f457fb6fcb304bcaf6c331&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 06 Nov 2025 17:35:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[大道至简，中科院等提出OneRef：统一视觉定位和指代分割]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsyba4iavErNFtUTDh5donxrz5oaKQTRdzkoI5MWC0E6Sf3rEgXtmZSViaDHcqtDshliafNScpOdcBZQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>在很多工作中常看到“统一视觉与语言”表示的论文，今天分享一篇语言指代定位与分割领域的工作，来自中国科学院、鹏城实验室和哈尔滨工业大学（深圳）等机构的研究者们，他们提出了一种名为 OneRef 的框架。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644594&amp;idx=2&amp;sn=558ac581a8c7f8546fac26361a9ff20a&amp;chksm=97989716a26f8e34e00dadffebec7d6564b3667b9f74930efc9a9f7374f01418056a3d5663c8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 06 Nov 2025 17:35:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[低光图像增强新探索：SASW-Loss，无需改网络，即插即用提升多种模型性能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsmKeClPicNhOicAH45nrxVtxORSN8ptNCIjibdN1xJPsxg1ibFscWicxvcL8cECECeB11DryYFK3FSic4Q/640?wxtype=jpeg&amp;wxfrom=0"/><p>晚上拍照，光线不好，照片总是黑乎乎一团，还伴随着奇怪的色偏？这在计算机视觉里，是个经典的老大难问题——低光图像增强（Low-Light Image Enhancement, LLIE）。为了让黑夜变白</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247644445&amp;idx=1&amp;sn=600de0b3040c79781ecc3b5ae1037467&amp;chksm=97db0215c710a22a61c879574ddd555c578a2fd23b0595f24d1333a586fd951d06e35881d589&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 05 Nov 2025 13:12:07 +0800</pubDate>
    </item>
  </channel>
</rss>