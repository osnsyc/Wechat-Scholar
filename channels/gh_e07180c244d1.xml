<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://raw.githubusercontent.com/osnsyc/Wechat-Scholar/refs/heads/main/icon/gh_e07180c244d1.jpg</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[KnapFormer：为DiT训练提速2-3倍，Adobe提出在线负载均衡器新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGoEiaFBcE5ZUkvmzOFsR06GluJGxwpYJdic0Jd9u7GZshjeqIhJiaTMI7w/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇解读来自Adobe的研究论文《KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training》。</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632321&amp;idx=1&amp;sn=03c415eca63c08565f25d1f344f950ab&amp;chksm=973671c0e2cb16b34f850bcadc32fedd69a07819b48bf1148218b6c571085ea3b4514eb87298&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 23:35:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ACMMM 2025 | TaAM-CPT：南理工提出“文本即万物”，仅用文本数据实现多模态零样本分类]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGhv3OFvxMV86jRJeldYVmKxQ4rLWCgI0Coz2daQUARicI2Ep3icJOhXTw/640?wxtype=jpeg&amp;wxfrom=0"/><p>多模态学习的目标是让AI能够像人一样，同时理解图像、视频、音频、文本等多种信息。然而，当前的主流方法（如CLIP）都建立在一个昂贵的基础上：需要海量的、成对的“模态-文本”标注数据（例如，亿级的“图像</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632295&amp;idx=1&amp;sn=4c1f3a45f7ffcb120a1cb115a6687da0&amp;chksm=978a1e88dff8b188b7711d8a4e7d3472cb9143ec580dc0ff5d0dcd37518e1fe09366feae76c1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 10:01:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[CVPR 2025 | DPC：用于微调视觉-语言模型的双提示协作]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGM95dRLAjZlic7cZ5Tr3Eo4I9TBhvfvaTE4ib85dmYZIXpOhDNuibLPBAw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文题目：DPC: Dual-Prompt Collaboration for Tuning Vision-Language Modelsarxiv 链接：https://arxiv.org/abs/</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632279&amp;idx=1&amp;sn=bdcc8756baa5f52c96da2dd611a17601&amp;chksm=97727db8934c3482baafd2a738a27f732d09ccbae53cdbc372ed388bc50a4eefa7f8b5060c52&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 04:45:25 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南洋理工等提出SSTGNN: 当Deepfake检测遇上图神经网络，一个参数量减少42倍的轻量级统一框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGMyZn0O9FV4663M5HtzQ45FngLb5pWZNgQz7rBh1cH6jwxsg6B5iaOuQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>SSTGNN: 当Deepfake检测遇上图神经网络，一个参数量减少42倍的轻量级统一框架从Sora的惊艳亮相到各类AI视频生成工具的普及，我们进入了一个“眼见不一定为实”的时代。Deepfake（深</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632260&amp;idx=1&amp;sn=e9eba68220be2c7a152636c92ba54cd1&amp;chksm=973dbe16d06b4c125848195de2c3e1cf957f52f8fcb7146919064bc9a8b59ed17add0fa089b7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 02:21:36 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IJCB 2025 | FaceAnonyMixer：在潜在空间中混合身份，打造可撤销、可保护隐私的人脸]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsfVdAcI7BMxbIHSNb0R7aGjqwWBOCqib3SUyP1fL2kK3uMbAT0Q7M53iaiaLva9IIDdjOSibhsNictMqw/640?wxtype=jpeg&amp;wxfrom=0"/><p>随着人脸识别（FR）技术在身份验证、移动支付、公共安全等领域的广泛应用，个人生物信息的隐私泄露风险也日益凸显。如何在享受技术便利的同时，有效保护我们的“脸”？传统的面部匿名化技术往往以牺牲识别性能为代</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632244&amp;idx=1&amp;sn=9df8ecacfee1730b6c59e10a17deaa6f&amp;chksm=976f644e00bf9a3bee06d2e7e0609c77f500840225baa242a268b7fd2a40731431f23912d843&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 11 Aug 2025 01:27:59 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[天大&amp;深圳理工提出：ODOV，迈向“开放域、开放词汇”的目标检测新纪元]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77rb9dK1NO2O6L1xuGCyicWuuUKBlTyc8fkKJYcGTudAkY02IlzQQvp7Lw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天要介绍的论文是 《ODOV: Towards Open-Domain Open-Vocabulary Object Detection》 ，由天津大学和中国科学院深圳理工大学的研究者共同完成。这项</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632231&amp;idx=1&amp;sn=53940289b831ee623e0bea83f4c2ab82&amp;chksm=97d4d8243ade5388b48eee450b65346c44f96efca7fe132179c54ecaf394b219c66c40bed22c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 10 Aug 2025 10:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2025 | DAPT：先解耦再对齐，破解视觉语言模型“信息不对称”难题，显著提升“提示调优”性能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77rDKlUDAbNlbkyNqz1fovkDQKQDcg6H6WKTkMrLIHLIZz7NCW7hH0iaEA/640?wxtype=jpeg&amp;wxfrom=0"/><p>近日，一篇发表于顶级期刊TPAMI的论文提出了一种名为 DAPT（Decouple before Align Prompt Tuning） 的新型提示调优（Prompt Tuning, PT）框架，旨</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632185&amp;idx=1&amp;sn=390d283cdfc37d246336fd231c1e929c&amp;chksm=97d6b83bc0729c2f4308b487dd62c94865caa5003b8e7cf4769bf0a1ab3bfda6282bcc4f6a3b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 10 Aug 2025 04:18:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[厦大、Meta AI等提出OmniEvent：首个统一“事件表征学习”框架，性能最高提升68.2%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77rMbARk06msAawfpz1sndPHqMWXUbyIVfRME90iavMuhicclpA7wSeyoHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>事件相机（Event Camera），作为一种模仿生物视网膜工作原理的新型传感器，正凭借其超高的动态范围、微秒级的时间分辨率和极低的功耗，在计算机视觉领域掀起一场新的革命。然而，这种“未来之眼”也给研</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632220&amp;idx=1&amp;sn=bcf48becc31717d8c857de685839a78f&amp;chksm=97167d9d7f1f36ef216ecfdee495c0da60ec7b124c5395bb099d58d2885c9acba2a6cf9f8c64&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 09 Aug 2025 23:35:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[TIP 2025 | 大连理工等提出PSD：巧借“深度基础模型”，终结分布外（OOD）深度补全难题]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77ribgkcecFK0xbibicTIMOZzpT1GEibsJn1I9la4mz1ic75JCqYRyPWw6qcbw/640?wxtype=jpeg&amp;wxfrom=0"/><p>深度补全（Depth Completion）是计算机视觉中的一项关键技术，旨在从稀疏的深度测量点（通常由LiDAR等传感器提供）和配对的RGB图像中，恢复出完整的稠密深度图。然而，现有的深度学习模型严</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632198&amp;idx=1&amp;sn=931b69e4b6a48c1a91a5ef654ee3c0c4&amp;chksm=97a6435fa25feb8838ecf672888ce625d3aad7cb4190e42d90f30c4d8e44a6e70fec51544e25&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 09 Aug 2025 13:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[MOSEv2震撼发布：专为真实复杂场景打造的视频对象分割新基准，顶尖模型性能骤降!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsFvBZQAn7dMibkGx1Jfn77rM9IdeHd2XBJRbMaoRqSQ4JlypFRwQP4Qo5lJHfiamX9C5SCK9DO7Hxw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天，深入解读一篇在视频理解领域具有里程碑意义的工作——《MOSEv2: A More Challenging Dataset for Video Object Segmentation in Com</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632173&amp;idx=1&amp;sn=f631a3d74b08a57524137261d4045679&amp;chksm=97dacca3d375a72ee893a0425db54e8505a569aad600efaf0a09dce1cbc738630f3018ecff49&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 09 Aug 2025 08:09:54 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IROS 2025 | 北大提出SDGPA：仅凭文本描述，实现零样本域适应分割SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvESE3ZCvibzZ54vQZeKSkM3HHGQEaqPYxbxVGjPXtxejuiaVpctqdqTsmrg9nZ3Cg68skh6mdScWaw/640?wxtype=jpeg&amp;wxfrom=0"/><p>想象一下，你训练了一个能在晴天自动驾驶的AI，但它一到下雪天就“失明”了。这就是AI领域的经典难题——域偏移（Domain Shift）。如何让模型在从未见过的新环境（目标域）中也能正常工作？最极致的</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632160&amp;idx=1&amp;sn=8cd5c8b56b8dbc3443378905cad222c0&amp;chksm=97790db7ebbfbada85fadb13f82604245090970d3b098bd78a52c4a512ab5c7e2ffbbbbbe6ec&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 09 Aug 2025 02:13:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 清华提出 GAP：从“无色”到“多彩”，首个文本引导的点云“高斯化”新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvESE3ZCvibzZ54vQZeKSkM3d2EX9yic86lEFn8viaI63ZPop5xaCQFEJCOwJUu143hV3CnLFEgttyTQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>3D高斯溅射（3D Gaussian Splatting, 3DGS）技术以其快速、高质量的渲染能力，正迅速成为3D内容创作领域的新宠。与此同时，点云作为一种基础且易于获取的3D表示形式，其应用无处不</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632144&amp;idx=1&amp;sn=da28c74e38a824cae7eeae7c58bd48a1&amp;chksm=97b500ff0deed7b388002fea9f4006ac5d55d557ed4519aa522732b7f78e626367ad49868e3c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 08 Aug 2025 23:35:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IEEE TPAMI | 超越“模态鸿沟”：电子科技大学等提出统一模态分离框架，UDA性能与效率双提升]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvESE3ZCvibzZ54vQZeKSkM3E3q02a0FEGvtX6Epwhtr9ibsicKq4fog7AjOmiaQH0SMmOubUlu80YficQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文解读一篇已被顶级期刊 IEEE TPAMI 接收的论文《Unified modality separation: A vision-language framework for unsupervi</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632128&amp;idx=1&amp;sn=1da6fcee97b3efc0224f77db0ee761c8&amp;chksm=97e90bf5ac1f597d0f44e7c9dc7a882b638bc5acbd849f0edf7479176dcfd12a657dec037758&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 08 Aug 2025 12:35:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[IEEE TPAMI | 旋转等变性拯救任意尺度超分：西安交大等提出旋转等变ASISR框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvESE3ZCvibzZ54vQZeKSkM3d3pBDDick6Wct6fjh7xKECiaHLwiajJyWk0ic9b27LjMEFuFYVPuo71Tqg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇介绍的论文是《Rotation Equivariant Arbitrary-scale Image Super-Resolution》，它被计算机视觉顶级期刊 IEEE TPAMI 接收。该研究由</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632096&amp;idx=1&amp;sn=37d3c307a38f58796cd1f3554a020771&amp;chksm=97ed585b1be586930f2d239e86557a898f17e88cf55571d68e4378c2bba989979f387ec02ba9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 08 Aug 2025 10:21:12 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[RPCANet++: 让分割网络像数学模型一样可解释，告别“黑箱”！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs2ibat8s7dEkCicoaNtu60awAE1M8kofamQXaJRva011o1LwZGhdTPfQzbnKloU4GgGkPWjibnXUb2g/640?wxtype=jpeg&amp;wxfrom=0"/><p>深度学习模型虽性能强大，但其“黑箱”特性一直备受诟病——往往只知其然，不知其所以然。如果一个模型既有深度学习的强大性能，又有传统算法的清晰可解释性，会是怎样一种体验？本篇介绍的论文是《RPCANet+</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632073&amp;idx=1&amp;sn=884cfd31d04d1b7e91faaf406f866ecc&amp;chksm=971bdd13ed18ae08ed1090655b07c797f97bfb84f3297e53255481042438e372428716b04ee3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 07 Aug 2025 23:34:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | AFOG：当注意力机制成“内鬼”，巧妙瓦解目标检测Transformer]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs2ibat8s7dEkCicoaNtu60aw53icOUKfTmaNlPFnD41ibGv6q64dMEdAs1fCaHtWfbzZHwnrLdibFSxmg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇介绍的论文是《Adversarial Attention Perturbations for Large Object Detection Transformers》，这项研究来自佐治亚理工学院等</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247632019&amp;idx=1&amp;sn=bda5626e3be8fa73d496cd39d1b7cc91&amp;chksm=970ecc4e4b7e37d5c650135ee1bb25607aaddab6682d30f5072532e25dd61c9bd452776f8f0b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 07 Aug 2025 08:28:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[小红书 hi lab 开源最强多模态大模型 dots.vlm1，性能对标闭源 Gemini 2.5 Pro和Seed-VL1.5]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs2ibat8s7dEkCicoaNtu60awGHvO8kYTJomKiaBY2tVon3iar10PKicTWutDud4ib3UlrDLOliazibS0A2hA/640?wxtype=jpeg&amp;wxfrom=0"/><p>关注公众号，发现CV技术之美2025 年8月，小红书 hi lab 发布并开源了首个自研多模态大模型 dots.vlm1。该模型基于 12 亿参数的 NaViT 视觉编码器和 DeepSeek V3 </p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631954&amp;idx=1&amp;sn=695fa833af8800ad996f25057d15d467&amp;chksm=975a4d5b9bad2ff553f49cb9c814b0a41d5c54852e670130a22dd3f243f95089eee73ce69adf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 07 Aug 2025 06:15:31 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 清华等提出YOLO-Count：让AI“心中有数”，可微分“对象计数”精准控制图像生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcPFTP0eUAGbV4Uu5cfFmqicgxLHOg8gSJ1GcyGDRXGEIeGrW6JO9JiaQw/640?wxtype=jpeg&amp;wxfrom=0"/><p>你是否曾让AI画“三只猫”，结果它却给你画了五只，或者干脆糊成一团？当前强大的文生图（T2I）模型虽然在艺术风格和真实感上表现惊人，但在精确控制生成对象的“数量”上却常常“数不清”。为了解决这个业界难</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631916&amp;idx=1&amp;sn=03f6aac8708471a21138af86e9bd385f&amp;chksm=97a98eeb918379ee70494af3a28cf41a79db93f723e34adddf8a314f3eb116ed53f80573f1d0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 07 Aug 2025 01:10:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[无人机多目标跟踪AMOT：让无人机在复杂动态中“看”得更稳、跟得更准]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcL93fVp04RxdanDFz7ficWzoy7A2VoXuFwf6ly9AWibZlTibsTAKUvLgicQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇由中国科学院、阿德莱德大学、麦考瑞大学、加利福尼亚大学默塞德分校等机构的研究者们共同完成的最新成果，论文标题为《Tracking the Unstable: Appearance-Guid</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631902&amp;idx=1&amp;sn=b076b3a41062db259806261219de01f8&amp;chksm=97873689dd0bbc19c4657052d52752a708b62fb2234131e163c4c26a915079bbef1eec5a8bed&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 06 Aug 2025 23:33:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 马里兰大学和Meta提出Trokens：语义感知的关系轨迹令牌，革新少样本动作识别]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcs3roXBtQJrknRTIPs2zERzMXibPmB2eteicgm5M3iajbNvH8KetzB9MaQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇来自马里兰大学和Meta的研究论文《Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Rec</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631859&amp;idx=1&amp;sn=baaa5cc8b4cf132c3cc93bd34e762594&amp;chksm=973eff8bb669ba63ff5137d1256401fb3d89b7c3c285d45c89e0afa01298212141a595d7a9b3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 06 Aug 2025 15:19:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南大&amp;复旦&amp;南洋理工等提出LongVie：突破一分钟界限，迈向可控的超长视频生成！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcWuiaOctIC09yvQQLNk7AvOH7HehPkdfCLIqNFDqtdGeeQ4vyJOqlm5A/640?wxtype=jpeg&amp;wxfrom=0"/><p>从Sora的惊艳亮相到Kling的开源，视频生成技术在2024年迎来了爆发。然而，无论是学术界还是工业界，生成几十秒的短片已是极限，想要生成 一分钟以上、且内容、动态、风格都高度可控的 超长视频，仍然</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631843&amp;idx=1&amp;sn=de32e1e11daccdc142ec960177a944eb&amp;chksm=972fd4dd122d585f64ab9b81a3f504fba46e2582c1a4843d70c9419fbe6cbc689ea092c2bfe6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 06 Aug 2025 13:44:14 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 | 浙大等提出H3R：融合显式几何与隐式注意力，通用3D重建性能SOTA，收敛速度提升一倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTticNXmxYKZ3GSPxUtwrlKZcia8hR034uh9oL506a4xmYWzvwSNAV5ia1wntO0ybJa6cNLicxl0u5UPTA/300?wxtype=jpeg&amp;wxfrom=0"/><p>从几张任意视角的照片，就能“凭空”生成一个完整、逼真的3D模型——这是“通用三维重建”（Generalizable 3D Reconstruction）的终极目标。近年来，随着3D高斯泼溅（3D Ga</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631843&amp;idx=2&amp;sn=677cfff13e8113580d40b3dd3920990e&amp;chksm=97f502e1fd9e6a55f0fb4845ba5269bc7bd0d4e7f19725ecb69373a456a30c34d8d0016f5764&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 06 Aug 2025 13:44:14 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[ICCV 2025 Highlight | 彻底告别相机姿态！帝国理工提出SPFSplat：稀疏视图自监督3D高斯溅射新SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsrciamnRdQm2dib8DwzbWvAaotNos4tS084QOiafHs9ib2y23kBfsd9IhIGCMvTiaGfHZjDXVAXSJg9Kg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇来自帝国理工学院的最新研究成果，论文标题为《No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Spa</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631788&amp;idx=1&amp;sn=9799a89d9c6c539ad678a0f7c825ce1e&amp;chksm=975c209ab252e4f0aadb18eaa8ef9108c0cfdb778e809188211e9280d67b3cfb540601f1baac&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 05 Aug 2025 23:33:00 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[南开提出GlimpsePrune动态剪枝，砍掉92%视觉Token性能反超10%！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsrciamnRdQm2dib8DwzbWvAaHBVHmY60m48njdqMItjHRPIHW0wic6icYIplvQxqDNMDM1yL73noww1A/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍一篇由南开大学、Shanghai Innovation Institute、天津大学及vivo AI团队联合发表的最新研究，论文标题为《A Glimpse to Compress: Dynam</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631764&amp;idx=1&amp;sn=db8da1df44df0b8d80dfbaeda1974038&amp;chksm=97c51fa32b99bdd26f2fc879d49d5a9ae31e4bba0ee190f330a3a4f8512c4b44d460f978f29e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 05 Aug 2025 14:54:41 +0000</pubDate>
    </item>
    <item>
      <title><![CDATA[阿里发布Qwen-Image：不止于图，“文图”并茂的AIGC新篇章！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsrciamnRdQm2dib8DwzbWvAaVicfspeCJ9cKu7mViakVoDZuNBMW9wPicH1vwLicTXPS3tiaaWwEX90t12Q/300?wxtype=jpeg&amp;wxfrom=0"/><p>近日，Qwen Team团队发布了其Qwen系列在图像生成领域的最新力作——Qwen-Image。这份技术报告详细介绍了一款在复杂文本渲染和精准图像编辑方面取得显著突破的基础模型。Qwen-Image</p> ]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247631764&amp;idx=2&amp;sn=edebb70fc95668a53472f521cbd13642&amp;chksm=97126a4e295a023823267474d047cefecc5d7026f9b290d6ab6e56bc802a6b182e9956efccf8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 05 Aug 2025 14:54:41 +0000</pubDate>
    </item>
  </channel>
</rss>