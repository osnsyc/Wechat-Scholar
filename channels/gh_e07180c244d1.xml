<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM6aYkwkiboia6lA9D7ANy49WBe9icxn5NQqJjvn4Pyntzvfw/132</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[告别视觉瓶颈！蚂蚁集团、同济大学联手提出CLI，让大模型“看”得更准、推理更强]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuuiccwvsUrxbwJuqNJVTQrlVKntSH9HicC42zd0XfeRGDZ0yxgXkKWgBAdEr8AqHmibOZloSLJbqAAQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天想和大家聊聊一个最近在多模态大模型领域很有意思的进展。我们都知道，现在的大模型（VLMs）看图说话越来越溜了，但你有没有想过，它们“看”图的方式可能有点“死板”？目前，像LLaVA这样的主</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654771&amp;idx=1&amp;sn=00bba3cd629e0206fbc4c571be4be679&amp;chksm=97959f6ed04a36ba2052633ec6800d00c349f8cd9115567bee6bea312a1d48b18e64309510d3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 17 Jan 2026 21:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[2026年截稿的重要CCF会议DDL目录]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuuiccwvsUrxbwJuqNJVTQrlELq63Q1Bv0Na90MBYXVWE7uJfeVmMibpDlbll71n86X0zRVOyB0UbfA/640?wxtype=jpeg&amp;wxfrom=0"/><p>2026年新的一轮CCF刷榜开始了！精准把握计算机方向会议等级分类、截稿日期、rebuttal时间、征稿频次是顶会中稿非常重要的4打因素。时间把握的准确，一年可以4篇A会，把握时机精准转投。反之，一年</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654656&amp;idx=1&amp;sn=588f0382a08b7d21c378d96c6d789486&amp;chksm=9757a87bb7842753819fd4865a437f3bcced594af6049060cb4357f932607bc5a8f3eb3b6467&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 16 Jan 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Ultralytics YOLO26发布：专为下一代计算机视觉而生，边缘视觉AI的新起点]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvriaAodVGM5wWcgesQY9q4UzicMQVTX1kD6efHYEfobv7OUPqaCM2LxibfJSj9eJ8ybQ8xvAoa3HUxw/640?wxtype=jpeg&amp;wxfrom=0"/><p>近日，Ultralytics 正式发布 YOLO26，这是迄今为止最先进、同时也是最易于部署的 YOLO 模型。YOLO26 最早在 YOLO Vision 2025（YV25）大会上首次亮相，它标志</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654614&amp;idx=1&amp;sn=ac96d447cc8e7a0233f6877d10e93409&amp;chksm=973258a15ce592572ae1404b065c43d664f0b17a0d5db0a0a0bca7347c5f953331a124636160&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 15 Jan 2026 21:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦大学推出CME-CAD！异构多专家协同学习刷新CAD代码生成SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuXYQTEQjzWIGicejmH6FeN0LicJqxUEJq1ZHlHGUgyYcaIcMVLdTzogWSfDCwHEbwU5Lzyzr92aqCQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>本文为粉丝投稿。工业设计领域，计算机辅助设计（CAD）不可或缺，但传统CAD建模的复杂度及流程难以实现自动化生成高精度、可编辑模型的目标。现有技术（如从草图构建3D模型）常产生不可编辑的近似结果，无法</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654431&amp;idx=1&amp;sn=f2d9d0e32b1299121d59a75e735b39a8&amp;chksm=97b8c92bc33d741776163051bb5f5db2a6d345b38d246e225c4e04491cd482d5c62f60f86ccb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 14 Jan 2026 13:49:18 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[真实世界3D分割新范式！MVGGT：融合视觉、几何与语言，性能大幅超越传统方法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuXYQTEQjzWIGicejmH6FeN02PGCEFhzQ6xVCnIZgcrw7MyynxrsAhia5iaibfh6yIl0licr8X1wMv2bRw/640?wxtype=jpeg&amp;wxfrom=0"/><p>当我们在谈论让机器人或AR设备理解并与三维世界互动时，一个核心任务是“指代性分割”（Referring Expression Segmentation, RES）——即根据一句自然语言描述（如“左边那</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654416&amp;idx=1&amp;sn=2f9e9ec6075561c3d4c2d61ca0026736&amp;chksm=97093fefc2c95ecd13db2bd6fcac2d0c207d21b5edbe293e5babfea220f007b234de784d2719&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 13 Jan 2026 21:38:35 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[MLLM想得多也降智？Meta新作VideoAuto-R1：首创“二次作答”机制，推理提速3.3倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu6oNAfnlzLEhKj66UsQPnB8Tvcq8f5HbBGWqrs4oorvgPibdgEHhsJ3CTEtz770fl54loguw6dezQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，多模态大模型在视频理解领域可谓是高歌猛进，尤其是“链式思考”（Chain-of-thought, CoT）技术的引入，让模型能够像人一样，通过一步步的分析推理来解决复杂问题。但你有没有想过，我们</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654189&amp;idx=1&amp;sn=41f18de3f62799baa7fd6be54f6add08&amp;chksm=971bafe6725e120f45789f46212474c917f01de76c0904f70eaade105ba5b58c334d85fd0d95&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 12 Jan 2026 12:46:41 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[3D场景生成新突破！浙大、字节提出Gen3R：结构高手+美学专家，单图“脑补”完整三维世界]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsicSwwOfhcwCqHGKj45xUgf89ibBP4mx5VdOm0XWiah7NwHhLEPiaauzZgvB35ey7gib8icibGkn6MwyfjQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天想和大家聊一篇非常有趣的新工作——Gen3R，它巧妙地将两个看似独立的领域——强大的3D重建模型和前沿的视频扩散模型——捏合在了一起，实现了1+1>2的效果。过去，我们想要凭空生成一个三维场景，要</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247654005&amp;idx=1&amp;sn=64d1e05bdbc1b7c2a569ecdf2d41bb95&amp;chksm=97874a4c2cb8210c5cb48bb559171fed3bf99aa03d2b9879398b04f43fd15c0000c4b920fa43&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 10 Jan 2026 12:34:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[2026年1月截稿！AI领域CCFA/B/C会截稿汇总]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvniaHAqcbxxztJJqD7nibok5nIiazzDMlToo6NDTkqxDCBOqkyyjq0k1FE4FmX0FadcwzoFJWZ58VdA/640?wxtype=jpeg&amp;wxfrom=0"/><p>2026年的学术征程已悄然开启！对于人工智能领域的科研人而言，2026年1月堪称关键截稿窗口期——多个CCF A/B/C级权威会议集中收官，既是检验研究成果的黄金赛场，更是链接全球学术资源的重要契机。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247653693&amp;idx=1&amp;sn=3347cb645520f3d9af1f1007927a4f2a&amp;chksm=972451db482904d114560a2cc071d724a83a2f226c09e5a6422473b7a0d9fe0e4aa013ede3a9&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 09 Jan 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[浙大、理想汽车联手推出InfiniDepth：用神经隐式场实现任意分辨率深度估计，效果惊艳！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtU3cHtAOp6biaDaQfmHPfzhibicSqXqVk4QVKxktRgaQdMbQc38ofw0lY6dqE7OsxiaKjwHKwW6cMWMQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>AI生成的深度图总是感觉有点“糊”，尤其在面对栏杆、树枝、人物头发丝等复杂场景时，细节总是表现不佳。这背后的一个核心瓶颈，在于长久以来深度学习模型都将深度图视为一个固定的、由像素格子组成的“棋盘”。这</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247653676&amp;idx=1&amp;sn=986f4fb4e8e85019900a6eaa8490a5f7&amp;chksm=974580b4682ae9f333b5730b848530ef6d66e7979fa69d3b13c1a4ebeea18739bb31b3f093bf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 08 Jan 2026 13:18:12 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | 3D目标检测的升维突破：LabelAny3D来了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuD30IAxvvSTVMDOMkPbpfuiblMibTMt7xAsr18NdHyHRFIeV5NHe2LLLE493icHsAUZzicInZ7fb6bgQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>想象一下，只用一张普通的2D照片，就能精确地框出其中所有物体的三维空间位置和尺寸——这就是单目3D目标检测的魅力。这项技术在机器人、自动驾驶、AR/VR等领域有着巨大潜力。然而，长期以来，它一直被一个</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247653465&amp;idx=1&amp;sn=dac3e7fb884e5cba85ab5927d6c84f23&amp;chksm=97a1ab0926d213069d4b9ec1d2aa1dbb0ebbabef83def0b05cdccc0ac5a08790659208d9da81&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 07 Jan 2026 15:37:52 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[年度总结｜2025多模态领域前沿技术进展！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtgjuGNqSCXPe7YoOq47icSm0cHDvfcjgZ15oRKxtBGI54AImSLQR4XugaDQQ07G4Q4u5vd1ibFd4MA/640?wxtype=jpeg&amp;wxfrom=0"/><p>多模态可以说是当下最火的领域之一，CV和NLP都在积极拥抱它，VLM和3D文生图更是当红辣子鸡。尤为值得一提的是，其任务场景非常广泛、故事性强、且缺乏统一的理论框架，可发论文的着手点很多，创新空间广阔</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247653157&amp;idx=1&amp;sn=8d21a5cd9f122ec033e7eed0b12763f6&amp;chksm=9712624a94d3a423e827d9a6c611e188d539367e4411667f3caa74a31cab413e560079f97e43&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 06 Jan 2026 12:30:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Playmate2：开源多角色语音驱动动画新技术，免训练方法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtgjuGNqSCXPe7YoOq47icSmwdmwLz3qjgZOicj3jibIVBU2ZCpmc7SAYH6HicvU0QBRc2T5oIAVibE3rA/640?wxtype=jpeg&amp;wxfrom=0"/><p>想象一下，你正在制作一个电影短片，或者设计一个虚拟直播间，你需要多个角色能够自然地根据各自的台词进行表演，他们的口型与声音完美同步，肢体动作流畅自然，而且整个过程无需繁琐的训练或大量的人工调整。这听起</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247653062&amp;idx=1&amp;sn=c8f0e07cf9ebdd51cd604923f7713bb9&amp;chksm=975a53ad3ce55d4ae21c5f4cd4484ed00e5cff3dfbf315e2aaf4666d216d5fa323dc17d618e6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 05 Jan 2026 21:09:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[哈工大&amp;湖南大学&amp;西电&amp;澳门大学等三个IEEE Fellow发表的余弦网络的图像超分辨方法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvniaHAqcbxxztJJqD7nibok5Nc0Y7icYCefCSVo4LfZI83xI6pficS4rSkkFzf9VS5S8eQ0wzkKLUnfg/640?wxtype=jpeg&amp;wxfrom=0"/><p>本篇分享的论文《面向图像超分辨的余弦网络》（原英文标题 A Cosine Network for Image Super-Resolution），聚焦于单图像超分辨率任务中结构信息提取不充分、训练过程</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652835&amp;idx=1&amp;sn=c9724892c59805f25ada7f863258878c&amp;chksm=975b8e72ceb03e1889f42e44a1dc75e0790e7d1410fbe96931de2b16e6dcd157ba17e976196e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 04 Jan 2026 21:09:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[“实时数字人”进入次秒级响应！上交大等开源LiveTalk：20倍加速，多轮对话连贯性超Sora2]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv47jmk350p2rfLmbsqPNE22VEyK68geZroH40BOxD8DniaGKRQiaFfFfOqy2EwRVhTjfzMQEWW84cQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文名称: LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652671&amp;idx=1&amp;sn=47a955bf5328058458cf6781a66721f9&amp;chksm=97936bfd6cd4f179195ed2afa4ed3b70262fb01ee3b0fcbb034a7514f163219a3dd0666f9f5e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 03 Jan 2026 22:52:14 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[浙江大学、西湖大学与蚂蚁提出OmniAgent：让模型“先听后看”，实现细粒度音视频推理]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv47jmk350p2rfLmbsqPNE2ab0ONvNdV61IUfbsbc3bxr3CkVWU5rccZaujHI78D1lnbN4wh9XBaA/640?wxtype=jpeg&amp;wxfrom=0"/><p>你是否也曾对多模态大模型的表现感到困惑？它们看似能“看懂”视频，但一问到需要音视频精确同步的细节问题，就常常“翻车”。比如，视频里的人说了某句话时，旁边牌子上写了什么字？目前的许多模型，要么无法对齐信</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652644&amp;idx=1&amp;sn=a78154c3956fd690c511a9a9129cfb1c&amp;chksm=979bc644f0cc34d153aa9bc1ffd0e53e393b7897f91a8b3519d62438228a5a587ecedd0e7e6f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 01 Jan 2026 21:14:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[RealDPO：把“真实”当作偏好，让视频生成模型学会自我纠错]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv47jmk350p2rfLmbsqPNE24H1uFW887h9TagYfQBN5SiarYT1DbZfb4riaLUzMXkQKT11gqoJYiaD3A/640?wxtype=jpeg&amp;wxfrom=0"/><p>你可能见过这样的生成视频：第一帧看起来很惊艳，但一旦人物开始运动，肢体就开始“飘”、动作不连贯、互动不自然，甚至出现难以解释的穿模与关节扭曲。复杂动作（尤其是日常人类动作）依然是当前视频生成的硬骨头。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652592&amp;idx=1&amp;sn=6bb1f2c26165f9d0ffe84c2c08a7cb03&amp;chksm=97d1068833b5e569ecc96e568871736d77e6f3bf8f38b071270ce7550be97e3ee802ebb26004&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 31 Dec 2025 21:04:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | Adobe与JHU提出OmniVCus：前馈式多主体视频定制，多模态控制玩出新花样]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSTmJMxgdicEupPxfzhweJzmnpgSHso00ibBX6DsScaYzSFka0tmMrPZnVibn3pnrPMV92EmzhXg8Jw/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有料的 NeurIPS 2025 论文，来自 Adobe 研究院、约翰斯·霍普金斯大学、港大、港中文和上海交大的研究者们，共同推出了一个名为 OmniVCus 的新框架。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652350&amp;idx=1&amp;sn=35dd5fd450216dc78cb0612fcf5f2c14&amp;chksm=97068003242315d1534eac9089e3c0710257bf4f0bf0bf6683abde5ae70e67c9bee421efd36c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 30 Dec 2025 13:51:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[博士招生 | 中国科学院大学光电学院招收类脑视觉方向]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvn2cGNKvjXDZ2kjTlgNaBk0ibM2ibm0NLSb5Rh6S2EDsib92zhGEHeOgca2oQKVt7twJSkuqUkZpyIQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>中国科学院大学光电学院孟祥悦教授课题组现招收类脑视觉方向博士生，欢迎具有相关背景的优秀学生加入。团队简介本研究团队在类脑视觉成像芯片领域取得系统性突破，成功实现有机无机杂化离子半导体薄膜的可控制备，并</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652350&amp;idx=2&amp;sn=e9bde59c6ac9f87fef248f4c4a008995&amp;chksm=97870cd30002679d6d29d0a1d77beb2d8de926b32d1089307e867092233ed744c674abc059de&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 30 Dec 2025 13:51:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[清华&amp;港科大提出MVInverse：前馈式多视角逆向渲染，数秒内分解材质！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuvbfDAokdlb0ibicWtuvjpbUicq3lm0wvkiabBlLSrl8ZSlEzpwRu2TJyZeElAxRTxM7DibsFAbOotpBA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在3D内容创作、增强现实和机器人技术等领域，从2D图像中精确地恢复物体的三维几何、材质与场景光照——即逆向渲染（Inverse Rendering）——是一项至关重要的基础技术。然而，现有的方法往往陷</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652147&amp;idx=1&amp;sn=d456d25fea62fdf58a2209ded9475ee3&amp;chksm=9746e99a4336dd14d400c9c226b7ab2e8fca77cb4d0609e41a446372083a82dd520ddbac920d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 29 Dec 2025 09:05:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[浸大&amp;腾讯新作Streamo：统一决策与生成，破解流式视频理解难题，性能全面SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuvbfDAokdlb0ibicWtuvjpbUTRWEyzAT6fDeeVQtn34qtYpsYiaYIQ61xLhNQ7S3oLWyZ7CTm7O9bvg/640?wxtype=jpeg&amp;wxfrom=0"/><p>当我们在谈论视频大模型时，我们通常想到的是它能对一段已经录制好的、完整的视频进行总结、问答或打标签。但如果视频是实时、连续不断的直播流呢？传统的视频大模型往往会“傻眼”，因为它们的设计初衷是“事后诸葛</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652133&amp;idx=1&amp;sn=b62d9a91dd3f05ed828dbe537e69351e&amp;chksm=9764ee4ec8e7dd8d100711ad0a313b02ea8354b86fe40f2ad5153f4ed07b51456206ca703f4c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 28 Dec 2025 22:56:36 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Overleaf也能Vibe氛围写作了？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuvbfDAokdlb0ibicWtuvjpbUE2z7Xx2RvVuOiclK0aIWX3mFsGIibIIRv0rwIrRxSQIX1AgiahJicDAvDA/640?wxtype=jpeg&amp;wxfrom=0"/><p>文智云助手是一款Chrome/Edge浏览器插件，深度集成到Overleaf，提供从氛围Vibe氛围写作、边写边译、自动续写的全流程智能辅助。我测试了以下几个使用场景，发现效果非常好。场景1【氛围写作</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652112&amp;idx=1&amp;sn=da5b43df45e17d409b3295f48d6fd240&amp;chksm=9717b9ac5c4fee8c60ebd1a851c45d3a2c08a3e1ef9fab5de05695b673960ba602fe6718fdaa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 27 Dec 2025 08:38:54 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[KAIST新研究DiTracker：Video DiT的新妙用，更鲁棒的点跟踪]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsHD9F0pLvDdx6OtFwW0iakGWCov7NR0ibSpmXibn50znyArRoMcelhVXV4kVH6OXqBwa7vyGeoz5iaXQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>机构：KAIST AI, Google DeepMind论文地址：https://arxiv.org/abs/2512.20606代码仓库：https://github.com/cvlab-kaist</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651937&amp;idx=1&amp;sn=1630636eb418edddc8a5109e429f538d&amp;chksm=97390ed69247b1287db135b25b98a1f9124e12efb7b72948432112ba62f219e8eb000dac458c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 26 Dec 2025 08:49:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[107倍加速生成！南洋理工&amp;Meta AI等提出HiStream：让高清视频生成摆脱“算力噩梦”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsHD9F0pLvDdx6OtFwW0iakGtNCtIIq5zOjKiaJZ4j5tegdv9f9ibt7GiagtxlhWiba5LmIOYhRxicIgZEw/640?wxtype=jpeg&amp;wxfrom=0"/><p>虽然现在文生视频、图生视频的模型层出不穷，但想要生成真正“高清”（比如1080p）而且“长一点”的视频，实在是太——慢——了！这背后的元凶，就是视频扩散模型那高昂的计算成本，尤其是当时间和空间维度一上</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651913&amp;idx=1&amp;sn=d68b7dd53e49fe6900860fdb5cf55342&amp;chksm=97b4ebe64d0a4b3a8d1dfd9affa7d025072a55590dc3cb8e4cfb4a645f5a033ba7a12d423f64&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 25 Dec 2025 22:21:46 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[UCSD&amp;Insta360等发布DAP：全景图单目尺度深度估计基座模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuMw0GVbRr4NKcSJatOB1zBhPSlxoPH1nAPtfvbC0OpplpTKaRshs1ib8ItZC9dBjcqj0PEHfD7ruQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自 影石Insta360、加利福尼亚大学圣迭戈分校、武汉大学 和 加利福尼亚大学默塞德分校 的研究者们联手，推出了一篇名为 《Depth Any Panoramas: A Foundation</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651618&amp;idx=1&amp;sn=f4c64c6e9be472626b76fd97f3573b2e&amp;chksm=9790ecae6c9d25efe3ce415715099fb2a27ce272f34fa31934ec6609382a1e3a4195a0ffda40&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 24 Dec 2025 08:15:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[200多篇论文深度梳理！首个通用端到端自动驾驶（GE2E）综述发布，揭秘三大范式演进之路]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuMw0GVbRr4NKcSJatOB1zBr2rhZO2waM28C6bpFOF69z6Lmu3HdZgeMoIGMfsy64HwjslZDcGSZQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>自动驾驶的终极目标是构建一个能够无缝将原始传感器输入映射为驾驶决策的集成系统。为了克服传统模块化管道信息丢失和误差累积的局限性，也是为了追求更接近人类的驾驶智能，学术界和工业界正经历一场从模块化向数据</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651569&amp;idx=1&amp;sn=c9de49be2440c9ea12d49606ac3694b1&amp;chksm=9756c5bb03904c07045cf3345a22d233cf2140b666c846689edbfe5c4a34fccf27a92ddf9b3c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 23 Dec 2025 18:36:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦、MSRA等提出FlashPortrait：推理速度飙升6倍，高质量“无限长”人像动画成为现实]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs0oUQFOp3NbuGoq4BerhQZfx4vrib6JNw9G589liaDk5hZHLIhQqm6P9w4euicr1jich9QkIgWZ4It6g/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有趣的新工作，它来自复旦大学、微软亚洲研究院（MSRA）、西安交通大学、腾讯和阿里通义实验室的学者们。这篇名为 FlashPortrait 的论文，真正解决了当前AI人像</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651212&amp;idx=1&amp;sn=e98e64468f53a00b45b2b84763da3813&amp;chksm=97cfc2a84093bd705b2d05366254eb14fd2ee7344b4b993046c6aa3bb2bceae04a404342888b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 22 Dec 2025 22:05:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[研究助理教授 | 博后 | 香港中文大学影像及介入放射学系招募]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs0oUQFOp3NbuGoq4BerhQZDkPKzQdFJU2Ruf7kYXAb42XO55md3q0KZVichoIK7ezV6kJ3qmddjHw/300?wxtype=jpeg&amp;wxfrom=0"/><p>岗位招聘：研究助理教授 (Research Assistant Professor) 与博士后研究员（香港中文大学影像及介入放射学系）入职时间香港中文大学影像及介入放射学系现开始招收计划2026-20</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651212&amp;idx=2&amp;sn=1509e6d3cc9449685da7667fdb2c1ecb&amp;chksm=97b689e543d16122389c63b85ac2deded7f8a5ae7482d8154c7482347fe38d1b3ce63d494431&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 22 Dec 2025 22:05:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[豆瓣9.5，荣登京东IT新书榜第1名，这本书为啥这么受欢迎？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtHy8mUu6MUib23vXq3o2v6MP03lE3xbtBERO2eibbWeAsiaGtzPsInUSqNrBfHPy2PVLutHnDa0QtFw/640?wxtype=jpeg&amp;wxfrom=0"/><p>关注我们丨文末赠书在人工智能飞速发展的今天，有一本书被全球学者奉为经典，称为机器学习的“圣经”。它就是克里斯托弗·毕晓普的《模式识别与机器学习》，简称“PRML”。该书在豆瓣保持着9.5分的惊人高分，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651076&amp;idx=1&amp;sn=403c1b5cd2b61c9ead9e5947398ff640&amp;chksm=9739390676c9da2575dc2f0e05a5f5fecd4799ccd7f15939e6544f58868731f82d12c15117a0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 21 Dec 2025 08:15:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[大胆推进，小心修补！清华&amp;华为推出 X-Slim：极致压榨缓存冗余，扩散模型推理最高提升 4.97 倍！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTugY56ZahiaPKPz7zmN7YoDRiaibRp36bzgpyzcJohgwJibAv9icJoN9tms3Fb8MDCp6btenKV9r05Kfxw/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近介绍了一些针对扩散模型加速的工作，发现很多朋友感兴趣。今天分享的是来自 清华大学 和 华为 的研究者们联手提出的一种名为 X-Slim (eXtreme-Slimming Caching) 的方法</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651039&amp;idx=1&amp;sn=ad3a4b84006114a9fd72d3756f7b7581&amp;chksm=976c5a13848f56e28d5a89a4bd7e5e6b22921cd7819871c154bedf5796802b41b58aa689a95a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 20 Dec 2025 15:05:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | Styletailor：首个集成设计/推荐/试衣/打分的负反馈多智能体框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTugY56ZahiaPKPz7zmN7YoDRnBicGLzypbnxdN7HvHDpUENB0FuKHk2I93ibLic1icYlHj4M8qibVvrpmUg/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自人工智能与数字经济广东省属实验室（深圳）、清华大学、新加坡国立大学、Bytedance Seed、、杭州电子科技大学和香港大学的研究者们联手，推出了一项名为 StyleTailor 的新工作</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650942&amp;idx=1&amp;sn=194dcc5d163529e694dd08e360e98c55&amp;chksm=97ccc7c52db8a8d0596fed38fb88d8d8cefed8cd72d14a656031855b4c0e703016d9afe096e5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 19 Dec 2025 12:41:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TIP 2025 | 基于傅里叶解耦的联合暗光增强和去模糊算法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTugY56ZahiaPKPz7zmN7YoDR73R14dwF6hgxcD2NQELZKY1oUiaOQI8doZscfpNT5xsgUL5hffnHOfA/300?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍论文 Fourier-based Decoupling Network for Joint Low-Light Image Enhancement and Deblurring ，已被图像处理</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650942&amp;idx=2&amp;sn=a52d5e5f5da84263f9f174ee2d076660&amp;chksm=97d7ea00c90fe9284df6877ebd4a3b3e7b98cacaf23f1536b8de9a0efda0b1b05cdf22881652&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 19 Dec 2025 12:41:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[【IEEE计算机会议合集】2025年12月-2026年4月热门EI会议推荐，算法、人工智能、大数据、计算机视觉多主题征稿！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTthjtacCXfUMiarDB3hvyoTFaye2nt0wCyGs4OLLBrFQvUZQGMWbxxKn3I1gem9zxLsnGEQJTulnTQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>IEEE权威出版·IEEE/IET/CAA Fellow大咖云集·EI Compendex, Scopus稳定检索1、2025年电力系统、智能电网和人工智能国际会议（PSGAI 2025）会议官网：w</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650708&amp;idx=1&amp;sn=a2956d9cd7c7d829176f8a2d6490604d&amp;chksm=97f2a114933efde61f119a1f7928ee0e87269ddd8fdb34fb9a98796b3473953486ab87648885&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 18 Dec 2025 10:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ETH Zurich提出轻量级点云模型LitePT：参数少3.6倍，速度快2倍，性能超越SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTthjtacCXfUMiarDB3hvyoTFAbg4GDWU3bcT6HAb3b7qSFZ44hUNgibLUGwYibWme4CeRQbegmdlWZPQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>这是一篇新出的3D点云领域非常有意义的论文——《LitePT: Lighter Yet Stronger Point Transformer》。顾名思义，LitePT意为“更轻量但更强大的点云Tran</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650708&amp;idx=2&amp;sn=55f019a814dac3c398641a02ea98b110&amp;chksm=97ab0616440c7b3ada6a5a8597975238352b99e7d0982393a05434a0fd03e569d8862d26e10f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 18 Dec 2025 10:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[MindDrive：华科&amp;小米汽车联合发布，基于在线强化学习自动驾驶 VLA 模型，闭环仿真新SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtgGRS4mFBhYYsaF6WsvziapQTicQjPAKOBicmp7QLQ4zHvHSFonsArB6TibiaDYl2U35muprVnhiaVFI5g/640?wxtype=jpeg&amp;wxfrom=0"/><p>当自动驾驶迈入 “端到端” 新时代，视觉 - 语言 - 动作（VLA）范式成为打破传统模块化瓶颈的关键，但模仿学习带来的分布偏移、因果混淆等问题，始终制约着闭环驾驶的安全性与鲁棒性。如今，华中科技大学</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650537&amp;idx=1&amp;sn=c0393a7333109df41b5ddbc4fb0e5ee9&amp;chksm=977208e0e80d92990061f8dad31ffa16bfd5e2a8c7433cd3632b6cc7fa2c69c3aa87a0b58208&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 17 Dec 2025 11:26:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[【VIVO招聘】影像算法研究部 | AIGC&amp;空间智能方向（正式员工/实习生）]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtgGRS4mFBhYYsaF6WsvziapBNjcT7GJtrtt6WIJ1hqVTSicD6lUN1fQk9kB3kibSibmAeZkMSDX2U5bQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>公司介绍VIVO影像算法研究部作为公司核心研发团队，专注于旗舰手机影像算法的前沿创新。我们与高通、联发科等顶级芯片厂商深度合作，定制专属算法芯片，致力于提升照片画质、影调、颜色及光学表现等核心影像体验</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650537&amp;idx=2&amp;sn=b97e4d1f79fb44e1ee0be908852b311f&amp;chksm=97fbcdde44f4a81a8ad7100b3e7aeffd383be1609c4878797b9df9130efe89dea57ed2384667&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 17 Dec 2025 11:26:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICASSP 2026出分！顶会4大高频问题最全解答（附春节前截稿CCF A-B会议）]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtgGRS4mFBhYYsaF6WsvziapaNogvAojaATj4Yn1gBibhiakZjOcSV4OVlyJbOhMCtejh8YTZlArIibrw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今年 ICASSP 2026 作者回复（rebuttal）截止日期是2025年12月22日，大家今年对 rebuttal 有点摸不着头脑？我整理了大家问的比较高频的 4 大问题，给大家一一解答：第一：</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650499&amp;idx=1&amp;sn=a550224edd8b35c5c7f69c2aaa99cbe6&amp;chksm=97e7102f73cab0ec369e6faff9422b9dd6c5a549d9335bb0f9a2219e614fe6218620463c8b67&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 16 Dec 2025 21:02:41 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Adobe联合NYU提出iREPA：3 行代码，重塑生成模型“表征对齐”，空间结构才是关键！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuKHay0iciceNndYtCmrxnYXW0hoMz6E6cA90hPf1PnIABcJpk7hD8cLM4A6j1ttB9wwWRVzD5xcW2g/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：What matters for Representation Alignment: Global Information or Spatial Structure?论文作者：Jaskira</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650499&amp;idx=2&amp;sn=68bbc0680d7e887b9a9e5d589e4948b1&amp;chksm=973cdcb02f118af1110668d1064f8f4f9e065f7c33b7d0f281dae8d73fbb630e5fbe9ae734c1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 16 Dec 2025 21:02:41 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[仅用视频训练，3D视觉自监督模型终获成功：E-RayZer]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuKHay0iciceNndYtCmrxnYXWA8arO5XBmZa0She6vZibPMqLsViaotdkDRlPYC63fDonBsZqMSj1iastw/640?wxtype=jpeg&amp;wxfrom=0"/><p>介绍一篇今天刚挂在arXiv上的重磅论文——E-RayZer，它可能将成为3D视觉领域的一个重要里程碑。我们知道，在大语言模型（LLM）和2D视觉领域，自监督预训练（Self-supervised P</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650297&amp;idx=1&amp;sn=750aacbc6c48fcdabfaa3880005b19b6&amp;chksm=97e6ee42e14f87ffac95a9feb9de5987316eed4d48a9a09eca5172a17fd64f9b44a7182d8efe&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 15 Dec 2025 23:54:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | SuperCLIP：对比学习加上分类任务，使CLIP更强了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtkH6u0HwWfiabuqWyH0HINSVWLDcCJib6Ep70KI9eNo8xbOa2R8MCRvcsJkXF2n5LxXrmkgmx4MmCw/640?wxtype=jpeg&amp;wxfrom=0"/><p>自诞生以来，CLIP 就凭借其强大的零样本学习和跨模态理解能力，成为了视觉语言模型领域的基石。然而，这位“优等生”也有自己的烦恼：它擅长把握图像和文本的“全局大意”，却常常在“细枝末节”上犯迷糊。比如</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650154&amp;idx=1&amp;sn=8fc4960956e8303d94464ac4a346a3d6&amp;chksm=97f53f755a16b568696f15b218a369e6dde1dbcb10e5cee6ef298977d1dfbb521f0a4160ad5f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 14 Dec 2025 23:16:29 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[SpatialDreamer：通过主动心理想象，激发MLLMs空间推理能力]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtXaPcZBTAooj2SXDeM54JXxWNhe2ZVPrOFM99vUaicEwgX8vrfcq14uJKJIQXHZ8toAficKo2V83TA/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天想和大家聊一篇非常有意思的新工作，来自MBZUAI和中山大学的研究者们提出的SpatialDreamer。虽然现在的多模态大模型（MLLMs）看图说话能力一流，但和人类相比，在需要“脑补”</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650103&amp;idx=1&amp;sn=638b7b4d74b8a48195421dad67f7729b&amp;chksm=976222417907aadc887c345af896b509da9db5248a11e00eb577003d335c3ca9d7fbc88b27bf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 13 Dec 2025 20:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[12月即将截稿的5个CCF-B类会议，录用率超高！速看！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu9M7sjur1DwMEJwBPgdOJTCIic5Cf3gw0FaTfZ3TKEzGxkGDnN3oWReibPGQd16LvDFzg7WBI89x0g/300?wxtype=jpeg&amp;wxfrom=0"/><p>12月快过去一半了，大家投稿情况怎么样啦，现在为大家精心整理了部分本月截稿的CCF-B类会议！录用率比较高，大家抓紧时间冲呀！ICAPS 2026截稿日期： 2025年12月14日会议简介： 自动化规</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650103&amp;idx=2&amp;sn=632c5e3466d5958940ac9e1415bb908f&amp;chksm=97a6c75cd2e70cbf7f12101ec31f6b1ec31867e8fbaa794248493f307d3601efc1d7989a825e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 13 Dec 2025 20:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[WorldLens: 真正评估“世界模型”的基准，终于来了]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtl5r3Qx5VGatFzJDSFMAciakPXTt63xToOOUtNS7OF5lbnqbD5Fiam9lZWedN3ntjiaGNxlTeAkQy6g/640?wxtype=jpeg&amp;wxfrom=0"/><p>近年来，生成式世界模型 (World Models) 迅速成为自动驾驶与具身智能领域的核心方向。从文本生成驾驶视频，到可控 4D 场景生成，模型已经能够生成视觉上极具真实感的驾驶画面。但一个关键问题长</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649988&amp;idx=1&amp;sn=3571c12e2d2c5548bc86b29d03785f35&amp;chksm=97bbe0e8fd615343f5e3baf8eec00e1f85bdcbea90cbfa6f87e7426b6658ce1bfc68fa9347e2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 12 Dec 2025 13:22:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[清华等提出FacePhys：极低计算需求（3.6MB内存、9.46ms/帧），超高精度的心率检测模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtl5r3Qx5VGatFzJDSFMAcia9Jib3VibqIfuSfXall1RLQFYvvWLj2uHAQgibWaGJDiaOH1BcRvZCZeNEA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天介绍一篇新出的生理信号检测领域的新工作，题为《FacePhys: State of the Heart Learning》，这篇论文聚焦在如何通过摄像头，更精准、更高效地测量我们的生命体征，特别是</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649988&amp;idx=2&amp;sn=3514f361611a7c0369fa6f023af1664b&amp;chksm=9776b2e6760089abd33294d301033b6e2e81e49e6452b27b6256308b482ce7045be5c4f396bd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 12 Dec 2025 13:22:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2026 哈工大&amp;清华等提出DiTFuse：迈向统一、可控的图像融合新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu2ypW2psy7pxK0ZaavMCMqWszX5M28XIUibvYTBtFOgaxW6EibxdFb2TG3DtBfEtoiaFTPAnuaKJRxQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇新出的被顶刊TPAMI录用的工作，它来自哈尔滨工业大学、清华大学和武汉大学等机构的研究者们。这篇论文提出了一个名为 DiTFuse 的框架，致力于解决图像融合领域长期以来存在</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649662&amp;idx=1&amp;sn=bc1ee625ca2b52ff968d1e1d779a15d7&amp;chksm=97f51a44bb0b5813e8b1da1b6130c32fa332b828658a918939d2975b7a9d569613f49a68811d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 11 Dec 2025 12:54:12 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[BulletTime：解耦时空控制，斯坦福与ETH Zurich重新定义4D视频生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu2ypW2psy7pxK0ZaavMCMq07mHPUm7ate4xPEVhicYFVrDibdtDDGxxYiaPdtMBpxBugaia6xlF7VibxA/300?wxtype=jpeg&amp;wxfrom=0"/><p>相信看过电影《黑客帝国》的朋友，都对其中主角尼奥躲避子弹的经典慢镜头记忆犹新。镜头围绕着几乎静止的主角高速旋转，展现出无与伦比的视觉冲击力，这就是著名的“子弹时间”（Bullet Time）特效。长久</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649662&amp;idx=2&amp;sn=3231fb180a236654ea031526249e668a&amp;chksm=974a899ae1c5c35d69599a051c9b24a90ec35f4f8e99cb3df5d68b07aab4c1f9091530a92e36&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 11 Dec 2025 12:54:12 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[EMMA：华为发布统一多模态新架构，4B模型实现理解、生成、编辑功能的齐头并进]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0HI6FKKK2DQpC1waXBNWXsM189ibRbEGHLJ3zoS7iaS97RmUzkVojnEHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天，我们来聊聊多模态领域的新工作：EMMA,全称是“Efficient Multimodal Understanding, Generation, and Editing with a Unifie</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649426&amp;idx=1&amp;sn=98a97872d0fd5425062e18232a5ee618&amp;chksm=97c8d5c295ac1992f75892254c80a88ac981cc53a32aac55b333f6f3d6be3d250217df4b6a44&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 10 Dec 2025 12:25:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[与贤同行 创领未来｜合肥工业大学人工智能创新学院诚聘天下英才]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0aHAQ3ibW36cicJzwyWoqm49pLVOwCGYWclzofxed6MTzjzib0A4MNqOUg/300?wxtype=jpeg&amp;wxfrom=0"/><p></p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649426&amp;idx=2&amp;sn=e14eecdbfc34a3302a1d959be0d0b6d0&amp;chksm=97ce3eaaa84e545a09effe055d5c6edb9a988a8a18a0095dadf72e0d16284e6ca5b2182b8b9d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 10 Dec 2025 12:25:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[投稿需交100美金的CCF-A！IJCAI 2026倒计时一个月啦！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0IF6eLtyoFjPZpcNGVYOo4zyWSpvnuFMqVvDU9DncLDh8q5KGBwic3tg/300?wxtype=jpeg&amp;wxfrom=0"/><p>IJCAI（全称 International Joint Conference on Artificial Intelligence）是人工智能领域公认的国际顶级学术会议之一，亦位列中国计算机学会CC</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649426&amp;idx=3&amp;sn=ec18a453d432b66a553d3b94bc305477&amp;chksm=9704c699dddb7472faed964b0d5642ed954ea539fafa192e9ae4ba97130ed022dbeb98adf9f6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 10 Dec 2025 12:25:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[最前沿的强化学习课程来了！斯坦福CS224R深度强化学习全套课程开放！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0Sic7v8IfxLfAwCUylTclpQ5gf1FZu6UF7T5Ku2kWupnYzicWMGpbXoNQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近AI圈风起云涌，大模型技术日新月异。但不知道你有没有想过，让ChatGPT这类大模型能够如此“善解人意”的背后，除了海量数据的预训练，一项关键技术功不可没——那就是强化学习，特别是基于人类反馈的强</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649236&amp;idx=1&amp;sn=b96b34a0839e94623e63f09e6c8ba99a&amp;chksm=97ad6a3e4d7c5e6a8a74f15699039189a2fdfb2d6d7e912517704e765392667754806151d803&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 09 Dec 2025 14:01:56 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[架构解耦为什么对统一多模态模型有效？港中文联合美团提出AIA，揭示其真正的奥秘！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0oUCNUcIicfS0jZS5PxUWoiaT6X9hCPjINw81AbtozDZsxaYoDYXaCVAA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近来自港中文MMLab、美团等机构，对当前火热的统一多模态模型（Unified Multimodal Models, UMMs）提出了一个“反潮流”的观点。当我们希望一个AI模型既能“看懂图说话”（</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649236&amp;idx=2&amp;sn=6f86a3ee12f7a5045d030cfe25ee4aea&amp;chksm=97c59bbb6dd14bc1e00b668804e6911acdaecc60cbb069bf4f61f2ee4ab8eee73270de979af6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 09 Dec 2025 14:01:56 +0800</pubDate>
    </item>
  </channel>
</rss>