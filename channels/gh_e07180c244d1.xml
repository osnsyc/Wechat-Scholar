<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM6aYkwkiboia6lA9D7ANy49WBe9icxn5NQqJjvn4Pyntzvfw/132</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[仅用视频训练，3D视觉自监督模型终获成功：E-RayZer]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuKHay0iciceNndYtCmrxnYXWA8arO5XBmZa0She6vZibPMqLsViaotdkDRlPYC63fDonBsZqMSj1iastw/640?wxtype=jpeg&amp;wxfrom=0"/><p>介绍一篇今天刚挂在arXiv上的重磅论文——E-RayZer，它可能将成为3D视觉领域的一个重要里程碑。我们知道，在大语言模型（LLM）和2D视觉领域，自监督预训练（Self-supervised P</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650297&amp;idx=1&amp;sn=750aacbc6c48fcdabfaa3880005b19b6&amp;chksm=97e6ee42e14f87ffac95a9feb9de5987316eed4d48a9a09eca5172a17fd64f9b44a7182d8efe&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 15 Dec 2025 23:54:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | SuperCLIP：对比学习加上分类任务，使CLIP更强了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtkH6u0HwWfiabuqWyH0HINSVWLDcCJib6Ep70KI9eNo8xbOa2R8MCRvcsJkXF2n5LxXrmkgmx4MmCw/640?wxtype=jpeg&amp;wxfrom=0"/><p>自诞生以来，CLIP 就凭借其强大的零样本学习和跨模态理解能力，成为了视觉语言模型领域的基石。然而，这位“优等生”也有自己的烦恼：它擅长把握图像和文本的“全局大意”，却常常在“细枝末节”上犯迷糊。比如</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650154&amp;idx=1&amp;sn=8fc4960956e8303d94464ac4a346a3d6&amp;chksm=97f53f755a16b568696f15b218a369e6dde1dbcb10e5cee6ef298977d1dfbb521f0a4160ad5f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 14 Dec 2025 23:16:29 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[SpatialDreamer：通过主动心理想象，激发MLLMs空间推理能力]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtXaPcZBTAooj2SXDeM54JXxWNhe2ZVPrOFM99vUaicEwgX8vrfcq14uJKJIQXHZ8toAficKo2V83TA/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天想和大家聊一篇非常有意思的新工作，来自MBZUAI和中山大学的研究者们提出的SpatialDreamer。虽然现在的多模态大模型（MLLMs）看图说话能力一流，但和人类相比，在需要“脑补”</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650103&amp;idx=1&amp;sn=638b7b4d74b8a48195421dad67f7729b&amp;chksm=976222417907aadc887c345af896b509da9db5248a11e00eb577003d335c3ca9d7fbc88b27bf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 13 Dec 2025 20:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[12月即将截稿的5个CCF-B类会议，录用率超高！速看！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu9M7sjur1DwMEJwBPgdOJTCIic5Cf3gw0FaTfZ3TKEzGxkGDnN3oWReibPGQd16LvDFzg7WBI89x0g/300?wxtype=jpeg&amp;wxfrom=0"/><p>12月快过去一半了，大家投稿情况怎么样啦，现在为大家精心整理了部分本月截稿的CCF-B类会议！录用率比较高，大家抓紧时间冲呀！ICAPS 2026截稿日期： 2025年12月14日会议简介： 自动化规</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650103&amp;idx=2&amp;sn=632c5e3466d5958940ac9e1415bb908f&amp;chksm=97a6c75cd2e70cbf7f12101ec31f6b1ec31867e8fbaa794248493f307d3601efc1d7989a825e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 13 Dec 2025 20:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[WorldLens: 真正评估“世界模型”的基准，终于来了]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtl5r3Qx5VGatFzJDSFMAciakPXTt63xToOOUtNS7OF5lbnqbD5Fiam9lZWedN3ntjiaGNxlTeAkQy6g/640?wxtype=jpeg&amp;wxfrom=0"/><p>近年来，生成式世界模型 (World Models) 迅速成为自动驾驶与具身智能领域的核心方向。从文本生成驾驶视频，到可控 4D 场景生成，模型已经能够生成视觉上极具真实感的驾驶画面。但一个关键问题长</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649988&amp;idx=1&amp;sn=3571c12e2d2c5548bc86b29d03785f35&amp;chksm=97bbe0e8fd615343f5e3baf8eec00e1f85bdcbea90cbfa6f87e7426b6658ce1bfc68fa9347e2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 12 Dec 2025 13:22:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[清华等提出FacePhys：极低计算需求（3.6MB内存、9.46ms/帧），超高精度的心率检测模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtl5r3Qx5VGatFzJDSFMAcia9Jib3VibqIfuSfXall1RLQFYvvWLj2uHAQgibWaGJDiaOH1BcRvZCZeNEA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天介绍一篇新出的生理信号检测领域的新工作，题为《FacePhys: State of the Heart Learning》，这篇论文聚焦在如何通过摄像头，更精准、更高效地测量我们的生命体征，特别是</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649988&amp;idx=2&amp;sn=3514f361611a7c0369fa6f023af1664b&amp;chksm=9776b2e6760089abd33294d301033b6e2e81e49e6452b27b6256308b482ce7045be5c4f396bd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 12 Dec 2025 13:22:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2026 哈工大&amp;清华等提出DiTFuse：迈向统一、可控的图像融合新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu2ypW2psy7pxK0ZaavMCMqWszX5M28XIUibvYTBtFOgaxW6EibxdFb2TG3DtBfEtoiaFTPAnuaKJRxQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇新出的被顶刊TPAMI录用的工作，它来自哈尔滨工业大学、清华大学和武汉大学等机构的研究者们。这篇论文提出了一个名为 DiTFuse 的框架，致力于解决图像融合领域长期以来存在</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649662&amp;idx=1&amp;sn=bc1ee625ca2b52ff968d1e1d779a15d7&amp;chksm=97f51a44bb0b5813e8b1da1b6130c32fa332b828658a918939d2975b7a9d569613f49a68811d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 11 Dec 2025 12:54:12 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[BulletTime：解耦时空控制，斯坦福与ETH Zurich重新定义4D视频生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu2ypW2psy7pxK0ZaavMCMq07mHPUm7ate4xPEVhicYFVrDibdtDDGxxYiaPdtMBpxBugaia6xlF7VibxA/300?wxtype=jpeg&amp;wxfrom=0"/><p>相信看过电影《黑客帝国》的朋友，都对其中主角尼奥躲避子弹的经典慢镜头记忆犹新。镜头围绕着几乎静止的主角高速旋转，展现出无与伦比的视觉冲击力，这就是著名的“子弹时间”（Bullet Time）特效。长久</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649662&amp;idx=2&amp;sn=3231fb180a236654ea031526249e668a&amp;chksm=974a899ae1c5c35d69599a051c9b24a90ec35f4f8e99cb3df5d68b07aab4c1f9091530a92e36&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 11 Dec 2025 12:54:12 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[EMMA：华为发布统一多模态新架构，4B模型实现理解、生成、编辑功能的齐头并进]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0HI6FKKK2DQpC1waXBNWXsM189ibRbEGHLJ3zoS7iaS97RmUzkVojnEHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天，我们来聊聊多模态领域的新工作：EMMA,全称是“Efficient Multimodal Understanding, Generation, and Editing with a Unifie</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649426&amp;idx=1&amp;sn=98a97872d0fd5425062e18232a5ee618&amp;chksm=97c8d5c295ac1992f75892254c80a88ac981cc53a32aac55b333f6f3d6be3d250217df4b6a44&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 10 Dec 2025 12:25:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[与贤同行 创领未来｜合肥工业大学人工智能创新学院诚聘天下英才]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0aHAQ3ibW36cicJzwyWoqm49pLVOwCGYWclzofxed6MTzjzib0A4MNqOUg/300?wxtype=jpeg&amp;wxfrom=0"/><p></p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649426&amp;idx=2&amp;sn=e14eecdbfc34a3302a1d959be0d0b6d0&amp;chksm=97ce3eaaa84e545a09effe055d5c6edb9a988a8a18a0095dadf72e0d16284e6ca5b2182b8b9d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 10 Dec 2025 12:25:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[投稿需交100美金的CCF-A！IJCAI 2026倒计时一个月啦！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0IF6eLtyoFjPZpcNGVYOo4zyWSpvnuFMqVvDU9DncLDh8q5KGBwic3tg/300?wxtype=jpeg&amp;wxfrom=0"/><p>IJCAI（全称 International Joint Conference on Artificial Intelligence）是人工智能领域公认的国际顶级学术会议之一，亦位列中国计算机学会CC</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649426&amp;idx=3&amp;sn=ec18a453d432b66a553d3b94bc305477&amp;chksm=9704c699dddb7472faed964b0d5642ed954ea539fafa192e9ae4ba97130ed022dbeb98adf9f6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 10 Dec 2025 12:25:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[最前沿的强化学习课程来了！斯坦福CS224R深度强化学习全套课程开放！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0Sic7v8IfxLfAwCUylTclpQ5gf1FZu6UF7T5Ku2kWupnYzicWMGpbXoNQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近AI圈风起云涌，大模型技术日新月异。但不知道你有没有想过，让ChatGPT这类大模型能够如此“善解人意”的背后，除了海量数据的预训练，一项关键技术功不可没——那就是强化学习，特别是基于人类反馈的强</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649236&amp;idx=1&amp;sn=b96b34a0839e94623e63f09e6c8ba99a&amp;chksm=97ad6a3e4d7c5e6a8a74f15699039189a2fdfb2d6d7e912517704e765392667754806151d803&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 09 Dec 2025 14:01:56 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[架构解耦为什么对统一多模态模型有效？港中文联合美团提出AIA，揭示其真正的奥秘！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0oUCNUcIicfS0jZS5PxUWoiaT6X9hCPjINw81AbtozDZsxaYoDYXaCVAA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近来自港中文MMLab、美团等机构，对当前火热的统一多模态模型（Unified Multimodal Models, UMMs）提出了一个“反潮流”的观点。当我们希望一个AI模型既能“看懂图说话”（</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649236&amp;idx=2&amp;sn=6f86a3ee12f7a5045d030cfe25ee4aea&amp;chksm=97c59bbb6dd14bc1e00b668804e6911acdaecc60cbb069bf4f61f2ee4ab8eee73270de979af6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 09 Dec 2025 14:01:56 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICML2026！CCF-A！截稿仅剩45天，速看！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvib80yz1ntDuRVcI7Ayfuw4UibIY7d8w2A3GPwib5KMgC7h85NZ0vwVRg0kcGbeC8aZVgBNRmzzaQlA/300?wxtype=jpeg&amp;wxfrom=0"/><p>ICML 2026，即第43届国际机器学习大会，是机器学习与人工智能方向备受推崇的全球顶尖学术会议之一，与NeurIPS、ICLR并列被誉为该领域的“三大旗舰会议”。在中国计算机学会（CCF）的权威推</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649236&amp;idx=3&amp;sn=746f5d674c6caa7e2af1bea38ae68629&amp;chksm=97117b4153dcc574e99762d16220121b8d7f5952f0db415a5d2c3334d36e0df9f9a24cbd7180&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 09 Dec 2025 14:01:56 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[苏黎世联邦理工等提出LeAD-M3D：无需LiDAR，单目3D检测也能SOTA且实时！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsdE6icV8QYnIqOb5CsLt93R8JH1ZrIyjTo5Tt2V9SkSrI5SHEsVxuuhOBlWnmsibjNVKriaOVRicz7rg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection作者机构：DeepScena</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649182&amp;idx=1&amp;sn=5bd41ab4d4fe7e62d4864ca2ee356a89&amp;chksm=97f8a3a9bbac1bb9385fe01fc99d90d5f768698632f27c4b86d69041aaa4a3fe4a7582606238&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 08 Dec 2025 23:26:06 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[打破文本记忆局限！ViLoMem要记视觉关注点，显著提升多模态推理能力！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvib80yz1ntDuRVcI7Ayfuw4KBtVagz1Jg5xlxMGMx3uwBd10GJeZiaXFhIG5NAnXe3ZxXsNXR7M6xA/640?wxtype=jpeg&amp;wxfrom=0"/><p>你是否发现，现在的多模态大模型（MLLMs）虽然在单次回答上表现惊艳，但往往像个“健忘症患者”？它们处理每一个问题时都是从零开始（de novo），反复掉进同一个坑里，昨天犯过的视觉识别错误，今天换个</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649046&amp;idx=1&amp;sn=dd5bac6438021e3b1dd161357eab4cc5&amp;chksm=9747b6e7445a9e0b6888782353cc52e7479ce6ac091fd9297927b8168462daab49a34342c96e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 07 Dec 2025 22:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[突破视频重照明瓶颈！南洋理工、清华等提出Light-X：首个实现单目视频相机与光照联合控制的生成框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsia9tJcNGo1BIlmX5k0gUfYPQZALXjopdN5EMPF0KZDCU4HDv5Dia2eibKyI8K0ueXBFldiaDtmgfG7A/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文名称: Light-X: Generative 4D Video Rendering with Camera and Illumination Control作者: Tianqi Liu, Zha</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648997&amp;idx=1&amp;sn=9e924748e811a0e98f20a7917f051471&amp;chksm=97a8f56ca7008ffd274c122ab782de58fbcdcef74cb4d5ed6465be22c4bb94213f875d2aac8c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 06 Dec 2025 22:21:29 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[南理工\SUTD等提出Artemis：结构化视觉推理，MLLM感知策略学习新突破]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsia9tJcNGo1BIlmX5k0gUfYP4lIZFiajh2QWb0VFsm45oWkaucozcI776ibynFzQs5XDl5lJYCC4znw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：Artemis: Structured Visual Reasoning for Perception Policy Learning论文作者：Wei Tang, Yanpeng Sun,</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648990&amp;idx=1&amp;sn=b3ec88ca2acd8ec7ec87501d636d7682&amp;chksm=97f8d549d468cda9c39478384a22a83cd376d1829474b2638b45d7e265c8eb9b5e4f10cf2887&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 05 Dec 2025 18:41:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CCF-B类！人工智能顶会录用率近40%，截稿仅剩70天！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtHy8mUu6MUib23vXq3o2v6MNfglOqzr7xiahhrR4pk4icmMz6Mpicpfa7FyfgnvOIIa5M3hI7UwkjMwg/300?wxtype=jpeg&amp;wxfrom=0"/><p>知识表示与推理是人工智能中一条成熟而活跃的研究主线，其核心思想是：把对世界的认识整理成显式、陈述性的符号结构，让专用推理引擎按语义规则演算，从而把原本隐含的信息变成可用的知识。这套方法不仅为智能体、自</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648990&amp;idx=2&amp;sn=7475ede33ce44b9f4a58a869f331c7fc&amp;chksm=973e6637dc8f4f2c2e28283e5a6cf5af4e8ec6c9b05e9a94c66253d52d98bd0b6e4169a83e1c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 05 Dec 2025 18:41:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[RobustVGGT来了！不惧噪声，显著提升3D重建鲁棒性]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtHy8mUu6MUib23vXq3o2v6MaqMHy9xwR7e9iav7VzpJEpqmlILGINhfQrWhia51h6vlRlcMaGc94FJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Emergent Outlier View Rejection in Visual Geometry Grounded Transformers论文作者: Jisang Han, Sung</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648852&amp;idx=1&amp;sn=3d418d3062a10ec11571ee8cb20fedbd&amp;chksm=9777357152cd3904d20c9013700c1cead41504f25f54b3f6c6ef673ccd2307d53918b698ebdb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 04 Dec 2025 18:46:25 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CCF-B类！录用率32.5%，ICME2026截稿倒计时不足一个月！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvtUBBIAia0hr2ribSpibOgLZeNb1RqJPVXmzUxrS1LiaWsfGncflmCvPPBZOFCFvg12ic0wGkuOu4qvjw/300?wxtype=jpeg&amp;wxfrom=0"/><p>ICME 2026是多媒体领域的权威IEEE旗舰会议。自2000年起，该会议由IEEE下属四大学会共同主办，并获中国计算机学会推荐为B类学术会议，内容涵盖图像视频处理、多模态计算、多媒体通信与检索等重</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648852&amp;idx=2&amp;sn=1e704a8e499bda6c5343aacc8e1c7e04&amp;chksm=9702640f1b73717aaa3db893699d35f26bf8bdd439284700d91e9b70afec3ac91a51765869ab&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 04 Dec 2025 18:46:25 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[涨点神器！265个顶会上的开源即插即用模块汇总（附源码）！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvtUBBIAia0hr2ribSpibOgLZeVks2acdG2H2TQDlGC8QOX9Rt98kerTnZvUsrXQeMmLsicsFQzibrTGEA/640?wxtype=jpeg&amp;wxfrom=0"/><p>看了很多论文，创新点完全没头绪？或者好不容易找到个baseline，反复去看，去跑实验，却根本找不到破绽，模型无从改起……那我强烈推荐你用即插即用的模块进行缝合，即便小白，也能快速出文章！这些模块通常</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648497&amp;idx=1&amp;sn=a12541574296a81dce3dbefd23c35b05&amp;chksm=9741f092771d607633b4158a54c22c0d15874bcb3a3849ee7f1bd3ddb5705601f72d49b3c286&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 03 Dec 2025 12:31:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[突破规模瓶颈！JHU&amp;深大推出FoundationGait：步态理解自监督基础模型，大幅刷新下游步态识别与分析精度]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvhicG5KVuDbMKdyM6BZpZtRG6gFGic1LwDEQ65IUicJbVZUvHRQOYGrOGrQfjtazshheExrcRA0iaHicg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天，我们要聊的这篇论文是来自约翰霍普金斯大学（JHU）和深圳大学的最新研究成果——“Silhouette-based Gait Foundation Model”，直译过来是“基于轮廓的步态基础模型</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648497&amp;idx=2&amp;sn=9a819bf634f069f41b9d3ab937ef4f1b&amp;chksm=97046daf5fda55c80112b8319853e896e2afd25b3b225867e96038c898a536da3d3915e94277&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 03 Dec 2025 12:31:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[三篇论文看多模态LLMs“视觉文本压缩”，清华、DeepSeek、AI2“不约而同”的深度探索]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvtUBBIAia0hr2ribSpibOgLZeZB6aCdDX8026TW4N6RtvRDEj3W2kGnSKR9kvDI09Wahfew1f4xtn7A/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天我们来聊一个最近在AI圈子里激起不小浪花的新方向。大家都知道，让大语言模型（LLM）处理越来越长的文本，是通往更强通用人工智能路上的关键一环。无论是分析一本小说、一份财报，还是进行多轮对话</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648265&amp;idx=1&amp;sn=0ec214efd0c030504c78ae45dac8f31a&amp;chksm=97cf065fd0d21153386b97f8621adfd43e5263d5ba49f6f92b5c50144ad78b94691974fc383f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 02 Dec 2025 12:54:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[华科&amp;清华&amp;快手推出VGT：20倍加速，唤醒任意视觉语言模型的“隐藏绘画技能”！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvC3L5x1XvGia2SiaG6qsBy5Hk6XYjTuvDEKGEKaDaP8a0WpsXVCVL8qrNMoYKPo2kqYtvj5otVsYuA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自华中科技大学、清华大学和快手等机构的研究者们，带来了一篇非常有趣的工作，提出了一种名为VGT (Visual Generation Tuning)的新范式。简单来说，这是一种“视觉生成调优”</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648265&amp;idx=2&amp;sn=cca2d33133c5559d445f803c7b6e4c14&amp;chksm=97b7ca6e4964c1093c65244e3c91acf769e499be65c5ef7b90ee2f14e645303add2e0ef39b75&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 02 Dec 2025 12:54:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[【EI检索|计算机领域】2025年12月-2026年3月 IEEE 国际会议推荐]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsMTQLdCZg1u8A7Qv2W2SOZN82oNcpiaLkISuA6xAicQWxRunJws8FXnBOcJDDEx9PueKBNPh298vsg/640?wxtype=jpeg&amp;wxfrom=0"/><p>加★标 点关注不迷路IEEE出版| 江苏大学主办1、人工智能驱动图像处理与计算机视觉技术国际学术研讨会 (AIPCVT 2025)会议官网：www.ipcvt.com召开时间：2025年12月12-1</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647975&amp;idx=1&amp;sn=05b4fa2b9710d99f7615b0c4895ff093&amp;chksm=97a707f28309b6ca697dcdefc40923fbcb4a5c23e7a5dc722652474e773ce6a9f1903e6c81db&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 01 Dec 2025 08:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Mamba杀入3D追踪！新框架MambaTrack3D破解高时延难题，精度提升9.5%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTtNY34WOnAOibQmD2ezbY776bXLu9Wov3IhbL0JrfdVwlt1jBnjlE4zQSTwdRCOk8FiaFiau1yRKnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，状态空间模型（State Space Model, SSM），特别是Mamba架构，在序列数据处理领域掀起了一股热潮，大有与Transformer分庭抗礼之势。这股风也吹到了3D视觉领域。今天，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647975&amp;idx=2&amp;sn=a9754b0564aee3565a445ddeefbd78df&amp;chksm=97e0645dc7e2a262a2fa1638e141fe48f83d4aa22c5fb19c07a458ee953208e1d1fcc64e94e7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 01 Dec 2025 08:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[视频运动编辑新进展：谷歌MotionV2V发布，实现对任意物体、任意时间的精准运动控制]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8JNml6HqoVUcFuLIHicWNAj58MjKnmiaRfhibA7UxsEkyZzEtGISXOqtag/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: MotionV2V: Editing Motion in a Video作者: Ryan Burgert, Charles Herrmann, Forrester Cole, Michae</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647902&amp;idx=1&amp;sn=9139f307fc04cc6218fa04fce05574dd&amp;chksm=973e7900c2314ced763eab1fe3515e49850b86681fc855ed4ea0324395910062ba42f798c8d0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 30 Nov 2025 08:00:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Seeing without Pixels！DeepMind新研究CamFormer，相机“轨迹线索”解锁视频理解更多可能！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTOOlJM72Alu3F8PYnTAgMl87kU8jX2yBzT1NSeNQ5v8dylclxdzt5mSo2dLQFZrfe2CFZnnO4mA/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Seeing without Pixels: Perception from Camera Trajectories论文作者: Zihui Xue, Kristen Grauman, Di</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647865&amp;idx=1&amp;sn=0ddde4ee3cd32967dc483c7da24ef91b&amp;chksm=97fa629bfbf81a950fcc8cc1049fe90fae5a06f715c63c4d26ed0d504423214e8652bff8e621&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 29 Nov 2025 17:07:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | ByteDance推出MERIT：首个支持交错多条件查询的多语言语义检索数据集]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTOOlJM72Alu3F8PYnTAgMN5E0MCpntVvlSvnVIf1m7w8ecSjdCZiavAehEGY7jz9YcWDmpHvncxQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>主页：https://merit-2025.github.io/论文：https://arxiv.org/abs/2506.03144代码：https://github.com/weichow23/m</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647865&amp;idx=2&amp;sn=b05aa9f60cc417e17a0ff7eeaefe5591&amp;chksm=9728445c40b6b546a0d89e619d6272f0f9fc9026b53f7a37be86fabb9aa0ced0f188732a56cb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 29 Nov 2025 17:07:28 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[北大、南大&amp;华为提出DeCo：训练提速10倍，端到端像素扩散模型首次追平潜在扩散！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8WhyVdHGqibKy1jDEQl6egae3JH33jVaYDvdiaRUia0m7oyVsuiawMuxsRQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation论文作者: Zehong Ma, Long</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647763&amp;idx=1&amp;sn=8fd3fb388a0286cc8540bb708f3ca3e4&amp;chksm=97bf3e4fa8f322a6698912f81894ccbf50cd5998f80bda5caeecde0d0edac32d7b4fb081a757&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 28 Nov 2025 11:43:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ACL：DDL40天！查缺补漏篇含时间线!]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsMTQLdCZg1u8A7Qv2W2SOZtDkxouscfrPkmRPKclS8LpSHlO3Hjkej2mY4oreAII3jwNkGQHyGkg/300?wxtype=jpeg&amp;wxfrom=0"/><p>了解ACL 2026的关键时间节点对于你规划投稿至关重要。根据目前的搜索结果，ACL 2026采用了与其他顶会不同的ACL Rolling Review (ARR) 系统，其投稿和评审流程分为两个阶段</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647763&amp;idx=2&amp;sn=2520d17918619150e586a9ccb349046f&amp;chksm=97ba1751aed93cb758670cad56914b3a2fee7bb84828e3a7c572700fbee86e1223abfa76294b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 28 Nov 2025 11:43:03 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[iMontage：南洋理工联合StepFun新作，将视频模型“魔改”为万能多图生成器]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8y2XpBk09xYXlVniayyFzrOZGrqVjFsdxuhLnxxLFn5JrpiaAVq9thcVQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天早前分享了一篇百度用视频生成模型进行图像编辑的文章（把图像编辑看作视频生成，百度提出Video4Edit用1%数据追平SOTA），下面分享的这篇也很有意思，用视频生成模型进行多图生成和编辑。论文标</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647717&amp;idx=1&amp;sn=f9c59d264b60bdda09ff9ae1759430fd&amp;chksm=9721ffaa6eb8ff5d4c50e82ef684de477b1d6e396c9a6737e9ade82de4171521318642ab6dbc&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 27 Nov 2025 13:20:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[下一代多模态智能的基石：浙江大学万字长文综述，构建从诊断到治理的LVLM数据生态]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY86f2KnUyM4ibXxS3wNaFd5Wgcc1riaYf87yXtP2tluL0JkibT8s1cVibFmA/300?wxtype=jpeg&amp;wxfrom=0"/><p>下一代多模态智能的基石：浙江大学万字长文综述，构建从诊断到治理的LVLM数据生态—— ARC 框架：划出多模态智能发展的「数据弧度」论文标题: Data Quality Management for</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647717&amp;idx=2&amp;sn=afd06f2bffca9d8df0ec251893503362&amp;chksm=974e8b5e5a142c16d47eb0f8d2b2af87f6ff4363c65b43dcb2f4366be4ec36e474ca3a80c896&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 27 Nov 2025 13:20:05 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[告别“感知-决策”分离式内耗，Percept-WAM以统一模型实现58.9 mAP的BEV检测，引领自动驾驶新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8CzcaFibhcxbOwVNYetTzc7FgSfDNWAEoqfOvpqOHhCmlW414Y8kVPUg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647479&amp;idx=1&amp;sn=46b4f6ab2c042bbf485892aa89a6bebe&amp;chksm=97a4290eb9384a094d28af59f27b45d3e4be1a67f04abbe2173090f34813a4cca2e29424e1d2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 26 Nov 2025 16:09:27 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CS/AI博士发论文：找到“有价值的问题”就是最好的idea]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsibCBrncbfyiaybeVStZYvqjxOQDxicETGkMpAMOgGmNO9IY9DXoGXMsia0qf6ibicmWSxPk4JXcqpiaPsw/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近发现，其他方向博士大多都只需要2篇顶会或高区期刊，而CS博士要3篇起。而且尤其是人工智能、体系结构、网络、软件工程等子领域发表数量更多，博士延毕的焦虑感更重。昨天看到一位博士大佬发了7篇顶会，1篇</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647479&amp;idx=2&amp;sn=23ddb2463cf3e74c55a20081d2e86403&amp;chksm=97261df21dfa935adb5803d4d013bd7b213ab1f492ef9adcf156b6c2c4e8575b44bf743c5ffb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 26 Nov 2025 16:09:27 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦腾讯联手攻克4D检测难题，DetAny4D实现端到端时序一致性目标感知]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvaWCXgAI5eRE1ZxR3MZqY8ffFL1pic4sicgflR9SeZn1oDtiaKcWy2c9Zjawv0QOp5emj8mKBFMzwpg/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video作者：Jiawei Hou, Shenghao Zhang,</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647479&amp;idx=3&amp;sn=244fd32a38469afa799f2474d28c84cc&amp;chksm=97a3afbd2e975a78bac77a1e1737a869f8720af9e33c1b0bec8c6e42b07eccd8b12c8de89e26&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 26 Nov 2025 16:09:27 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 教机器人时间管理：华科大&amp;小米提出GRANT，教智能体并行执行任务，效率提升超30%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsibCBrncbfyiaybeVStZYvqjE96RSC1lXQCEu2Licicbol6ibXaa72Rob4G88GrK0lV0sLLbqOibamUdgw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution作者: Dingkang Lian</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647335&amp;idx=1&amp;sn=98095d818e1995657a7ea62f445d9f02&amp;chksm=9749cdf645a0af274b9aa7c09cb41dfe3ebebe448bb67af4afe0328a9af33061d06c86bb5ae1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 25 Nov 2025 20:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[笔墨未动，草图先行！UNC提出SketchVerify，让视频生成告别“物理翻车现场”，规划效率提升超15倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsibCBrncbfyiaybeVStZYvqjHkIdCqiaEN3SEzLdbCYcNELn8kN1JML8KoxWzVQ4VutNJBMnHfSXWnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，视频生成领域真是越来越热闹了，但不知道大家有没有发现，AI生成的视频虽然越来越惊艳，却常常在一些基本问题上“翻车”。比如，物体运动轨迹飘忽不定，或者干脆无视牛顿定律，上演“反重力”奇观。这背后其</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647335&amp;idx=2&amp;sn=cc7619cbff3fc0cbbbb7b33891677a3e&amp;chksm=972be9bf073d814dea6479c03ab4dfd321d0c34d3242f852ca072043a3a5a7c53522ce454112&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 25 Nov 2025 20:07:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | 浙大&amp;上海AI实验室发布RacketVision：首个跨多种运动的球拍分析基准]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTtNY34WOnAOibQmD2ezbY7owG4k7xyibQpTUWzvc0mku1P3zicIex8v4Xtxr78dzbzyNpXRe5aSyPg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis作者: Linfe</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647200&amp;idx=1&amp;sn=e947ebc47021ba55b9aafb97b0c959c0&amp;chksm=97b9e0d2170b36d5ee8231e638ac19476fd98a6c2be457f34bbea6474b18292b58be2511e9b3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 24 Nov 2025 18:25:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[图像去模糊新突破：同济、港浸大联合提出四元数卷积，精准建模通道耦合关系]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTtNY34WOnAOibQmD2ezbY7U5HPTmESTiaIncPmtCgAdggSpbpiaCaSEIlGv35s8ZfmM9viasTSUwHtA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇关于图像去模糊很有意思的新工作。我们都知道，照片拍糊了是个很头疼的问题，而“盲去卷积”技术就是为了在不知道模糊具体成因（比如怎么抖的、怎么失焦的）的情况下，把模糊照片恢复清晰</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647200&amp;idx=2&amp;sn=a585f8ec3f7a91707b95c94e6397fd28&amp;chksm=975e4fb0b812b05810c705145ec206f8c40e4507c947c55467fe1970aad5b8ad9331c055170e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 24 Nov 2025 18:25:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[基础架构的新探索：清华提出Step by Step Network]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTueI12bhdACsFO0uzaMHENEEmt5hmVeTBklpny5CNAGoHe83NdwH6rL4ddYQrVDfq1XP4Ba2jyuJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天我们来聊一篇非常有意思的新工作，来自清华大学和华为诺亚方舟实验室的研究者们提出了一个名为Step by Step Network (StepsNet)的通用网络架构。它的核心思想非常直观，就是让神</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647018&amp;idx=1&amp;sn=68f01a9ad012afde26db76cbaaff7ead&amp;chksm=97f8f4aa4ab29d02f4e572db4741efcf2a5095929666be64c112569019bc0567dc4e69bde76c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 23 Nov 2025 11:38:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | MIT新研究：数据集蒸馏迎来“线性时代”，一张图顶半个ImageNet？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvricRZfrtrTuIU5XiaOa5y1nJ16JGZroAZraL01eLTRFHiaJGfIDZ9bVEUsm52IdiatuichqJibdQBwn1w/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Dataset Distillation for Pre-Trained Self-Supervised Vision Models作者: George Cazenavette, Anto</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646966&amp;idx=1&amp;sn=88dac4b3c22e561dd422c109f18a1bfa&amp;chksm=97d75786643f827369fa08a13a1390cce16fca25861276e62c4acfb697736b73899bdbff0e75&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 22 Nov 2025 12:37:16 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[遥感变化检测，ChangeDINO来了：DINOv3驱动，IoU、F1指标全面SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvricRZfrtrTuIU5XiaOa5y1nfYe0HJ2OicIPIs0fsCcNDXcWm42t7W94IYPbHxGudLuCibKMFIVK6yZQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: ChangeDINO: DINOv3-Driven Building Change Detection in Optical Remote Sensing Imagery作者: Ching</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646911&amp;idx=1&amp;sn=7a79c513509f614e9b7126c9c75ed38e&amp;chksm=976a8376dffa53e2276c07c98163a9572e1dfc2120f0c7351d53b43cc55a65f8eddc31c863c2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 21 Nov 2025 17:38:11 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[解耦骨骼与体型，实现前所未有的动画真实感！Meta SAM 3D核心技术：开源人体参数化模型MHR]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvricRZfrtrTuIU5XiaOa5y1n7Qs2fKZCdzws88icoZZqzoRYj5h6T0s2zvlnWzEGhiaYiccZ8MNDjNQXQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天想和大家聊一篇非常有趣的新论文，它来自Meta，之昨天发布的SAM 3D的关键技术，题为《MHR: Momentum Human Rig》。这篇论文介绍了一种全新的参数化人体模型——MHR</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646911&amp;idx=2&amp;sn=1d9ef3256c8519b1337480753c3ca650&amp;chksm=9714c7b76b940088e5ed66c390ba16c397c4ba35d64adf1fe5e1cef5a06f5188a3a561aed3b5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 21 Nov 2025 17:38:11 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[发布即产品！SAM 3D横空出世：Meta再次颠覆3D视觉，单图即可实现高精度三维重建]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtroia98h5rf12wVfAsP1m3WnVQVamGKYG8WjTyjsUtxtW6JYGAf4fjbGkjIa4nzVVianGiaa2zJj5fw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天，Meta AI 推出了其 Segment Anything Model (SAM) 系列的最新力作——SAM 3D，一个致力于理解和重建物理世界三维形态的开创性模型。无论你是探索 AR/VR 前</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646694&amp;idx=1&amp;sn=b812cbbd33278ddb551d1bface962b15&amp;chksm=97f4ba5b7a5beb7a9e6ac57304f5a9b3a5b2eb97cd8627d2c7babf49bfa7fbce97bf16856570&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 20 Nov 2025 15:11:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[很强很惊艳！Meta重磅开源SAM 3：可概念提示，统一检测、分割与追踪，性能提升2倍]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtroia98h5rf12wVfAsP1m3WTh0icdZpPRtFAubJwR3ljTNsp2sBqpcCa9ibibzxVMmiaGU2vmz9V03adA/300?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天Meta AI终于公开发布了他们“分割一切”系列的最新力作：SAM 3（之前在匿名投稿阶段）。这不仅仅是一次常规升级，更像是一次“王炸”级别的进化。简单来说，SAM 3现在能够在一个统一的</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646694&amp;idx=2&amp;sn=e451f05131a76f3d52450a0add0822ff&amp;chksm=97fc2e2703ef5146eb253d9b7ada13f1e37b6c3b5de75bfab6b5a239c5fa0536af3d0f02f3aa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 20 Nov 2025 15:11:26 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[何恺明团队再出手！将ARC视为视觉问题，ViT从零训练60.4%准确率，达到人类平均水平]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsgQHqgLkzkHJEyvk4cicj0n68BUCzoIpMOUhluPUsf9EibqicZ2UlaSvXr9wNzxBciadk6MTBTibqkiaTw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: ARC Is a Vision Problem!作者: Keya Hu, Ali Cy, Linlu Qiu, Xiaoman Delores Ding, Runqian Wang, Ye</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646512&amp;idx=1&amp;sn=8c6aa75138f40097522c1fe5378aff69&amp;chksm=97ec23f9a299dcdc02e80e4de01edf86ff17a8c538f367312b8ca30be07d2d0763516a1651a3&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 19 Nov 2025 15:57:10 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CMU新作Co-Me：无需重训，VGGT长序列迎11.3倍加速！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsgQHqgLkzkHJEyvk4cicj0nZwZCzxlb3h3fWK8s6KT2XRRicdPGknHWeTsCcRyUg0QRC2H69bwIFibw/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers作者: Yutian Chen, Yuhen</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646512&amp;idx=2&amp;sn=8441816f904d834fbff257f53a367549&amp;chksm=97a37b5d060525f115a2f983b90e82704b3d560013849563aaf1a07881bb0b28a21ebc598cb8&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 19 Nov 2025 15:57:10 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[MIT何恺明团队新作：让扩散模型回归“去噪”本质，简单Transformer即可实现SOTA性能]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbfkzY7GbB6TyibzDBq4OFRvCGpbDr889LoAyrCgQnhsWVfVkMhGjQMct5OMuf9KjdnYVMsNv6W5A/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天来自麻省理工学院（MIT）的何恺明团队发表了一篇引人深思的技术报告，对当前主流的扩散生成模型提出了一个根本性的拷问：我们真的需要让模型去预测“噪声”吗？论文标题: Back to Basics:</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247646237&amp;idx=1&amp;sn=88640d1ae12bfb2b12f2c594280a0ada&amp;chksm=97bf3da5a7372749ffeb673f0c25ee42f4be233ba90dda8639c12b5cacf3923ef86345e74386&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 18 Nov 2025 15:20:14 +0800</pubDate>
    </item>
  </channel>
</rss>