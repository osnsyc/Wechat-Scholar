<?xml version="1.0" ?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title><![CDATA[我爱计算机视觉]]></title>
    <link>https://mp.weixin.qq.com/</link>
    <description><![CDATA[我爱计算机视觉公众号]]></description>
    <language>zh-cn</language>
    <image>
      <url>https://wx.qlogo.cn/mmhead/Q3auHgzwzM6aYkwkiboia6lA9D7ANy49WBe9icxn5NQqJjvn4Pyntzvfw/132</url>
      <title>gh_e07180c244d1</title>
    </image>
    <item>
      <title><![CDATA[浙江大学、西湖大学与蚂蚁提出OmniAgent：让模型“先听后看”，实现细粒度音视频推理]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv47jmk350p2rfLmbsqPNE2ab0ONvNdV61IUfbsbc3bxr3CkVWU5rccZaujHI78D1lnbN4wh9XBaA/640?wxtype=jpeg&amp;wxfrom=0"/><p>你是否也曾对多模态大模型的表现感到困惑？它们看似能“看懂”视频，但一问到需要音视频精确同步的细节问题，就常常“翻车”。比如，视频里的人说了某句话时，旁边牌子上写了什么字？目前的许多模型，要么无法对齐信</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652644&amp;idx=1&amp;sn=a78154c3956fd690c511a9a9129cfb1c&amp;chksm=979bc644f0cc34d153aa9bc1ffd0e53e393b7897f91a8b3519d62438228a5a587ecedd0e7e6f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 01 Jan 2026 21:14:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[RealDPO：把“真实”当作偏好，让视频生成模型学会自我纠错]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTv47jmk350p2rfLmbsqPNE24H1uFW887h9TagYfQBN5SiarYT1DbZfb4riaLUzMXkQKT11gqoJYiaD3A/640?wxtype=jpeg&amp;wxfrom=0"/><p>你可能见过这样的生成视频：第一帧看起来很惊艳，但一旦人物开始运动，肢体就开始“飘”、动作不连贯、互动不自然，甚至出现难以解释的穿模与关节扭曲。复杂动作（尤其是日常人类动作）依然是当前视频生成的硬骨头。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652592&amp;idx=1&amp;sn=6bb1f2c26165f9d0ffe84c2c08a7cb03&amp;chksm=97d1068833b5e569ecc96e568871736d77e6f3bf8f38b071270ce7550be97e3ee802ebb26004&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 31 Dec 2025 21:04:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | Adobe与JHU提出OmniVCus：前馈式多主体视频定制，多模态控制玩出新花样]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsSTmJMxgdicEupPxfzhweJzmnpgSHso00ibBX6DsScaYzSFka0tmMrPZnVibn3pnrPMV92EmzhXg8Jw/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有料的 NeurIPS 2025 论文，来自 Adobe 研究院、约翰斯·霍普金斯大学、港大、港中文和上海交大的研究者们，共同推出了一个名为 OmniVCus 的新框架。</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652350&amp;idx=1&amp;sn=35dd5fd450216dc78cb0612fcf5f2c14&amp;chksm=97068003242315d1534eac9089e3c0710257bf4f0bf0bf6683abde5ae70e67c9bee421efd36c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 30 Dec 2025 13:51:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[博士招生 | 中国科学院大学光电学院招收类脑视觉方向]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvn2cGNKvjXDZ2kjTlgNaBk0ibM2ibm0NLSb5Rh6S2EDsib92zhGEHeOgca2oQKVt7twJSkuqUkZpyIQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>中国科学院大学光电学院孟祥悦教授课题组现招收类脑视觉方向博士生，欢迎具有相关背景的优秀学生加入。团队简介本研究团队在类脑视觉成像芯片领域取得系统性突破，成功实现有机无机杂化离子半导体薄膜的可控制备，并</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652350&amp;idx=2&amp;sn=e9bde59c6ac9f87fef248f4c4a008995&amp;chksm=97870cd30002679d6d29d0a1d77beb2d8de926b32d1089307e867092233ed744c674abc059de&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 30 Dec 2025 13:51:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[清华&amp;港科大提出MVInverse：前馈式多视角逆向渲染，数秒内分解材质！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuvbfDAokdlb0ibicWtuvjpbUicq3lm0wvkiabBlLSrl8ZSlEzpwRu2TJyZeElAxRTxM7DibsFAbOotpBA/640?wxtype=jpeg&amp;wxfrom=0"/><p>在3D内容创作、增强现实和机器人技术等领域，从2D图像中精确地恢复物体的三维几何、材质与场景光照——即逆向渲染（Inverse Rendering）——是一项至关重要的基础技术。然而，现有的方法往往陷</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652147&amp;idx=1&amp;sn=d456d25fea62fdf58a2209ded9475ee3&amp;chksm=9746e99a4336dd14d400c9c226b7ab2e8fca77cb4d0609e41a446372083a82dd520ddbac920d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 29 Dec 2025 09:05:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[浸大&amp;腾讯新作Streamo：统一决策与生成，破解流式视频理解难题，性能全面SOTA]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuvbfDAokdlb0ibicWtuvjpbUTRWEyzAT6fDeeVQtn34qtYpsYiaYIQ61xLhNQ7S3oLWyZ7CTm7O9bvg/640?wxtype=jpeg&amp;wxfrom=0"/><p>当我们在谈论视频大模型时，我们通常想到的是它能对一段已经录制好的、完整的视频进行总结、问答或打标签。但如果视频是实时、连续不断的直播流呢？传统的视频大模型往往会“傻眼”，因为它们的设计初衷是“事后诸葛</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652133&amp;idx=1&amp;sn=b62d9a91dd3f05ed828dbe537e69351e&amp;chksm=9764ee4ec8e7dd8d100711ad0a313b02ea8354b86fe40f2ad5153f4ed07b51456206ca703f4c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 28 Dec 2025 22:56:36 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Overleaf也能Vibe氛围写作了？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuvbfDAokdlb0ibicWtuvjpbUE2z7Xx2RvVuOiclK0aIWX3mFsGIibIIRv0rwIrRxSQIX1AgiahJicDAvDA/640?wxtype=jpeg&amp;wxfrom=0"/><p>文智云助手是一款Chrome/Edge浏览器插件，深度集成到Overleaf，提供从氛围Vibe氛围写作、边写边译、自动续写的全流程智能辅助。我测试了以下几个使用场景，发现效果非常好。场景1【氛围写作</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247652112&amp;idx=1&amp;sn=da5b43df45e17d409b3295f48d6fd240&amp;chksm=9717b9ac5c4fee8c60ebd1a851c45d3a2c08a3e1ef9fab5de05695b673960ba602fe6718fdaa&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 27 Dec 2025 08:38:54 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[KAIST新研究DiTracker：Video DiT的新妙用，更鲁棒的点跟踪]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsHD9F0pLvDdx6OtFwW0iakGWCov7NR0ibSpmXibn50znyArRoMcelhVXV4kVH6OXqBwa7vyGeoz5iaXQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>机构：KAIST AI, Google DeepMind论文地址：https://arxiv.org/abs/2512.20606代码仓库：https://github.com/cvlab-kaist</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651937&amp;idx=1&amp;sn=1630636eb418edddc8a5109e429f538d&amp;chksm=97390ed69247b1287db135b25b98a1f9124e12efb7b72948432112ba62f219e8eb000dac458c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 26 Dec 2025 08:49:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[107倍加速生成！南洋理工&amp;Meta AI等提出HiStream：让高清视频生成摆脱“算力噩梦”]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsHD9F0pLvDdx6OtFwW0iakGtNCtIIq5zOjKiaJZ4j5tegdv9f9ibt7GiagtxlhWiba5LmIOYhRxicIgZEw/640?wxtype=jpeg&amp;wxfrom=0"/><p>虽然现在文生视频、图生视频的模型层出不穷，但想要生成真正“高清”（比如1080p）而且“长一点”的视频，实在是太——慢——了！这背后的元凶，就是视频扩散模型那高昂的计算成本，尤其是当时间和空间维度一上</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651913&amp;idx=1&amp;sn=d68b7dd53e49fe6900860fdb5cf55342&amp;chksm=97b4ebe64d0a4b3a8d1dfd9affa7d025072a55590dc3cb8e4cfb4a645f5a033ba7a12d423f64&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 25 Dec 2025 22:21:46 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[UCSD&amp;Insta360等发布DAP：全景图单目尺度深度估计基座模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuMw0GVbRr4NKcSJatOB1zBhPSlxoPH1nAPtfvbC0OpplpTKaRshs1ib8ItZC9dBjcqj0PEHfD7ruQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自 影石Insta360、加利福尼亚大学圣迭戈分校、武汉大学 和 加利福尼亚大学默塞德分校 的研究者们联手，推出了一篇名为 《Depth Any Panoramas: A Foundation</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651618&amp;idx=1&amp;sn=f4c64c6e9be472626b76fd97f3573b2e&amp;chksm=9790ecae6c9d25efe3ce415715099fb2a27ce272f34fa31934ec6609382a1e3a4195a0ffda40&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 24 Dec 2025 08:15:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[200多篇论文深度梳理！首个通用端到端自动驾驶（GE2E）综述发布，揭秘三大范式演进之路]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuMw0GVbRr4NKcSJatOB1zBr2rhZO2waM28C6bpFOF69z6Lmu3HdZgeMoIGMfsy64HwjslZDcGSZQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>自动驾驶的终极目标是构建一个能够无缝将原始传感器输入映射为驾驶决策的集成系统。为了克服传统模块化管道信息丢失和误差累积的局限性，也是为了追求更接近人类的驾驶智能，学术界和工业界正经历一场从模块化向数据</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651569&amp;idx=1&amp;sn=c9de49be2440c9ea12d49606ac3694b1&amp;chksm=9756c5bb03904c07045cf3345a22d233cf2140b666c846689edbfe5c4a34fccf27a92ddf9b3c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 23 Dec 2025 18:36:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[复旦、MSRA等提出FlashPortrait：推理速度飙升6倍，高质量“无限长”人像动画成为现实]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs0oUQFOp3NbuGoq4BerhQZfx4vrib6JNw9G589liaDk5hZHLIhQqm6P9w4euicr1jich9QkIgWZ4It6g/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇非常有趣的新工作，它来自复旦大学、微软亚洲研究院（MSRA）、西安交通大学、腾讯和阿里通义实验室的学者们。这篇名为 FlashPortrait 的论文，真正解决了当前AI人像</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651212&amp;idx=1&amp;sn=e98e64468f53a00b45b2b84763da3813&amp;chksm=97cfc2a84093bd705b2d05366254eb14fd2ee7344b4b993046c6aa3bb2bceae04a404342888b&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 22 Dec 2025 22:05:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[研究助理教授 | 博后 | 香港中文大学影像及介入放射学系招募]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTs0oUQFOp3NbuGoq4BerhQZDkPKzQdFJU2Ruf7kYXAb42XO55md3q0KZVichoIK7ezV6kJ3qmddjHw/300?wxtype=jpeg&amp;wxfrom=0"/><p>岗位招聘：研究助理教授 (Research Assistant Professor) 与博士后研究员（香港中文大学影像及介入放射学系）入职时间香港中文大学影像及介入放射学系现开始招收计划2026-20</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651212&amp;idx=2&amp;sn=1509e6d3cc9449685da7667fdb2c1ecb&amp;chksm=97b689e543d16122389c63b85ac2deded7f8a5ae7482d8154c7482347fe38d1b3ce63d494431&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 22 Dec 2025 22:05:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[豆瓣9.5，荣登京东IT新书榜第1名，这本书为啥这么受欢迎？]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtHy8mUu6MUib23vXq3o2v6MP03lE3xbtBERO2eibbWeAsiaGtzPsInUSqNrBfHPy2PVLutHnDa0QtFw/640?wxtype=jpeg&amp;wxfrom=0"/><p>关注我们丨文末赠书在人工智能飞速发展的今天，有一本书被全球学者奉为经典，称为机器学习的“圣经”。它就是克里斯托弗·毕晓普的《模式识别与机器学习》，简称“PRML”。该书在豆瓣保持着9.5分的惊人高分，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651076&amp;idx=1&amp;sn=403c1b5cd2b61c9ead9e5947398ff640&amp;chksm=9739390676c9da2575dc2f0e05a5f5fecd4799ccd7f15939e6544f58868731f82d12c15117a0&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 21 Dec 2025 08:15:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[大胆推进，小心修补！清华&amp;华为推出 X-Slim：极致压榨缓存冗余，扩散模型推理最高提升 4.97 倍！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTugY56ZahiaPKPz7zmN7YoDRiaibRp36bzgpyzcJohgwJibAv9icJoN9tms3Fb8MDCp6btenKV9r05Kfxw/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近介绍了一些针对扩散模型加速的工作，发现很多朋友感兴趣。今天分享的是来自 清华大学 和 华为 的研究者们联手提出的一种名为 X-Slim (eXtreme-Slimming Caching) 的方法</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247651039&amp;idx=1&amp;sn=ad3a4b84006114a9fd72d3756f7b7581&amp;chksm=976c5a13848f56e28d5a89a4bd7e5e6b22921cd7819871c154bedf5796802b41b58aa689a95a&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 20 Dec 2025 15:05:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[AAAI 2026 Oral | Styletailor：首个集成设计/推荐/试衣/打分的负反馈多智能体框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTugY56ZahiaPKPz7zmN7YoDRnBicGLzypbnxdN7HvHDpUENB0FuKHk2I93ibLic1icYlHj4M8qibVvrpmUg/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自人工智能与数字经济广东省属实验室（深圳）、清华大学、新加坡国立大学、Bytedance Seed、、杭州电子科技大学和香港大学的研究者们联手，推出了一项名为 StyleTailor 的新工作</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650942&amp;idx=1&amp;sn=194dcc5d163529e694dd08e360e98c55&amp;chksm=97ccc7c52db8a8d0596fed38fb88d8d8cefed8cd72d14a656031855b4c0e703016d9afe096e5&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 19 Dec 2025 12:41:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TIP 2025 | 基于傅里叶解耦的联合暗光增强和去模糊算法]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTugY56ZahiaPKPz7zmN7YoDR73R14dwF6hgxcD2NQELZKY1oUiaOQI8doZscfpNT5xsgUL5hffnHOfA/300?wxtype=jpeg&amp;wxfrom=0"/><p>本文介绍论文 Fourier-based Decoupling Network for Joint Low-Light Image Enhancement and Deblurring ，已被图像处理</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650942&amp;idx=2&amp;sn=a52d5e5f5da84263f9f174ee2d076660&amp;chksm=97d7ea00c90fe9284df6877ebd4a3b3e7b98cacaf23f1536b8de9a0efda0b1b05cdf22881652&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 19 Dec 2025 12:41:48 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[【IEEE计算机会议合集】2025年12月-2026年4月热门EI会议推荐，算法、人工智能、大数据、计算机视觉多主题征稿！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTthjtacCXfUMiarDB3hvyoTFaye2nt0wCyGs4OLLBrFQvUZQGMWbxxKn3I1gem9zxLsnGEQJTulnTQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>IEEE权威出版·IEEE/IET/CAA Fellow大咖云集·EI Compendex, Scopus稳定检索1、2025年电力系统、智能电网和人工智能国际会议（PSGAI 2025）会议官网：w</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650708&amp;idx=1&amp;sn=a2956d9cd7c7d829176f8a2d6490604d&amp;chksm=97f2a114933efde61f119a1f7928ee0e87269ddd8fdb34fb9a98796b3473953486ab87648885&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 18 Dec 2025 10:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ETH Zurich提出轻量级点云模型LitePT：参数少3.6倍，速度快2倍，性能超越SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTthjtacCXfUMiarDB3hvyoTFAbg4GDWU3bcT6HAb3b7qSFZ44hUNgibLUGwYibWme4CeRQbegmdlWZPQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>这是一篇新出的3D点云领域非常有意义的论文——《LitePT: Lighter Yet Stronger Point Transformer》。顾名思义，LitePT意为“更轻量但更强大的点云Tran</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650708&amp;idx=2&amp;sn=55f019a814dac3c398641a02ea98b110&amp;chksm=97ab0616440c7b3ada6a5a8597975238352b99e7d0982393a05434a0fd03e569d8862d26e10f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 18 Dec 2025 10:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[MindDrive：华科&amp;小米汽车联合发布，基于在线强化学习自动驾驶 VLA 模型，闭环仿真新SOTA！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtgGRS4mFBhYYsaF6WsvziapQTicQjPAKOBicmp7QLQ4zHvHSFonsArB6TibiaDYl2U35muprVnhiaVFI5g/640?wxtype=jpeg&amp;wxfrom=0"/><p>当自动驾驶迈入 “端到端” 新时代，视觉 - 语言 - 动作（VLA）范式成为打破传统模块化瓶颈的关键，但模仿学习带来的分布偏移、因果混淆等问题，始终制约着闭环驾驶的安全性与鲁棒性。如今，华中科技大学</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650537&amp;idx=1&amp;sn=c0393a7333109df41b5ddbc4fb0e5ee9&amp;chksm=977208e0e80d92990061f8dad31ffa16bfd5e2a8c7433cd3632b6cc7fa2c69c3aa87a0b58208&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 17 Dec 2025 11:26:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[【VIVO招聘】影像算法研究部 | AIGC&amp;空间智能方向（正式员工/实习生）]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtgGRS4mFBhYYsaF6WsvziapBNjcT7GJtrtt6WIJ1hqVTSicD6lUN1fQk9kB3kibSibmAeZkMSDX2U5bQ/300?wxtype=jpeg&amp;wxfrom=0"/><p>公司介绍VIVO影像算法研究部作为公司核心研发团队，专注于旗舰手机影像算法的前沿创新。我们与高通、联发科等顶级芯片厂商深度合作，定制专属算法芯片，致力于提升照片画质、影调、颜色及光学表现等核心影像体验</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650537&amp;idx=2&amp;sn=b97e4d1f79fb44e1ee0be908852b311f&amp;chksm=97fbcdde44f4a81a8ad7100b3e7aeffd383be1609c4878797b9df9130efe89dea57ed2384667&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 17 Dec 2025 11:26:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICASSP 2026出分！顶会4大高频问题最全解答（附春节前截稿CCF A-B会议）]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtgGRS4mFBhYYsaF6WsvziapaNogvAojaATj4Yn1gBibhiakZjOcSV4OVlyJbOhMCtejh8YTZlArIibrw/640?wxtype=jpeg&amp;wxfrom=0"/><p>今年 ICASSP 2026 作者回复（rebuttal）截止日期是2025年12月22日，大家今年对 rebuttal 有点摸不着头脑？我整理了大家问的比较高频的 4 大问题，给大家一一解答：第一：</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650499&amp;idx=1&amp;sn=a550224edd8b35c5c7f69c2aaa99cbe6&amp;chksm=97e7102f73cab0ec369e6faff9422b9dd6c5a549d9335bb0f9a2219e614fe6218620463c8b67&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 16 Dec 2025 21:02:41 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Adobe联合NYU提出iREPA：3 行代码，重塑生成模型“表征对齐”，空间结构才是关键！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuKHay0iciceNndYtCmrxnYXW0hoMz6E6cA90hPf1PnIABcJpk7hD8cLM4A6j1ttB9wwWRVzD5xcW2g/300?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：What matters for Representation Alignment: Global Information or Spatial Structure?论文作者：Jaskira</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650499&amp;idx=2&amp;sn=68bbc0680d7e887b9a9e5d589e4948b1&amp;chksm=973cdcb02f118af1110668d1064f8f4f9e065f7c33b7d0f281dae8d73fbb630e5fbe9ae734c1&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 16 Dec 2025 21:02:41 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[仅用视频训练，3D视觉自监督模型终获成功：E-RayZer]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTuKHay0iciceNndYtCmrxnYXWA8arO5XBmZa0She6vZibPMqLsViaotdkDRlPYC63fDonBsZqMSj1iastw/640?wxtype=jpeg&amp;wxfrom=0"/><p>介绍一篇今天刚挂在arXiv上的重磅论文——E-RayZer，它可能将成为3D视觉领域的一个重要里程碑。我们知道，在大语言模型（LLM）和2D视觉领域，自监督预训练（Self-supervised P</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650297&amp;idx=1&amp;sn=750aacbc6c48fcdabfaa3880005b19b6&amp;chksm=97e6ee42e14f87ffac95a9feb9de5987316eed4d48a9a09eca5172a17fd64f9b44a7182d8efe&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 15 Dec 2025 23:54:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[NeurIPS 2025 | SuperCLIP：对比学习加上分类任务，使CLIP更强了！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtkH6u0HwWfiabuqWyH0HINSVWLDcCJib6Ep70KI9eNo8xbOa2R8MCRvcsJkXF2n5LxXrmkgmx4MmCw/640?wxtype=jpeg&amp;wxfrom=0"/><p>自诞生以来，CLIP 就凭借其强大的零样本学习和跨模态理解能力，成为了视觉语言模型领域的基石。然而，这位“优等生”也有自己的烦恼：它擅长把握图像和文本的“全局大意”，却常常在“细枝末节”上犯迷糊。比如</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650154&amp;idx=1&amp;sn=8fc4960956e8303d94464ac4a346a3d6&amp;chksm=97f53f755a16b568696f15b218a369e6dde1dbcb10e5cee6ef298977d1dfbb521f0a4160ad5f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 14 Dec 2025 23:16:29 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[SpatialDreamer：通过主动心理想象，激发MLLMs空间推理能力]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtXaPcZBTAooj2SXDeM54JXxWNhe2ZVPrOFM99vUaicEwgX8vrfcq14uJKJIQXHZ8toAficKo2V83TA/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好！今天想和大家聊一篇非常有意思的新工作，来自MBZUAI和中山大学的研究者们提出的SpatialDreamer。虽然现在的多模态大模型（MLLMs）看图说话能力一流，但和人类相比，在需要“脑补”</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650103&amp;idx=1&amp;sn=638b7b4d74b8a48195421dad67f7729b&amp;chksm=976222417907aadc887c345af896b509da9db5248a11e00eb577003d335c3ca9d7fbc88b27bf&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 13 Dec 2025 20:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[12月即将截稿的5个CCF-B类会议，录用率超高！速看！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu9M7sjur1DwMEJwBPgdOJTCIic5Cf3gw0FaTfZ3TKEzGxkGDnN3oWReibPGQd16LvDFzg7WBI89x0g/300?wxtype=jpeg&amp;wxfrom=0"/><p>12月快过去一半了，大家投稿情况怎么样啦，现在为大家精心整理了部分本月截稿的CCF-B类会议！录用率比较高，大家抓紧时间冲呀！ICAPS 2026截稿日期： 2025年12月14日会议简介： 自动化规</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247650103&amp;idx=2&amp;sn=632c5e3466d5958940ac9e1415bb908f&amp;chksm=97a6c75cd2e70cbf7f12101ec31f6b1ec31867e8fbaa794248493f307d3601efc1d7989a825e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 13 Dec 2025 20:33:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[WorldLens: 真正评估“世界模型”的基准，终于来了]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtl5r3Qx5VGatFzJDSFMAciakPXTt63xToOOUtNS7OF5lbnqbD5Fiam9lZWedN3ntjiaGNxlTeAkQy6g/640?wxtype=jpeg&amp;wxfrom=0"/><p>近年来，生成式世界模型 (World Models) 迅速成为自动驾驶与具身智能领域的核心方向。从文本生成驾驶视频，到可控 4D 场景生成，模型已经能够生成视觉上极具真实感的驾驶画面。但一个关键问题长</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649988&amp;idx=1&amp;sn=3571c12e2d2c5548bc86b29d03785f35&amp;chksm=97bbe0e8fd615343f5e3baf8eec00e1f85bdcbea90cbfa6f87e7426b6658ce1bfc68fa9347e2&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 12 Dec 2025 13:22:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[清华等提出FacePhys：极低计算需求（3.6MB内存、9.46ms/帧），超高精度的心率检测模型]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtl5r3Qx5VGatFzJDSFMAcia9Jib3VibqIfuSfXall1RLQFYvvWLj2uHAQgibWaGJDiaOH1BcRvZCZeNEA/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天介绍一篇新出的生理信号检测领域的新工作，题为《FacePhys: State of the Heart Learning》，这篇论文聚焦在如何通过摄像头，更精准、更高效地测量我们的生命体征，特别是</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649988&amp;idx=2&amp;sn=3514f361611a7c0369fa6f023af1664b&amp;chksm=9776b2e6760089abd33294d301033b6e2e81e49e6452b27b6256308b482ce7045be5c4f396bd&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 12 Dec 2025 13:22:42 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[TPAMI 2026 哈工大&amp;清华等提出DiTFuse：迈向统一、可控的图像融合新范式]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu2ypW2psy7pxK0ZaavMCMqWszX5M28XIUibvYTBtFOgaxW6EibxdFb2TG3DtBfEtoiaFTPAnuaKJRxQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天想和大家聊一篇新出的被顶刊TPAMI录用的工作，它来自哈尔滨工业大学、清华大学和武汉大学等机构的研究者们。这篇论文提出了一个名为 DiTFuse 的框架，致力于解决图像融合领域长期以来存在</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649662&amp;idx=1&amp;sn=bc1ee625ca2b52ff968d1e1d779a15d7&amp;chksm=97f51a44bb0b5813e8b1da1b6130c32fa332b828658a918939d2975b7a9d569613f49a68811d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 11 Dec 2025 12:54:12 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[BulletTime：解耦时空控制，斯坦福与ETH Zurich重新定义4D视频生成]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTu2ypW2psy7pxK0ZaavMCMq07mHPUm7ate4xPEVhicYFVrDibdtDDGxxYiaPdtMBpxBugaia6xlF7VibxA/300?wxtype=jpeg&amp;wxfrom=0"/><p>相信看过电影《黑客帝国》的朋友，都对其中主角尼奥躲避子弹的经典慢镜头记忆犹新。镜头围绕着几乎静止的主角高速旋转，展现出无与伦比的视觉冲击力，这就是著名的“子弹时间”（Bullet Time）特效。长久</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649662&amp;idx=2&amp;sn=3231fb180a236654ea031526249e668a&amp;chksm=974a899ae1c5c35d69599a051c9b24a90ec35f4f8e99cb3df5d68b07aab4c1f9091530a92e36&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 11 Dec 2025 12:54:12 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[EMMA：华为发布统一多模态新架构，4B模型实现理解、生成、编辑功能的齐头并进]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0HI6FKKK2DQpC1waXBNWXsM189ibRbEGHLJ3zoS7iaS97RmUzkVojnEHA/640?wxtype=jpeg&amp;wxfrom=0"/><p>今天，我们来聊聊多模态领域的新工作：EMMA,全称是“Efficient Multimodal Understanding, Generation, and Editing with a Unifie</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649426&amp;idx=1&amp;sn=98a97872d0fd5425062e18232a5ee618&amp;chksm=97c8d5c295ac1992f75892254c80a88ac981cc53a32aac55b333f6f3d6be3d250217df4b6a44&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 10 Dec 2025 12:25:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[与贤同行 创领未来｜合肥工业大学人工智能创新学院诚聘天下英才]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0aHAQ3ibW36cicJzwyWoqm49pLVOwCGYWclzofxed6MTzjzib0A4MNqOUg/300?wxtype=jpeg&amp;wxfrom=0"/><p></p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649426&amp;idx=2&amp;sn=e14eecdbfc34a3302a1d959be0d0b6d0&amp;chksm=97ce3eaaa84e545a09effe055d5c6edb9a988a8a18a0095dadf72e0d16284e6ca5b2182b8b9d&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 10 Dec 2025 12:25:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[投稿需交100美金的CCF-A！IJCAI 2026倒计时一个月啦！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0IF6eLtyoFjPZpcNGVYOo4zyWSpvnuFMqVvDU9DncLDh8q5KGBwic3tg/300?wxtype=jpeg&amp;wxfrom=0"/><p>IJCAI（全称 International Joint Conference on Artificial Intelligence）是人工智能领域公认的国际顶级学术会议之一，亦位列中国计算机学会CC</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649426&amp;idx=3&amp;sn=ec18a453d432b66a553d3b94bc305477&amp;chksm=9704c699dddb7472faed964b0d5642ed954ea539fafa192e9ae4ba97130ed022dbeb98adf9f6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 10 Dec 2025 12:25:13 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[最前沿的强化学习课程来了！斯坦福CS224R深度强化学习全套课程开放！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0Sic7v8IfxLfAwCUylTclpQ5gf1FZu6UF7T5Ku2kWupnYzicWMGpbXoNQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>最近AI圈风起云涌，大模型技术日新月异。但不知道你有没有想过，让ChatGPT这类大模型能够如此“善解人意”的背后，除了海量数据的预训练，一项关键技术功不可没——那就是强化学习，特别是基于人类反馈的强</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649236&amp;idx=1&amp;sn=b96b34a0839e94623e63f09e6c8ba99a&amp;chksm=97ad6a3e4d7c5e6a8a74f15699039189a2fdfb2d6d7e912517704e765392667754806151d803&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 09 Dec 2025 14:01:56 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[架构解耦为什么对统一多模态模型有效？港中文联合美团提出AIA，揭示其真正的奥秘！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvbicDZf5xlkobzicfdd7ibko0oUCNUcIicfS0jZS5PxUWoiaT6X9hCPjINw81AbtozDZsxaYoDYXaCVAA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近来自港中文MMLab、美团等机构，对当前火热的统一多模态模型（Unified Multimodal Models, UMMs）提出了一个“反潮流”的观点。当我们希望一个AI模型既能“看懂图说话”（</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649236&amp;idx=2&amp;sn=6f86a3ee12f7a5045d030cfe25ee4aea&amp;chksm=97c59bbb6dd14bc1e00b668804e6911acdaecc60cbb069bf4f61f2ee4ab8eee73270de979af6&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 09 Dec 2025 14:01:56 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[ICML2026！CCF-A！截稿仅剩45天，速看！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvib80yz1ntDuRVcI7Ayfuw4UibIY7d8w2A3GPwib5KMgC7h85NZ0vwVRg0kcGbeC8aZVgBNRmzzaQlA/300?wxtype=jpeg&amp;wxfrom=0"/><p>ICML 2026，即第43届国际机器学习大会，是机器学习与人工智能方向备受推崇的全球顶尖学术会议之一，与NeurIPS、ICLR并列被誉为该领域的“三大旗舰会议”。在中国计算机学会（CCF）的权威推</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649236&amp;idx=3&amp;sn=746f5d674c6caa7e2af1bea38ae68629&amp;chksm=97117b4153dcc574e99762d16220121b8d7f5952f0db415a5d2c3334d36e0df9f9a24cbd7180&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 09 Dec 2025 14:01:56 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[苏黎世联邦理工等提出LeAD-M3D：无需LiDAR，单目3D检测也能SOTA且实时！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsdE6icV8QYnIqOb5CsLt93R8JH1ZrIyjTo5Tt2V9SkSrI5SHEsVxuuhOBlWnmsibjNVKriaOVRicz7rg/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection作者机构：DeepScena</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649182&amp;idx=1&amp;sn=5bd41ab4d4fe7e62d4864ca2ee356a89&amp;chksm=97f8a3a9bbac1bb9385fe01fc99d90d5f768698632f27c4b86d69041aaa4a3fe4a7582606238&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 08 Dec 2025 23:26:06 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[打破文本记忆局限！ViLoMem要记视觉关注点，显著提升多模态推理能力！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvib80yz1ntDuRVcI7Ayfuw4KBtVagz1Jg5xlxMGMx3uwBd10GJeZiaXFhIG5NAnXe3ZxXsNXR7M6xA/640?wxtype=jpeg&amp;wxfrom=0"/><p>你是否发现，现在的多模态大模型（MLLMs）虽然在单次回答上表现惊艳，但往往像个“健忘症患者”？它们处理每一个问题时都是从零开始（de novo），反复掉进同一个坑里，昨天犯过的视觉识别错误，今天换个</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247649046&amp;idx=1&amp;sn=dd5bac6438021e3b1dd161357eab4cc5&amp;chksm=9747b6e7445a9e0b6888782353cc52e7479ce6ac091fd9297927b8168462daab49a34342c96e&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sun, 07 Dec 2025 22:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[突破视频重照明瓶颈！南洋理工、清华等提出Light-X：首个实现单目视频相机与光照联合控制的生成框架]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsia9tJcNGo1BIlmX5k0gUfYPQZALXjopdN5EMPF0KZDCU4HDv5Dia2eibKyI8K0ueXBFldiaDtmgfG7A/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文名称: Light-X: Generative 4D Video Rendering with Camera and Illumination Control作者: Tianqi Liu, Zha</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648997&amp;idx=1&amp;sn=9e924748e811a0e98f20a7917f051471&amp;chksm=97a8f56ca7008ffd274c122ab782de58fbcdcef74cb4d5ed6465be22c4bb94213f875d2aac8c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Sat, 06 Dec 2025 22:21:29 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[南理工\SUTD等提出Artemis：结构化视觉推理，MLLM感知策略学习新突破]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsia9tJcNGo1BIlmX5k0gUfYP4lIZFiajh2QWb0VFsm45oWkaucozcI776ibynFzQs5XDl5lJYCC4znw/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题：Artemis: Structured Visual Reasoning for Perception Policy Learning论文作者：Wei Tang, Yanpeng Sun,</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648990&amp;idx=1&amp;sn=b3ec88ca2acd8ec7ec87501d636d7682&amp;chksm=97f8d549d468cda9c39478384a22a83cd376d1829474b2638b45d7e265c8eb9b5e4f10cf2887&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 05 Dec 2025 18:41:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CCF-B类！人工智能顶会录用率近40%，截稿仅剩70天！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtHy8mUu6MUib23vXq3o2v6MNfglOqzr7xiahhrR4pk4icmMz6Mpicpfa7FyfgnvOIIa5M3hI7UwkjMwg/300?wxtype=jpeg&amp;wxfrom=0"/><p>知识表示与推理是人工智能中一条成熟而活跃的研究主线，其核心思想是：把对世界的认识整理成显式、陈述性的符号结构，让专用推理引擎按语义规则演算，从而把原本隐含的信息变成可用的知识。这套方法不仅为智能体、自</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648990&amp;idx=2&amp;sn=7475ede33ce44b9f4a58a869f331c7fc&amp;chksm=973e6637dc8f4f2c2e28283e5a6cf5af4e8ec6c9b05e9a94c66253d52d98bd0b6e4169a83e1c&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Fri, 05 Dec 2025 18:41:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[RobustVGGT来了！不惧噪声，显著提升3D重建鲁棒性]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTtHy8mUu6MUib23vXq3o2v6MaqMHy9xwR7e9iav7VzpJEpqmlILGINhfQrWhia51h6vlRlcMaGc94FJQ/640?wxtype=jpeg&amp;wxfrom=0"/><p>论文标题: Emergent Outlier View Rejection in Visual Geometry Grounded Transformers论文作者: Jisang Han, Sung</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648852&amp;idx=1&amp;sn=3d418d3062a10ec11571ee8cb20fedbd&amp;chksm=9777357152cd3904d20c9013700c1cead41504f25f54b3f6c6ef673ccd2307d53918b698ebdb&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 04 Dec 2025 18:46:25 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[CCF-B类！录用率32.5%，ICME2026截稿倒计时不足一个月！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvtUBBIAia0hr2ribSpibOgLZeNb1RqJPVXmzUxrS1LiaWsfGncflmCvPPBZOFCFvg12ic0wGkuOu4qvjw/300?wxtype=jpeg&amp;wxfrom=0"/><p>ICME 2026是多媒体领域的权威IEEE旗舰会议。自2000年起，该会议由IEEE下属四大学会共同主办，并获中国计算机学会推荐为B类学术会议，内容涵盖图像视频处理、多模态计算、多媒体通信与检索等重</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648852&amp;idx=2&amp;sn=1e704a8e499bda6c5343aacc8e1c7e04&amp;chksm=9702640f1b73717aaa3db893699d35f26bf8bdd439284700d91e9b70afec3ac91a51765869ab&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Thu, 04 Dec 2025 18:46:25 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[涨点神器！265个顶会上的开源即插即用模块汇总（附源码）！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvtUBBIAia0hr2ribSpibOgLZeVks2acdG2H2TQDlGC8QOX9Rt98kerTnZvUsrXQeMmLsicsFQzibrTGEA/640?wxtype=jpeg&amp;wxfrom=0"/><p>看了很多论文，创新点完全没头绪？或者好不容易找到个baseline，反复去看，去跑实验，却根本找不到破绽，模型无从改起……那我强烈推荐你用即插即用的模块进行缝合，即便小白，也能快速出文章！这些模块通常</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648497&amp;idx=1&amp;sn=a12541574296a81dce3dbefd23c35b05&amp;chksm=9741f092771d607633b4158a54c22c0d15874bcb3a3849ee7f1bd3ddb5705601f72d49b3c286&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 03 Dec 2025 12:31:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[突破规模瓶颈！JHU&amp;深大推出FoundationGait：步态理解自监督基础模型，大幅刷新下游步态识别与分析精度]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvhicG5KVuDbMKdyM6BZpZtRG6gFGic1LwDEQ65IUicJbVZUvHRQOYGrOGrQfjtazshheExrcRA0iaHicg/300?wxtype=jpeg&amp;wxfrom=0"/><p>今天，我们要聊的这篇论文是来自约翰霍普金斯大学（JHU）和深圳大学的最新研究成果——“Silhouette-based Gait Foundation Model”，直译过来是“基于轮廓的步态基础模型</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648497&amp;idx=2&amp;sn=9a819bf634f069f41b9d3ab937ef4f1b&amp;chksm=97046daf5fda55c80112b8319853e896e2afd25b3b225867e96038c898a536da3d3915e94277&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Wed, 03 Dec 2025 12:31:31 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[三篇论文看多模态LLMs“视觉文本压缩”，清华、DeepSeek、AI2“不约而同”的深度探索]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvtUBBIAia0hr2ribSpibOgLZeZB6aCdDX8026TW4N6RtvRDEj3W2kGnSKR9kvDI09Wahfew1f4xtn7A/640?wxtype=jpeg&amp;wxfrom=0"/><p>大家好，今天我们来聊一个最近在AI圈子里激起不小浪花的新方向。大家都知道，让大语言模型（LLM）处理越来越长的文本，是通往更强通用人工智能路上的关键一环。无论是分析一本小说、一份财报，还是进行多轮对话</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648265&amp;idx=1&amp;sn=0ec214efd0c030504c78ae45dac8f31a&amp;chksm=97cf065fd0d21153386b97f8621adfd43e5263d5ba49f6f92b5c50144ad78b94691974fc383f&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 02 Dec 2025 12:54:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[华科&amp;清华&amp;快手推出VGT：20倍加速，唤醒任意视觉语言模型的“隐藏绘画技能”！]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvC3L5x1XvGia2SiaG6qsBy5Hk6XYjTuvDEKGEKaDaP8a0WpsXVCVL8qrNMoYKPo2kqYtvj5otVsYuA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，来自华中科技大学、清华大学和快手等机构的研究者们，带来了一篇非常有趣的工作，提出了一种名为VGT (Visual Generation Tuning)的新范式。简单来说，这是一种“视觉生成调优”</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247648265&amp;idx=2&amp;sn=cca2d33133c5559d445f803c7b6e4c14&amp;chksm=97b7ca6e4964c1093c65244e3c91acf769e499be65c5ef7b90ee2f14e645303add2e0ef39b75&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Tue, 02 Dec 2025 12:54:49 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[【EI检索|计算机领域】2025年12月-2026年3月 IEEE 国际会议推荐]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTsMTQLdCZg1u8A7Qv2W2SOZN82oNcpiaLkISuA6xAicQWxRunJws8FXnBOcJDDEx9PueKBNPh298vsg/640?wxtype=jpeg&amp;wxfrom=0"/><p>加★标 点关注不迷路IEEE出版| 江苏大学主办1、人工智能驱动图像处理与计算机视觉技术国际学术研讨会 (AIPCVT 2025)会议官网：www.ipcvt.com召开时间：2025年12月12-1</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647975&amp;idx=1&amp;sn=05b4fa2b9710d99f7615b0c4895ff093&amp;chksm=97a707f28309b6ca697dcdefc40923fbcb4a5c23e7a5dc722652474e773ce6a9f1903e6c81db&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 01 Dec 2025 08:10:00 +0800</pubDate>
    </item>
    <item>
      <title><![CDATA[Mamba杀入3D追踪！新框架MambaTrack3D破解高时延难题，精度提升9.5%]]></title>
      <description><![CDATA[<img referrerpolicy="no-referrer" src="https://mmbiz.qpic.cn/mmbiz_jpg/BJbRvwibeSTvTtNY34WOnAOibQmD2ezbY776bXLu9Wov3IhbL0JrfdVwlt1jBnjlE4zQSTwdRCOk8FiaFiau1yRKnA/300?wxtype=jpeg&amp;wxfrom=0"/><p>最近，状态空间模型（State Space Model, SSM），特别是Mamba架构，在序列数据处理领域掀起了一股热潮，大有与Transformer分庭抗礼之势。这股风也吹到了3D视觉领域。今天，</p>]]></description>
      <link>http://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&amp;mid=2247647975&amp;idx=2&amp;sn=a9754b0564aee3565a445ddeefbd78df&amp;chksm=97e0645dc7e2a262a2fa1638e141fe48f83d4aa22c5fb19c07a458ee953208e1d1fcc64e94e7&amp;scene=0&amp;xtrack=1#rd</link>
      <pubDate>Mon, 01 Dec 2025 08:10:00 +0800</pubDate>
    </item>
  </channel>
</rss>